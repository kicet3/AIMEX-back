from fastapi import APIRouter, Depends
from pydantic import BaseModel
from typing import List
import os
from sqlalchemy.orm import Session
from app.database import get_db
from app.models.influencer import AIInfluencer
from app.models.user import HFTokenManage
from app.core.encryption import decrypt_sensitive_data
from app.core.security import get_current_user
import re
from fastapi import HTTPException
from app.services.runpod_manager import get_vllm_manager
import asyncio
import logging

logger = logging.getLogger(__name__)

router = APIRouter()

# ëª¨ë¸ idì™€ HuggingFace ëª¨ë¸ ê²½ë¡œ ë§¤í•‘ ì˜ˆì‹œ
MODEL_MAP = {}

SYSTEM_PROMPT = ""


class InfluencerInfo(BaseModel):
    influencer_id: str
    influencer_model_repo: str


class MultiChatRequest(BaseModel):
    influencers: List[InfluencerInfo]
    message: str


class InfluencerResponse(BaseModel):
    influencer_id: str
    response: str


class MultiChatResponse(BaseModel):
    results: List[InfluencerResponse]


async def process_single_influencer(
    influencer_info: InfluencerInfo,
    message: str,
    ai_influencer: AIInfluencer,
    hf_token: str,
    vllm_manager
) -> InfluencerResponse:
    """ë‹¨ì¼ ì¸í”Œë£¨ì–¸ì„œì— ëŒ€í•œ ì‘ë‹µ ìƒì„±"""
    try:
        # ì–´ëŒ‘í„° ë ˆí¬ì§€í† ë¦¬ ê²½ë¡œ ì •ë¦¬
        adapter_repo = influencer_info.influencer_model_repo
        
        # URL í˜•íƒœì˜ ë ˆí¬ì§€í† ë¦¬ë¥¼ Hugging Face ë ˆí¬ì§€í† ë¦¬ í˜•ì‹ìœ¼ë¡œ ë³€í™˜
        # í—ˆê¹…í˜ì´ìŠ¤ URLì—ì„œ ë ˆí¬ ê²½ë¡œë§Œ ì¶”ì¶œ
        from app.utils.hf_utils import extract_hf_repo_path
        adapter_repo = extract_hf_repo_path(adapter_repo)
        
        # ì–´ëŒ‘í„° ë ˆí¬ì§€í† ë¦¬ ìœ íš¨ì„± ê²€ì‚¬
        if adapter_repo in ["sample1", "sample2", "sample3"] or "sample" in adapter_repo or not adapter_repo.strip():
            logger.error(f"Invalid adapter repository for {influencer_info.influencer_id}: {adapter_repo}")
            return InfluencerResponse(
                influencer_id=influencer_info.influencer_id,
                response="ìœ íš¨í•˜ì§€ ì•Šì€ ì–´ëŒ‘í„° ë ˆí¬ì§€í† ë¦¬ì…ë‹ˆë‹¤. ì‹¤ì œ í—ˆê¹…í˜ì´ìŠ¤ ëª¨ë¸ ë ˆí¬ì§€í† ë¦¬ë¥¼ ì„¤ì •í•´ì£¼ì„¸ìš”."
            )
        
        # ëª¨ë¸ ID ìƒì„± (ì–´ëŒ‘í„° ì´ë¦„)
        model_id = influencer_info.influencer_id
        
        # RunPodëŠ” ë™ì ìœ¼ë¡œ ì–´ëŒ‘í„°ë¥¼ ë¡œë“œí•˜ë¯€ë¡œ ë¯¸ë¦¬ ë¡œë“œí•  í•„ìš” ì—†ìŒ
        logger.info(f"RunPod will dynamically load adapter: {model_id} from {adapter_repo}")
        
        # ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ ìƒì„±
        if ai_influencer.system_prompt:
            system_prompt = ai_influencer.system_prompt
            logger.info(f"âœ… ì €ì¥ëœ ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ ì‚¬ìš©: {ai_influencer.influencer_name}")
        else:
            # ê¸°ë³¸ ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ ìƒì„±
            system_prompt = f"ë„ˆëŠ” {ai_influencer.influencer_name}ë¼ëŠ” AI ì¸í”Œë£¨ì–¸ì„œì•¼.\n"
            desc = getattr(ai_influencer, "influencer_description", None)
            if desc is not None and str(desc).strip() != "":
                system_prompt += f"ì„¤ëª…: {desc}\n"
            personality = getattr(ai_influencer, "influencer_personality", None)
            if personality is not None and str(personality).strip() != "":
                system_prompt += f"ì„±ê²©: {personality}\n"
            system_prompt += "í•œêµ­ì–´ë¡œë§Œ ëŒ€ë‹µí•´.\n"
            logger.info(f"âš ï¸ ì €ì¥ëœ ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ê°€ ì—†ì–´ ê¸°ë³¸ í”„ë¡¬í”„íŠ¸ ì‚¬ìš©: {ai_influencer.influencer_name}")
        
        # vLLM ë§¤ë‹ˆì €ë¡œ ì‘ë‹µ ìƒì„± ìš”ì²­
        logger.info(f"Generating response for {influencer_info.influencer_id} using vLLM Manager")
        
        # ë©”ì‹œì§€ êµ¬ì„±
        messages = []
        if system_prompt:
            messages.append({"role": "system", "content": system_prompt})
        messages.append({"role": "user", "content": message})
        
        # RunPod vLLM workerì— ë§ëŠ” í˜ì´ë¡œë“œ êµ¬ì„±
        payload = {
            "input": {
                "messages": messages,
                "max_tokens": 150,
                "temperature": 0.7,
                "stream": False,
                "lora_adapter": str(influencer_info.influencer_id),
                "hf_repo": adapter_repo,
                "hf_token": hf_token
            }
        }
        
        logger.info(f"ğŸ¯ LoRA ì–´ëŒ‘í„° ì„¤ì •: {influencer_info.influencer_id} from {adapter_repo}")
        
        # RunPod runsync ë©”ì„œë“œ ì‚¬ìš©
        result = await vllm_manager.runsync(payload)
        
        # ì‘ë‹µ ì¶”ì¶œ
        if result.get("status") == "completed" and result.get("output"):
            output = result["output"]
            if output.get("status") == "success":
                response_text = output.get("generated_text", "")
            else:
                response_text = "ì‘ë‹µì„ ìƒì„±í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
        else:
            response_text = result.get("generated_text", "ì‘ë‹µì„ ìƒì„±í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        logger.info(f"âœ… Generated response for {influencer_info.influencer_id}")
        
        return InfluencerResponse(
            influencer_id=influencer_info.influencer_id,
            response=response_text
        )
        
    except Exception as e:
        logger.error(f"Error processing influencer {influencer_info.influencer_id}: {e}")
        return InfluencerResponse(
            influencer_id=influencer_info.influencer_id,
            response=f"ì‘ë‹µ ìƒì„± ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}"
        )


@router.post("/multi-chat", response_model=MultiChatResponse)
async def multi_chat(request: MultiChatRequest, db: Session = Depends(get_db), current_user: dict = Depends(get_current_user)):
    logger.info(f"Multi-chat request received: {request}")
    
    # í˜„ì¬ ì‚¬ìš©ì ì •ë³´
    user_id = current_user.get("sub")
    logger.info(f"Current user ID: {user_id}")
    
    # ì‚¬ìš©ìê°€ ì†í•œ ê·¸ë£¹ ID ëª©ë¡ ì¡°íšŒ
    from app.models.user import User
    user = db.query(User).filter(User.user_id == user_id).first()
    if not user:
        raise HTTPException(status_code=404, detail="User not found")
    
    user_group_ids = [team.group_id for team in user.teams]
    logger.info(f"User belongs to groups: {user_group_ids}")
    
    # vLLM ë§¤ë‹ˆì € ê°€ì ¸ì˜¤ê¸°
    vllm_manager = get_vllm_manager()
    
    # ë³‘ë ¬ ì²˜ë¦¬ë¥¼ ìœ„í•œ íƒœìŠ¤í¬ ë¦¬ìŠ¤íŠ¸
    tasks = []
    
    for influencer_info in request.influencers:
        logger.info(f"Processing influencer: {influencer_info.influencer_id}, repo: {influencer_info.influencer_model_repo}")
        
        # ë°ì´í„°ë² ì´ìŠ¤ì—ì„œ AI ì¸í”Œë£¨ì–¸ì„œ ì •ë³´ ì¡°íšŒ
        ai_influencer = (
            db.query(AIInfluencer)
            .filter(AIInfluencer.influencer_id == influencer_info.influencer_id)
            .first()
        )

        if not ai_influencer:
            async def return_error():
                return InfluencerResponse(
                    influencer_id=influencer_info.influencer_id,
                    response="AI ì¸í”Œë£¨ì–¸ì„œë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
                )
            tasks.append(asyncio.create_task(return_error()))
            continue

        # ê·¸ë£¹ ê¶Œí•œ í™•ì¸ - ì‚¬ìš©ìê°€ ì†í•œ ê·¸ë£¹ì˜ ëª¨ë¸ë§Œ ì ‘ê·¼ ê°€ëŠ¥
        if ai_influencer.group_id not in user_group_ids and ai_influencer.user_id != user_id:
            logger.warning(f"User {user_id} does not have access to influencer {influencer_info.influencer_id} in group {ai_influencer.group_id}")
            async def return_access_error():
                return InfluencerResponse(
                    influencer_id=influencer_info.influencer_id,
                    response="í•´ë‹¹ ëª¨ë¸ì— ëŒ€í•œ ì ‘ê·¼ ê¶Œí•œì´ ì—†ìŠµë‹ˆë‹¤."
                )
            tasks.append(asyncio.create_task(return_access_error()))
            continue

        # hf_manage_idê°€ ì—†ìœ¼ë©´ ì˜¤ë¥˜
        if ai_influencer.hf_manage_id is None:
            async def return_no_hf_token():
                return InfluencerResponse(
                    influencer_id=influencer_info.influencer_id,
                    response="í—ˆê¹…í˜ì´ìŠ¤ í† í°ì´ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤."
                )
            tasks.append(asyncio.create_task(return_no_hf_token()))
            continue

        # í—ˆê¹…í˜ì´ìŠ¤ í† í° ì¡°íšŒ
        hf_token = (
            db.query(HFTokenManage)
            .filter(HFTokenManage.hf_manage_id == ai_influencer.hf_manage_id)
            .first()
        )

        if not hf_token:
            async def return_hf_not_found():
                return InfluencerResponse(
                    influencer_id=influencer_info.influencer_id,
                    response="í—ˆê¹…í˜ì´ìŠ¤ í† í°ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
                )
            tasks.append(asyncio.create_task(return_hf_not_found()))
            continue

        # í—ˆê¹…í˜ì´ìŠ¤ í† í° ë³µí˜¸í™”
        encrypted_token_value = getattr(hf_token, "hf_token_value", None)
        if not encrypted_token_value:
            async def return_no_token_value():
                return InfluencerResponse(
                    influencer_id=influencer_info.influencer_id,
                    response="í† í° ê°’ì´ ì—†ìŠµë‹ˆë‹¤."
                )
            tasks.append(asyncio.create_task(return_no_token_value()))
            continue
        
        decrypted_token = decrypt_sensitive_data(encrypted_token_value)

        # ë¹„ë™ê¸° íƒœìŠ¤í¬ ìƒì„±
        task = asyncio.create_task(
            process_single_influencer(
                influencer_info=influencer_info,
                message=request.message,
                ai_influencer=ai_influencer,
                hf_token=decrypted_token,
                vllm_manager=vllm_manager
            )
        )
        tasks.append(task)
    
    # ëª¨ë“  íƒœìŠ¤í¬ ì™„ë£Œ ëŒ€ê¸°
    results = await asyncio.gather(*tasks)
    
    response = {"results": results}
    logger.info(f"Multi-chat response completed with {len(results)} responses")
    logger.info(f"Response: {response}")
    return response

