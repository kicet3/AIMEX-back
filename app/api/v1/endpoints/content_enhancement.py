from fastapi import APIRouter, Depends, HTTPException, status
from sqlalchemy.orm import Session
from typing import List
from pydantic import BaseModel
from typing import Optional

from app.database import get_db
from app.models.user import User
from app.api.v1.endpoints.auth import get_current_user
from app.schemas.content_enhancement import (
    ContentEnhancementRequest,
    ContentEnhancementResponse,
    ContentEnhancementApproval,
    ContentEnhancementList,
)
from app.services.content_enhancement_service import ContentEnhancementService
from app.models.content_enhancement import ContentEnhancement
from app.models.influencer import AIInfluencer
from app.models.user import HFTokenManage
from app.core.encryption import decrypt_sensitive_data
from app.core.security import get_current_user
from app.services.openai_service_simple import get_openai_service
import logging
import uuid
from datetime import datetime
from transformers import AutoModelForCausalLM, AutoTokenizer
from peft import PeftModel
import torch
import re

logger = logging.getLogger(__name__)
router = APIRouter()
content_service = ContentEnhancementService()


@router.post("/enhance", response_model=ContentEnhancementResponse)
async def enhance_content(
    request: ContentEnhancementRequest,
    db: Session = Depends(get_db),
    current_user: dict = Depends(get_current_user),
):
    """ê²Œì‹œê¸€ ì„¤ëª… í–¥ìƒ"""
    try:
        user_id = current_user.get("sub")
        if not user_id:
            raise HTTPException(
                status_code=status.HTTP_401_UNAUTHORIZED,
                detail="User authentication required",
            )

        # OpenAI API í‚¤ í™•ì¸
        from app.core.config import settings

        if not settings.OPENAI_API_KEY:
            raise HTTPException(
                status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
                detail="OpenAI API key not configured",
            )

        enhancement = await content_service.enhance_content(
            db=db, user_id=user_id, request=request
        )
        return enhancement
    except HTTPException:
        raise
    except Exception as e:
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            detail=f"Content enhancement failed: {str(e)}",
        )


@router.post("/approve", response_model=ContentEnhancementResponse)
async def approve_enhancement(
    approval: ContentEnhancementApproval,
    db: Session = Depends(get_db),
    current_user: dict = Depends(get_current_user),
):
    """ê²Œì‹œê¸€ ì„¤ëª… í–¥ìƒ ìŠ¹ì¸/ê±°ë¶€"""
    try:
        enhancement = content_service.approve_enhancement(
            db=db,
            enhancement_id=approval.enhancement_id,
            approved=approval.approved,
            improvement_notes=approval.improvement_notes,
        )
        return enhancement
    except ValueError as e:
        raise HTTPException(status_code=status.HTTP_404_NOT_FOUND, detail=str(e))
    except Exception as e:
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            detail=f"Approval failed: {str(e)}",
        )


@router.get("/history", response_model=ContentEnhancementList)
async def get_enhancement_history(
    page: int = 1,
    page_size: int = 10,
    db: Session = Depends(get_db),
    current_user: dict = Depends(get_current_user),
):
    """ì‚¬ìš©ìì˜ ê²Œì‹œê¸€ í–¥ìƒ ì´ë ¥ ì¡°íšŒ"""
    try:
        user_id = current_user.get("sub")
        if not user_id:
            raise HTTPException(
                status_code=status.HTTP_401_UNAUTHORIZED,
                detail="User authentication required",
            )

        result = content_service.get_user_enhancements(
            db=db, user_id=user_id, page=page, page_size=page_size
        )
        return result
    except Exception as e:
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            detail=f"Failed to get enhancement history: {str(e)}",
        )


@router.get("/{enhancement_id}", response_model=ContentEnhancementResponse)
async def get_enhancement(
    enhancement_id: str,
    db: Session = Depends(get_db),
    current_user: dict = Depends(get_current_user),
):
    """íŠ¹ì • ê²Œì‹œê¸€ í–¥ìƒ ë‚´ì—­ ì¡°íšŒ"""
    user_id = current_user.get("sub")
    if not user_id:
        raise HTTPException(
            status_code=status.HTTP_401_UNAUTHORIZED,
            detail="User authentication required",
        )

    from app.models.content_enhancement import ContentEnhancement

    enhancement = (
        db.query(ContentEnhancement)
        .filter(
            ContentEnhancement.enhancement_id == enhancement_id,
            ContentEnhancement.user_id == user_id,
        )
        .first()
    )

    if not enhancement:
        raise HTTPException(
            status_code=status.HTTP_404_NOT_FOUND, detail="Enhancement not found"
        )

    return enhancement


class InfluencerToneRequest(BaseModel):
    """ì¸í”Œë£¨ì–¸ì„œ ë§íˆ¬ ë³€í™˜ ìš”ì²­"""

    influencer_id: str
    content: str
    platform: str = "instagram"


class InfluencerToneResponse(BaseModel):
    """ì¸í”Œë£¨ì–¸ì„œ ë§íˆ¬ ë³€í™˜ ì‘ë‹µ"""

    original_content: str
    transformed_content: str
    influencer_name: str
    model_repo: str
    success: bool
    error_message: Optional[str] = None
    hashtags: List[str]


@router.post("/influencer-tone", response_model=InfluencerToneResponse)
async def transform_with_influencer_tone(
    request: InfluencerToneRequest,
    db: Session = Depends(get_db),
    current_user: dict = Depends(get_current_user),
):
    """ì„ íƒí•œ ì¸í”Œë£¨ì–¸ì„œì˜ ë§íˆ¬ë¡œ ì½˜í…ì¸  ë³€í™˜ (OpenAI ë°©ì‹ê³¼ ë™ì¼í•˜ê²Œ ë³¸ë¬¸+í•´ì‹œíƒœê·¸ ë¶„ë¦¬)"""
    try:
        user_id = current_user.get("sub")
        logger.info(f"ì¸í”Œë£¨ì–¸ì„œ ë§íˆ¬ ë³€í™˜ ìš”ì²­: {request.influencer_id}")
        user = db.query(User).filter(User.user_id == user_id).first()
        if not user:
            raise HTTPException(status_code=404, detail="User not found")
        user_group_ids = [team.group_id for team in user.teams]
        ai_influencer = (
            db.query(AIInfluencer)
            .filter(AIInfluencer.influencer_id == request.influencer_id)
            .first()
        )
        if not ai_influencer:
            raise HTTPException(
                status_code=404, detail="AI ì¸í”Œë£¨ì–¸ì„œë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
            )
        if (
            ai_influencer.group_id not in user_group_ids
            and ai_influencer.user_id != user_id
        ):
            raise HTTPException(
                status_code=403, detail="í•´ë‹¹ ëª¨ë¸ì— ëŒ€í•œ ì ‘ê·¼ ê¶Œí•œì´ ì—†ìŠµë‹ˆë‹¤."
            )
        if not ai_influencer.influencer_model_repo:
            raise HTTPException(
                status_code=400,
                detail="ì¸í”Œë£¨ì–¸ì„œì˜ í—ˆê¹…í˜ì´ìŠ¤ ëª¨ë¸ì´ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.",
            )
        if not ai_influencer.hf_manage_id:
            raise HTTPException(
                status_code=400, detail="í—ˆê¹…í˜ì´ìŠ¤ í† í°ì´ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤."
            )
        hf_token = (
            db.query(HFTokenManage)
            .filter(HFTokenManage.hf_manage_id == ai_influencer.hf_manage_id)
            .first()
        )
        if not hf_token:
            raise HTTPException(
                status_code=404, detail="í—ˆê¹…í˜ì´ìŠ¤ í† í°ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
            )
        encrypted_token_value = getattr(hf_token, "hf_token_value", None)
        if not encrypted_token_value:
            raise HTTPException(status_code=400, detail="í† í° ê°’ì´ ì—†ìŠµë‹ˆë‹¤.")
        decrypted_token = decrypt_sensitive_data(encrypted_token_value)

        # OpenAI ë°©ì‹ì²˜ëŸ¼ í”„ë¡¬í”„íŠ¸ ëª…í™•í™”
        system_prompt = f"""
ë„ˆëŠ” {ai_influencer.influencer_name}ë¼ëŠ” AI ì¸í”Œë£¨ì–¸ì„œì•¼.
ì„¤ëª…: {ai_influencer.influencer_description or "ì¹œê·¼í•˜ê³  í™œë°œí•œ ì¸í”Œë£¨ì–¸ì„œ"}
ì„±ê²©: {getattr(ai_influencer, 'influencer_personality', 'ì¹œê·¼í•˜ê³  í™œë°œí•œ')}

ë‹¤ìŒ ê²Œì‹œê¸€ ì„¤ëª…ì„ {ai_influencer.influencer_name}ì˜ ë§íˆ¬ì™€ ìŠ¤íƒ€ì¼ë¡œ ìì—°ìŠ¤ëŸ½ê²Œ ë³€í™˜í•´ì¤˜.
- ë³¸ë¬¸ì€ ìì—°ìŠ¤ëŸ¬ìš´ ë¬¸ì¥ìœ¼ë¡œ ì‘ì„±
- ë§ˆì§€ë§‰ì— í•´ì‹œíƒœê·¸ 5~10ê°œë§Œ #ìœ¼ë¡œ ì‹œì‘í•´ì„œ í•œ ì¤„ë¡œ ì¶”ê°€ (## ê¸ˆì§€)
- ë‹µë³€ í˜•ì‹ì´ ì•„ë‹ˆë¼, ë³€í™˜ëœ ì„¤ëª…(ë³¸ë¬¸)ê³¼ í•´ì‹œíƒœê·¸ë§Œ ë°˜í™˜í•  ê²ƒ

[ê²Œì‹œê¸€ ì„¤ëª…]
{request.content}
"""

        try:
            transformed_content = await generate_response_with_huggingface_model(
                str(ai_influencer.influencer_model_repo),
                system_prompt,
                request.content,
                str(ai_influencer.influencer_name),
                decrypted_token,
            )

            # ë³¸ë¬¸ê³¼ í•´ì‹œíƒœê·¸ ë¶„ë¦¬ (OpenAI ë°©ì‹ê³¼ ë™ì¼)
            import re

            # í•´ì‹œíƒœê·¸ í•œ ì¤„ ì¶”ì¶œ (ë§ˆì§€ë§‰ ì¤„)
            lines = transformed_content.strip().split("\n")
            hashtags_line = ""
            for i in range(len(lines) - 1, -1, -1):
                if re.search(r"#\w+", lines[i]):
                    hashtags_line = lines[i]
                    lines = lines[:i]
                    break
            main_text = "\n".join(lines).strip()
            # í•´ì‹œíƒœê·¸ ë¦¬ìŠ¤íŠ¸ ì¶”ì¶œ
            hashtags = re.findall(r"#\w+", hashtags_line)
            # í›„ì²˜ë¦¬: ## â†’ #, # ì—†ìœ¼ë©´ # ì¶”ê°€
            hashtags = [
                ("#" + tag[2:]) if tag.startswith("##") else ("#" + tag.lstrip("#"))
                for tag in hashtags
            ]

            return InfluencerToneResponse(
                original_content=request.content,
                transformed_content=main_text,
                influencer_name=str(ai_influencer.influencer_name),
                model_repo=str(ai_influencer.influencer_model_repo),
                success=True,
                error_message=None,
                hashtags=hashtags,
            )
        except Exception as e:
            logger.error(f"í—ˆê¹…í˜ì´ìŠ¤ ëª¨ë¸ ë³€í™˜ ì‹¤íŒ¨: {str(e)}")
            return InfluencerToneResponse(
                original_content=request.content,
                transformed_content=request.content,  # ì‹¤íŒ¨ ì‹œ ì›ë³¸ ë°˜í™˜
                influencer_name=str(ai_influencer.influencer_name),
                model_repo=str(ai_influencer.influencer_model_repo),
                success=False,
                error_message=f"ëª¨ë¸ ë³€í™˜ ì‹¤íŒ¨: {str(e)}",
                hashtags=[],
            )
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"ì¸í”Œë£¨ì–¸ì„œ ë§íˆ¬ ë³€í™˜ ì‹¤íŒ¨: {str(e)}")
        raise HTTPException(
            status_code=500, detail=f"ì¸í”Œë£¨ì–¸ì„œ ë§íˆ¬ ë³€í™˜ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤: {str(e)}"
        )


async def generate_response_with_huggingface_model(
    model_repo: str,
    system_message: str,
    user_message: str,
    influencer_name: str,
    hf_token: str = None,
) -> str:
    """í—ˆê¹…í˜ì´ìŠ¤ ëª¨ë¸ì„ ì‚¬ìš©í•œ ì‘ë‹µ ìƒì„±"""
    try:
        logger.info(f"í—ˆê¹…í˜ì´ìŠ¤ ëª¨ë¸ ë³€í™˜ ì‹œì‘: {model_repo}")

        # 1. ë² ì´ìŠ¤ ëª¨ë¸ ë¡œë“œ
        base_model_name = "LGAI-EXAONE/EXAONE-3.5-2.4B-Instruct"
        logger.info(f"ë² ì´ìŠ¤ ëª¨ë¸ ë¡œë”©: {base_model_name}")

        device = "cuda" if torch.cuda.is_available() else "cpu"
        logger.info(f"ì‚¬ìš© ë””ë°”ì´ìŠ¤: {device}")

        if device == "cuda":
            torch.cuda.empty_cache()

        tokenizer = AutoTokenizer.from_pretrained(
            base_model_name, trust_remote_code=True, token=hf_token
        )

        base_model = AutoModelForCausalLM.from_pretrained(
            base_model_name,
            trust_remote_code=True,
            token=hf_token,
            device_map="auto" if device == "cuda" else None,
            torch_dtype=torch.float16 if device == "cuda" else torch.float32,
        )

        # 2. LoRA ì–´ëŒ‘í„° ë¡œë“œ
        logger.info(f"LoRA ì–´ëŒ‘í„° ë¡œë”©: {model_repo}")
        model = PeftModel.from_pretrained(base_model, model_repo, token=hf_token)

        # íŒ¨ë”© í† í° ì„¤ì •
        if tokenizer.pad_token is None:
            tokenizer.pad_token = tokenizer.eos_token

        # 3. ë©”ì‹œì§€ êµ¬ì„± ë° í”„ë¡¬í”„íŠ¸ ìƒì„±
        messages = [
            {"role": "system", "content": system_message},
            {"role": "user", "content": user_message},
        ]

        try:
            if (
                hasattr(tokenizer, "apply_chat_template")
                and tokenizer.chat_template is not None
            ):
                prompt = tokenizer.apply_chat_template(
                    messages, tokenize=False, add_generation_prompt=True
                )
            else:
                prompt = (
                    f"{system_message}\n\nì‚¬ìš©ì: {user_message}\n\n{influencer_name}:"
                )
        except Exception as e:
            logger.warning(f"Chat template ì ìš© ì‹¤íŒ¨, ê¸°ë³¸ í˜•ì‹ ì‚¬ìš©: {e}")
            prompt = f"{system_message}\n\nì‚¬ìš©ì: {user_message}\n\n{influencer_name}:"

        # 4. í† í°í™” ë° ìƒì„±
        inputs = tokenizer(prompt, return_tensors="pt", truncation=True, max_length=512)

        if device == "cuda":
            inputs = {k: v.to(model.device) for k, v in inputs.items()}

        with torch.no_grad():
            outputs = model.generate(
                **inputs,
                max_new_tokens=200,
                temperature=0.7,
                do_sample=True,
                pad_token_id=tokenizer.eos_token_id,
                eos_token_id=tokenizer.eos_token_id,
            )

        # 5. ì‘ë‹µ ë””ì½”ë”© ë° í›„ì²˜ë¦¬
        generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)

        if (
            hasattr(tokenizer, "apply_chat_template")
            and tokenizer.chat_template is not None
        ):
            input_length = len(
                tokenizer.decode(inputs["input_ids"][0], skip_special_tokens=True)
            )
            if len(generated_text) > input_length:
                response = generated_text[input_length:].strip()
            else:
                response = generated_text.strip()
        else:
            response = generated_text.split(f"{influencer_name}:")[-1].strip()

        # ì‘ë‹µ í›„ì²˜ë¦¬
        response = response.strip()
        response = response.replace("<|im_end|>", "").replace("<|endoftext|>", "")
        response = response.replace("[/INST]", "").replace("</s>", "")

        # ë„ˆë¬´ ê¸¸ë©´ ìë¥´ê¸°
        if len(response) > 500:
            response = response[:500] + "..."

        # ë¹ˆ ì‘ë‹µì¸ ê²½ìš° ê¸°ë³¸ ì‘ë‹µ ì œê³µ
        if not response.strip():
            response = f"ì•ˆë…•í•˜ì„¸ìš”! {influencer_name}ì…ë‹ˆë‹¤! ğŸ˜Š {user_message}"

        logger.info(f"í—ˆê¹…í˜ì´ìŠ¤ ëª¨ë¸ ë³€í™˜ ì™„ë£Œ")
        return response

    except Exception as e:
        logger.error(f"í—ˆê¹…í˜ì´ìŠ¤ ëª¨ë¸ ë³€í™˜ ì‹¤íŒ¨: {str(e)}")
        raise e


class FullEnhancementRequest(BaseModel):
    content: str
    influencer_id: str
    platform: str = "instagram"


class FullEnhancementResponse(BaseModel):
    enhancement_id: str
    original_content: str
    enhanced_content: str
    influencer_name: str
    influencer_transformed_content: str
    influencer_hashtags: list
