"""
RAG (Retrieval-Augmented Generation) API ì—”ë“œí¬ì¸íŠ¸
VLLM GPU ë©”ëª¨ë¦¬ ê¸°ë°˜ í†µí•© RAG API
"""

from typing import Dict, List, Optional
from fastapi import APIRouter, Depends, File, Form, HTTPException, UploadFile
from pydantic import BaseModel, Field
from sqlalchemy.orm import Session
import tempfile
import os
import logging
from datetime import datetime

from app.core.config import settings
from app.database import get_db
from app.services.rag_service import get_rag_service
from app.services.rag_processor import process_with_rag

logger = logging.getLogger(__name__)

router = APIRouter()


# ==================== Response Models ====================


class DocumentUploadResponse(BaseModel):
    """ë¬¸ì„œ ì—…ë¡œë“œ ì‘ë‹µ ëª¨ë¸"""

    status: str
    message: str
    pipeline_info: Optional[Dict] = None


class ChatRequest(BaseModel):
    """ì±„íŒ… ìš”ì²­ ìŠ¤í‚¤ë§ˆ"""

    message: str = Field(..., description="ì‚¬ìš©ì ë©”ì‹œì§€")
    influencer_id: Optional[str] = Field(None, description="ì¸í”Œë£¨ì–¸ì„œ ID")
    similarity_threshold: float = Field(
        0.7, description="ìœ ì‚¬ë„ ì„ê³„ê°’"
    )  # 0.5ì—ì„œ 0.7ë¡œ ë†’ì„
    max_tokens: int = Field(1024, description="ìµœëŒ€ í† í° ìˆ˜")
    include_sources: Optional[bool] = Field(True, description="ì†ŒìŠ¤ í¬í•¨ ì—¬ë¶€")


class ChatResponse(BaseModel):
    """OpenAI ê¸°ë°˜ ì±„íŒ… ì‘ë‹µ"""

    response: str
    sources: List[Dict]
    query: str
    search_results: List[Dict]


class SearchResult(BaseModel):
    """ê²€ìƒ‰ ê²°ê³¼ ëª¨ë¸"""

    id: str
    text: str
    score: float
    metadata: Dict


class VectorStatsResponse(BaseModel):
    """ë²¡í„° í†µê³„ ì‘ë‹µ"""

    stats: Dict
    health: Dict


# ==================== Utility Functions ====================


async def _check_vllm_server_with_retry(max_retries: int = 3) -> bool:
    """VLLM ì„œë²„ ìƒíƒœ í™•ì¸ (ì¬ì‹œë„ í¬í•¨)"""
    import httpx

    vllm_url = settings.VLLM_BASE_URL or "http://localhost:8001"

    for attempt in range(max_retries):
        try:
            async with httpx.AsyncClient(timeout=5.0) as client:
                response = await client.get(f"{vllm_url}/health")
                if response.status_code == 200:
                    logger.info("âœ… VLLM ì„œë²„ ì—°ê²° í™•ì¸ë¨")
                    return True

        except Exception as e:
            logger.warning(
                f"VLLM ì„œë²„ í™•ì¸ ì‹¤íŒ¨ (ì‹œë„ {attempt + 1}/{max_retries}): {e}"
            )

        if attempt < max_retries - 1:
            import asyncio

            await asyncio.sleep(1)  # 1ì´ˆ ëŒ€ê¸°

    return False


# ==================== Document Upload Endpoints ====================


@router.post("/upload_document_gpu", response_model=DocumentUploadResponse)
async def upload_document_gpu(
    file: UploadFile = File(..., description="PDF íŒŒì¼"),
    system_message: Optional[str] = Form(
        "ë‹¹ì‹ ì€ ì œê³µëœ ì°¸ê³  ë¬¸ì„œì˜ ì •í™•í•œ ì •ë³´ì™€ ì‚¬ì‹¤ì„ ë°”íƒ•ìœ¼ë¡œ ë‹µë³€í•˜ëŠ” AI ì–´ì‹œìŠ¤í„´íŠ¸ì…ë‹ˆë‹¤.",
        description="ì‹œìŠ¤í…œ ë©”ì‹œì§€",
    ),
    influencer_name: Optional[str] = Form("AI", description="AI ìºë¦­í„° ì´ë¦„"),
    db: Session = Depends(get_db),
):
    """PDF ë¬¸ì„œë¥¼ VLLM GPU ë²¡í„° ìŠ¤í† ì–´ì— ì—…ë¡œë“œ (ìë™ ë²¡í„°DB ì´ˆê¸°í™” í¬í•¨)"""
    try:
        logger.info(
            f"ğŸ“¥ GPU ë¬¸ì„œ ì—…ë¡œë“œ ì‹œì‘ (ë²¡í„°DB ìë™ ì´ˆê¸°í™” í¬í•¨): {file.filename}"
        )

        # íŒŒì¼ ê²€ì¦
        if not file.filename.lower().endswith(".pdf"):
            raise HTTPException(status_code=400, detail="PDF íŒŒì¼ë§Œ ì—…ë¡œë“œ ê°€ëŠ¥í•©ë‹ˆë‹¤.")

        if file.size > 10 * 1024 * 1024:  # 10MB ì œí•œ
            raise HTTPException(
                status_code=400, detail="íŒŒì¼ í¬ê¸°ëŠ” 10MBë¥¼ ì´ˆê³¼í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
            )

        logger.info(f"âœ… íŒŒì¼ ê²€ì¦ í†µê³¼: {file.filename} ({file.size} bytes)")

        # ì„ì‹œ íŒŒì¼ë¡œ ì €ì¥
        with tempfile.NamedTemporaryFile(delete=False, suffix=".pdf") as temp_file:
            content = await file.read()
            temp_file.write(content)
            temp_file_path = temp_file.name
            logger.info(f"ğŸ“ ì„ì‹œ íŒŒì¼ ì €ì¥: {temp_file_path}")

        try:
            # RAG ì„œë¹„ìŠ¤ë¡œ ë¬¸ì„œ ì²˜ë¦¬
            logger.info("ğŸ”„ RAG ì„œë¹„ìŠ¤ ë¬¸ì„œ ì²˜ë¦¬ ì‹œì‘")
            rag_service = get_rag_service()
            qa_pairs = await rag_service.document_processor.process_pdf(temp_file_path)

            logger.info(f"ğŸ“„ QA ìŒ ìƒì„± ì™„ë£Œ: {len(qa_pairs)}ê°œ")

            if not qa_pairs:
                raise HTTPException(
                    status_code=500, detail="QA ìŒ ìƒì„±ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤."
                )

            # vLLM ì„œë²„ì˜ Milvus ë²¡í„°DBì— ì €ì¥
            source_file = file.filename
            logger.info("ğŸ’¾ vLLM ì„œë²„ ë²¡í„°DB ì €ì¥ ì‹œì‘")

            import httpx

            # QA ìŒì„ DocumentChunk í˜•ì‹ìœ¼ë¡œ ë³€í™˜
            documents = []
            for i, qa in enumerate(qa_pairs):
                documents.append(
                    {
                        "id": f"chunk_{i}",
                        "text": qa["answer"],  # ë‹µë³€ì„ í…ìŠ¤íŠ¸ë¡œ ì‚¬ìš©
                        "metadata": {
                            "question": qa["question"],
                            "chunk_id": qa["chunk_id"],
                            "source": source_file,
                            "page": qa.get("page", 1),
                        },
                    }
                )

            # vLLM ì„œë²„ì˜ ë²¡í„°DB API í˜¸ì¶œ
            vllm_url = settings.VLLM_BASE_URL or "http://localhost:8001"
            logger.info(f"ğŸ”— vLLM ì„œë²„ URL: {vllm_url}")

            async with httpx.AsyncClient(timeout=60.0) as client:
                # 1. ë²¡í„°DB ì´ˆê¸°í™”
                logger.info("ğŸ—‘ï¸ ë²¡í„°DB ì´ˆê¸°í™” ì‹œì‘")
                clear_response = await client.delete(f"{vllm_url}/vector-db/clear")

                if clear_response.status_code == 200:
                    logger.info("âœ… ë²¡í„°DB ì´ˆê¸°í™” ì™„ë£Œ")
                else:
                    logger.warning(
                        f"âš ï¸ ë²¡í„°DB ì´ˆê¸°í™” ì‹¤íŒ¨: {clear_response.status_code}"
                    )
                    # ì´ˆê¸°í™” ì‹¤íŒ¨í•´ë„ ê³„ì† ì§„í–‰

                # 2. ë¬¸ì„œ ì €ì¥
                logger.info(f"ğŸ“Š ì €ì¥í•  ë¬¸ì„œ ìˆ˜: {len(documents)}ê°œ")
                logger.info(
                    f"ğŸ“¤ vLLM ì„œë²„ë¡œ ìš”ì²­ ì „ì†¡: {vllm_url}/vector-db/embed-and-store"
                )
                response = await client.post(
                    f"{vllm_url}/vector-db/embed-and-store",
                    json={"documents": documents},
                )

                if response.status_code == 200:
                    result = response.json()
                    logger.info(f"âœ… vLLM ì„œë²„ ë²¡í„°DB ì €ì¥ ì™„ë£Œ: {result}")
                else:
                    logger.error(
                        f"âŒ vLLM ì„œë²„ ë²¡í„°DB ì €ì¥ ì‹¤íŒ¨: {response.status_code}"
                    )
                    logger.error(f"âŒ ì‘ë‹µ ë‚´ìš©: {response.text}")
                    raise HTTPException(
                        status_code=500, detail="vLLM ì„œë²„ ë²¡í„°DB ì €ì¥ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤."
                    )

                    # S3ì— PDF íŒŒì¼ ì—…ë¡œë“œ ë° ë°ì´í„°ë² ì´ìŠ¤ì— ì €ì¥
            documents_id = None
            try:
                from app.services.s3_service import get_s3_service
                from app.services.rag_document_service import get_rag_document_service

                s3_service = get_s3_service()
                rag_document_service = get_rag_document_service()

                if s3_service.is_available():
                    # S3 í‚¤ ìƒì„± (documents/YYYY-MM-DD/HH-MM-SS_filename.pdf)
                    timestamp = datetime.now().strftime("%Y-%m-%d_%H-%M-%S")
                    s3_key = f"documents/{timestamp}/{file.filename}"

                    # S3ì— ì—…ë¡œë“œ
                    s3_url = s3_service.upload_file(
                        temp_file_path, s3_key, content_type="application/pdf"
                    )

                    if s3_url:
                        logger.info(f"âœ… S3 ì—…ë¡œë“œ ì„±ê³µ: {s3_url}")

                        # ë°ì´í„°ë² ì´ìŠ¤ì— ë¬¸ì„œ ì •ë³´ ì €ì¥
                        documents_id = rag_document_service.save_document_info(
                            documents_name=file.filename,
                            file_size=file.size,
                            s3_url=s3_url,
                            db=db,
                        )

                        if documents_id:
                            logger.info(f"âœ… ë¬¸ì„œ ì •ë³´ ì €ì¥ ì™„ë£Œ: {documents_id}")

                            # ê¸°ì¡´ ë²¡í„°í™”ëœ ë¬¸ì„œë“¤ì˜ ìƒíƒœë¥¼ 0ìœ¼ë¡œ ë³€ê²½
                            rag_document_service.reset_all_vectorization_status(db=db)
                            logger.info("âœ… ê¸°ì¡´ ë²¡í„°í™” ìƒíƒœ ì´ˆê¸°í™” ì™„ë£Œ")

                            # ìƒˆ ë¬¸ì„œì˜ ë²¡í„°í™” ìƒíƒœë¥¼ 1ë¡œ ì„¤ì •
                            rag_document_service.update_vectorization_status(
                                documents_id=documents_id, db=db, is_vectorized=1
                            )
                            logger.info(
                                f"âœ… ìƒˆ ë¬¸ì„œ ë²¡í„°í™” ìƒíƒœ ì„¤ì • ì™„ë£Œ: {documents_id}"
                            )
                        else:
                            logger.error("âŒ ë¬¸ì„œ ì •ë³´ ì €ì¥ ì‹¤íŒ¨")
                            raise HTTPException(
                                status_code=500,
                                detail="ë°ì´í„°ë² ì´ìŠ¤ ì €ì¥ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤.",
                            )
                    else:
                        logger.warning("âš ï¸ S3 ì—…ë¡œë“œ ì‹¤íŒ¨")
                        raise HTTPException(
                            status_code=500, detail="S3 ì—…ë¡œë“œì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤."
                        )
                else:
                    logger.warning("âš ï¸ S3 ì„œë¹„ìŠ¤ë¥¼ ì‚¬ìš©í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤")
                    raise HTTPException(
                        status_code=500, detail="S3 ì„œë¹„ìŠ¤ë¥¼ ì‚¬ìš©í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
                    )
            except Exception as storage_error:
                logger.error(f"âŒ ì €ì¥ì†Œ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜: {storage_error}")
                raise HTTPException(
                    status_code=500, detail=f"ì €ì¥ì†Œ ì²˜ë¦¬ ì‹¤íŒ¨: {str(storage_error)}"
                )

            return DocumentUploadResponse(
                status="success",
                message=f"ë²¡í„°DBê°€ ì´ˆê¸°í™”ë˜ê³  ë¬¸ì„œê°€ vLLM ì„œë²„ì˜ Milvus ë²¡í„°DBì— ì„±ê³µì ìœ¼ë¡œ ì—…ë¡œë“œë˜ì—ˆìŠµë‹ˆë‹¤.",
                pipeline_info={"qa_count": len(qa_pairs), "source_file": source_file},
            )

        finally:
            # ì„ì‹œ íŒŒì¼ ì •ë¦¬
            if os.path.exists(temp_file_path):
                os.unlink(temp_file_path)
                logger.info(f"ğŸ—‘ï¸ ì„ì‹œ íŒŒì¼ ì‚­ì œ: {temp_file_path}")

    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"âŒ GPU ë¬¸ì„œ ì—…ë¡œë“œ ì‹¤íŒ¨: {e}")
        import traceback

        logger.error(f"âŒ ìƒì„¸ ì˜¤ë¥˜: {traceback.format_exc()}")
        raise HTTPException(status_code=500, detail=f"ë¬¸ì„œ ì—…ë¡œë“œ ì‹¤íŒ¨: {str(e)}")


# ==================== Chat Endpoints ====================


@router.post("/chat_gpu", response_model=ChatResponse)
async def chat_gpu(chat_request: ChatRequest, db: Session = Depends(get_db)):
    """VLLM GPU ë²¡í„° ê²€ìƒ‰ì„ ì‚¬ìš©í•œ ì±„íŒ… (ì¸í”Œë£¨ì–¸ì„œ ëª¨ë¸ ì§€ì›)"""
    try:
        query = chat_request.message
        influencer_id = chat_request.influencer_id
        top_k = 5  # Default value from ChatRequest
        similarity_threshold = chat_request.similarity_threshold
        include_sources = True  # Default value from ChatRequest
        max_tokens = chat_request.max_tokens

        logger.info(
            f"ğŸ” RAG GPU ì±„íŒ… ì‹œì‘: query='{query}', influencer_id={influencer_id}, top_k={top_k}, similarity_threshold={similarity_threshold}"
        )

        # ì¸í”Œë£¨ì–¸ì„œ ì •ë³´ ì¡°íšŒ (influencer_idê°€ ì œê³µëœ ê²½ìš°)
        influencer = None
        influencer_name = "AI ì–´ì‹œìŠ¤í„´íŠ¸"
        system_message = "ë‹¹ì‹ ì€ ì œê³µëœ ì°¸ê³  ë¬¸ì„œì˜ ì •í™•í•œ ì •ë³´ì™€ ì‚¬ì‹¤ì„ ë°”íƒ•ìœ¼ë¡œ ë‹µë³€í•˜ëŠ” AI ì–´ì‹œìŠ¤í„´íŠ¸ì…ë‹ˆë‹¤."
        
        if influencer_id:
            from app.models.influencer import AIInfluencer
            influencer = (
                db.query(AIInfluencer)
                .filter(AIInfluencer.influencer_id == influencer_id)
                .first()
            )
            
            if influencer:
                logger.info(f"âœ… ì¸í”Œë£¨ì–¸ì„œ ì¡°íšŒ ì„±ê³µ: {influencer.influencer_name}, model_repo={influencer.influencer_model_repo}")
                influencer_name = str(influencer.influencer_name)
                if influencer.system_prompt:
                    system_message = str(influencer.system_prompt)
            else:
                logger.warning(f"âš ï¸ ì¸í”Œë£¨ì–¸ì„œë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŒ: {influencer_id}")

        # RAG ì„œë¹„ìŠ¤ë¡œ ë¬¸ì„œ ê²€ìƒ‰
        rag_service = get_rag_service()
        search_results = await rag_service.vector_store.search_similar(
            query=query,
            top_k=top_k,
            score_threshold=similarity_threshold
        )

        if not search_results:
            logger.info("âŒ ì„ê³„ê°’ì„ ë„˜ëŠ” ë¬¸ì„œê°€ ì—†ìŒ")
            return ChatResponse(
                response="",
                sources=[],
                query=query,
                search_results=[],
            )

        logger.info(f"âœ… ê²€ìƒ‰ ì™„ë£Œ: {len(search_results)}ê°œ ë¬¸ì„œ ë°œê²¬")

        # ê°€ì¥ ê´€ë ¨ì„± ë†’ì€ ë¬¸ì„œ ì‚¬ìš©
        best_document = search_results[0]
        context = best_document.get("text", "")

        # RunPodë¥¼ ì‚¬ìš©í•˜ì—¬ ì‘ë‹µ ìƒì„± (ì¸í”Œë£¨ì–¸ì„œ ëª¨ë¸ ì‚¬ìš©)
        if influencer and influencer.influencer_model_repo:
            try:
                from app.services.runpod_manager import get_vllm_manager
                from app.services.hf_token_resolver import get_token_for_influencer
                
                # HF í† í° ê°€ì ¸ì˜¤ê¸°
                hf_token = None
                hf_username = None
                try:
                    hf_token, hf_username = await get_token_for_influencer(influencer, db)
                    if hf_token:
                        logger.info(f"ğŸ”‘ HF í† í° ì‚¬ìš© (user: {hf_username})")
                except Exception as e:
                    logger.warning(f"âš ï¸ HF í† í° ê°€ì ¸ì˜¤ê¸° ì‹¤íŒ¨: {e}")
                
                # í”„ë¡¬í”„íŠ¸ êµ¬ì„±
                prompt = f"""ë‹¤ìŒ ë¬¸ì„œ ë‚´ìš©ì„ ì°¸ê³ í•˜ì—¬ ì‚¬ìš©ìì˜ ì§ˆë¬¸ì— ë‹µë³€í•´ì£¼ì„¸ìš”.

ë¬¸ì„œ ë‚´ìš©:
{context}

ì‚¬ìš©ì ì§ˆë¬¸: {query}

ë‹µë³€:"""
                
                # ë©”ì‹œì§€ êµ¬ì„±
                messages = []
                if system_message:
                    messages.append({"role": "system", "content": system_message})
                messages.append({"role": "user", "content": prompt})
                
                # RunPod vLLM workerì— ë§ëŠ” í˜ì´ë¡œë“œ êµ¬ì„±
                payload = {
                    "input": {
                        "messages": messages,
                        "max_tokens": max_tokens,
                        "temperature": 0.7,
                        "stream": False,
                        "lora_adapter": str(influencer.influencer_id),
                        "hf_repo": str(influencer.influencer_model_repo),
                        "hf_token": hf_token
                    }
                }
                
                # vLLM ë§¤ë‹ˆì €ë¡œ í…ìŠ¤íŠ¸ ìƒì„±
                vllm_manager = get_vllm_manager()
                result = await vllm_manager.runsync(payload)
                
                # ì‘ë‹µ ì²˜ë¦¬
                if result.get("status") == "completed" and result.get("output"):
                    output = result["output"]
                    if output.get("status") == "success":
                        response = output.get("generated_text", "")
                    else:
                        response = "ì‘ë‹µ ìƒì„±ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤."
                else:
                    response = "ì‘ë‹µ ìƒì„±ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤."
                    
                logger.info(f"âœ… RunPod ì‘ë‹µ ìƒì„± ì„±ê³µ")
                
            except Exception as e:
                logger.error(f"âŒ RunPod ì‘ë‹µ ìƒì„± ì‹¤íŒ¨: {e}")
                # OpenAIë¡œ í´ë°±
                from app.services.openai_service_simple import OpenAIService
                openai_service = OpenAIService()
                response = await openai_service.openai_tool_selection(
                    user_prompt=prompt,
                    system_prompt=system_message,
                )
        else:
            # ì¸í”Œë£¨ì–¸ì„œ ëª¨ë¸ì´ ì—†ëŠ” ê²½ìš° OpenAI ì‚¬ìš©
            from app.services.openai_service_simple import OpenAIService
            openai_service = OpenAIService()
            
            prompt = f"""ë‹¤ìŒ ë¬¸ì„œ ë‚´ìš©ì„ ì°¸ê³ í•˜ì—¬ ì‚¬ìš©ìì˜ ì§ˆë¬¸ì— ë‹µë³€í•´ì£¼ì„¸ìš”.

ë¬¸ì„œ ë‚´ìš©:
{context}

ì‚¬ìš©ì ì§ˆë¬¸: {query}

ë‹µë³€ ìš”êµ¬ì‚¬í•­:
- ë¬¸ì„œ ë‚´ìš©ì„ ë°”íƒ•ìœ¼ë¡œ ì •í™•í•˜ê³  ëª…í™•í•˜ê²Œ ë‹µë³€
- ë¬¸ì„œì— ì—†ëŠ” ì •ë³´ëŠ” ì¶”ê°€í•˜ì§€ ì•ŠìŒ
- ìì—°ìŠ¤ëŸ½ê³  ì´í•´í•˜ê¸° ì‰½ê²Œ ì‘ì„±"""
            
            response = await openai_service.openai_tool_selection(
                user_prompt=prompt,
                system_prompt=system_message,
            )

        # ê²€ìƒ‰ ê²°ê³¼ë¥¼ ë”•ì…”ë„ˆë¦¬ë¡œ ë³€í™˜
        search_results_dict = []
        sources = []
        for result in search_results:
            result_dict = {
                "text": result.get("text", ""),
                "score": result.get("score", 0),
                "metadata": result.get("metadata", {}),
            }
            search_results_dict.append(result_dict)
            sources.append(result_dict)

        logger.info(f"âœ… RAG ì‘ë‹µ ìƒì„± ì™„ë£Œ: {len(sources)}ê°œ ë¬¸ì„œ ì°¸ì¡°")
        return ChatResponse(
            response=response,
            sources=sources,
            query=query,
            search_results=search_results_dict,
        )

    except Exception as e:
        logger.error(f"âŒ RAG GPU ì±„íŒ… ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
        raise HTTPException(status_code=500, detail=str(e))


# ==================== Vector Search Endpoints ====================


@router.post("/embed_and_search", response_model=List[SearchResult])
async def embed_and_search(
    query: str = Form(..., description="ê²€ìƒ‰ ì¿¼ë¦¬"),
    top_k: int = Form(5, description="ê²€ìƒ‰í•  ìƒìœ„ kê°œ"),
    score_threshold: float = Form(
        0.7, description="ìœ ì‚¬ë„ ì„ê³„ê°’"
    ),  # 0.5ì—ì„œ 0.7ë¡œ ë†’ì„
):
    """ì„ë² ë”© ìƒì„± ë° ë²¡í„° ê²€ìƒ‰"""
    try:
        logger.info(
            f"ğŸ” ì„ë² ë”© ê²€ìƒ‰ ì‹œì‘: query='{query}', top_k={top_k}, score_threshold={score_threshold}"
        )

        # RAG ì„œë¹„ìŠ¤ ê°€ì ¸ì˜¤ê¸°
        rag_service = get_rag_service()

        # VLLM GPU ë©”ëª¨ë¦¬ì—ì„œ ê²€ìƒ‰
        search_results = await rag_service.vector_store.search_similar(query, top_k)

        if not search_results:
            return []

        # ê²°ê³¼ë¥¼ SearchResult í˜•íƒœë¡œ ë³€í™˜
        results = []
        for result in search_results:
            if result["score"] >= score_threshold:
                results.append(
                    {
                        "id": result["metadata"].get("chunk_id", "unknown"),
                        "text": result["text"],
                        "score": result["score"],
                        "metadata": result["metadata"],
                    }
                )

        logger.info(f"âœ… ê²€ìƒ‰ ì™„ë£Œ: {len(results)}ê°œ ê²°ê³¼")
        return results

    except Exception as e:
        logger.error(f"âŒ ì„ë² ë”© ê²€ìƒ‰ ì‹¤íŒ¨: {e}")
        raise HTTPException(status_code=500, detail=f"ê²€ìƒ‰ ì‹¤íŒ¨: {str(e)}")


# ==================== Vector Store Management ====================


@router.get("/vector_stats", response_model=VectorStatsResponse)
async def get_vector_stats():
    """vLLM ì„œë²„ì˜ Milvus ë²¡í„°DB í†µê³„ ë° ìƒíƒœ í™•ì¸"""
    try:
        import httpx

        # vLLM ì„œë²„ì˜ ë²¡í„°DB í†µê³„ API í˜¸ì¶œ
        vllm_url = settings.VLLM_BASE_URL or "http://localhost:8001"

        async with httpx.AsyncClient(timeout=30.0) as client:
            response = await client.get(f"{vllm_url}/vector-db/stats")

            if response.status_code == 200:
                vllm_stats = response.json()

                # vLLM ì„œë²„ í†µê³„ ì •ë³´
                stats = {
                    "total_chunks": vllm_stats.get("num_entities", 0),
                    "embedding_dimension": 1024,  # BGE-M3 ê¸°ë³¸ ì°¨ì›
                    "device": "vllm_milvus",
                    "collection_name": vllm_stats.get(
                        "collection_name", "rag_documents"
                    ),
                }

                health = {
                    "status": (
                        "healthy" if vllm_stats.get("num_entities", 0) > 0 else "empty"
                    ),
                    "device": "vllm_milvus",
                    "total_chunks": vllm_stats.get("num_entities", 0),
                    "embedding_model_loaded": True,
                }

                return VectorStatsResponse(stats=stats, health=health)
            else:
                logger.error(f"âŒ vLLM ë²¡í„°DB í†µê³„ ì¡°íšŒ ì‹¤íŒ¨: {response.status_code}")
                raise HTTPException(
                    status_code=500, detail="vLLM ë²¡í„°DB í†µê³„ ì¡°íšŒ ì‹¤íŒ¨"
                )

    except Exception as e:
        logger.error(f"âŒ vLLM ë²¡í„°DB í†µê³„ ì¡°íšŒ ì‹¤íŒ¨: {e}")
        raise HTTPException(status_code=500, detail=f"í†µê³„ ì¡°íšŒ ì‹¤íŒ¨: {str(e)}")


@router.delete("/clear_vector_store")
async def clear_vector_store():
    """vLLM ì„œë²„ì˜ Milvus ë²¡í„°DB ì´ˆê¸°í™”"""
    try:
        import httpx

        # vLLM ì„œë²„ì˜ ë²¡í„°DB ì´ˆê¸°í™” API í˜¸ì¶œ
        vllm_url = settings.VLLM_BASE_URL or "http://localhost:8001"

        async with httpx.AsyncClient(timeout=30.0) as client:
            response = await client.delete(f"{vllm_url}/vector-db/clear")

            if response.status_code == 200:
                result = response.json()
                return {
                    "success": result.get("success", True),
                    "message": result.get("message", "vLLM ë²¡í„°DBê°€ ì´ˆê¸°í™”ë˜ì—ˆìŠµë‹ˆë‹¤."),
                }
            else:
                logger.error(f"âŒ vLLM ë²¡í„°DB ì´ˆê¸°í™” ì‹¤íŒ¨: {response.status_code}")
                raise HTTPException(status_code=500, detail="vLLM ë²¡í„°DB ì´ˆê¸°í™” ì‹¤íŒ¨")

    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"âŒ vLLM ë²¡í„°DB ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
        raise HTTPException(status_code=500, detail=f"ë²¡í„°DB ì´ˆê¸°í™” ì‹¤íŒ¨: {str(e)}")
