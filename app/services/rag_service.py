"""
RAG (Retrieval-Augmented Generation) ì„œë¹„ìŠ¤
í”„ë¡œì íŠ¸ êµ¬ì¡°ì— ë§ê²Œ ì¬êµ¬ì„±ëœ RAG ê¸°ëŠ¥
"""

import os
import logging
import re
from typing import List, Dict, Optional, Any
from dataclasses import dataclass
from pathlib import Path
import asyncio
from datetime import datetime

# Backend imports
from app.database import get_db
from app.models.user import HFTokenManage
from app.core.encryption import decrypt_sensitive_data
from app.core.config import settings
from app.services.embedding_client import VLLMEmbeddingClient, generate_embeddings
from app.services.runpod_manager import get_vllm_manager

logger = logging.getLogger(__name__)


@dataclass
class RAGConfig:
    """RAG ì„¤ì •"""

    # ë¬¸ì„œ ì²˜ë¦¬ ì„¤ì •
    chunk_size: int = 1000
    chunk_overlap: int = 200
    max_chunks: int = 50

    # ë²¡í„° ê²€ìƒ‰ ì„¤ì •
    search_top_k: int = 5
    score_threshold: float = 0.7  # 0.3ì—ì„œ 0.7ë¡œ ë†’ì„

    # ì‹œìŠ¤í…œ ë©”ì‹œì§€
    system_message: str = "ë‹¹ì‹ ì€ ì œê³µëœ ì°¸ê³  ë¬¸ì„œì˜ ì •í™•í•œ ì •ë³´ì™€ ì‚¬ì‹¤ì„ ë°”íƒ•ìœ¼ë¡œ ë‹µë³€í•˜ëŠ” AI ì–´ì‹œìŠ¤í„´íŠ¸ì…ë‹ˆë‹¤."
    influencer_name: str = "AI ì–´ì‹œìŠ¤í„´íŠ¸"


class RAGDocumentProcessor:
    """RAG ë¬¸ì„œ ì²˜ë¦¬ê¸°"""

    def __init__(self, config: RAGConfig):
        self.config = config

    async def process_pdf(self, pdf_path: str) -> List[Dict]:
        """PDF ë¬¸ì„œ ì²˜ë¦¬"""
        try:
            import re

            # PDF ì²˜ë¦¬ ë¼ì´ë¸ŒëŸ¬ë¦¬ í™•ì¸
            try:
                from PyPDF2 import PdfReader

                PYPDF2_AVAILABLE = True
            except ImportError:
                PYPDF2_AVAILABLE = False

            logger.info(f"ğŸ“„ PDF ì²˜ë¦¬ ì‹œì‘: {pdf_path}")

            # PDF ì½ê¸°
            text_content = ""

            with open(pdf_path, "rb") as file:
                reader = PdfReader(file)
                for page_num, page in enumerate(reader.pages):
                    page_text = page.extract_text()
                    text_content += f"\n--- í˜ì´ì§€ {page_num + 1} ---\n{page_text}\n"

            # í…ìŠ¤íŠ¸ ì „ì²˜ë¦¬ (PDF í…ìŠ¤íŠ¸ ì •ë¦¬)
            cleaned_text = self._preprocess_text(text_content)

            # í…ìŠ¤íŠ¸ ì²­í‚¹
            chunks = self._create_chunks(cleaned_text)

            # QA ìŒ ìƒì„±
            qa_pairs = await self._generate_qa_pairs(chunks)

            logger.info(f"âœ… PDF ì²˜ë¦¬ ì™„ë£Œ: {len(qa_pairs)}ê°œ QA ìŒ ìƒì„±")
            return qa_pairs

        except Exception as e:
            logger.error(f"âŒ PDF ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
            raise

    def _preprocess_text(self, text: str) -> str:
        """í…ìŠ¤íŠ¸ ì „ì²˜ë¦¬ (PDF í…ìŠ¤íŠ¸ ì •ë¦¬)"""
        # 1. ëª¨ë“  ìœ ë‹ˆì½”ë“œ ë¬¸ì ì¤‘ í•œê¸€, ì˜ë¬¸, ìˆ«ì, ê¸°ë³¸ ê¸°í˜¸ë§Œ ë‚¨ê¸°ê¸°
        text = re.sub(
            r"[^\uAC00-\uD7AF\u1100-\u11FF\u3130-\u318F\uA960-\uA97F\uAC00-\uD7AF\uD7B0-\uD7FFa-zA-Z0-9\s\.\,\!\?\;\:\-\(\)\[\]\{\}\nê°€-í£]",
            "",
            text,
        )

        # 2. ì—°ì†ëœ ê³µë°± ì œê±°
        text = re.sub(r"\s+", " ", text)

        # 3. í˜ì´ì§€ êµ¬ë¶„ì ì™„ì „ ì œê±°
        text = re.sub(r"--- í˜ì´ì§€ \d+ ---", "", text)

        # 4. ë¹ˆ ì¤„ ì œê±°
        text = re.sub(r"\n\s*\n", "\n", text)

        # 5. ì•ë’¤ ê³µë°± ì œê±°
        text = text.strip()

        # 6. í•œê¸€ ë¬¸ì¥ë§Œ ë‚¨ê¸°ê¸° (ë” ì—„ê²©í•œ í•„í„°ë§)
        lines = text.split("\n")
        cleaned_lines = []
        for line in lines:
            line = line.strip()
            if line and len(line) > 10:  # 10ì ì´ìƒì¸ ë¼ì¸ë§Œ ìœ ì§€
                # í•œê¸€ì´ í¬í•¨ëœ ë¼ì¸ë§Œ ìœ ì§€ (ìµœì†Œ 3ê¸€ì ì´ìƒ)
                korean_chars = re.findall(r"[ê°€-í£]", line)
                if len(korean_chars) >= 3:
                    cleaned_lines.append(line)

        return "\n".join(cleaned_lines)

    def _create_chunks(self, text: str) -> List[str]:
        """í…ìŠ¤íŠ¸ë¥¼ ì²­í¬ë¡œ ë¶„í•  (ë¬¸ì¥ ë‹¨ìœ„)"""
        # ë¬¸ì¥ ë‹¨ìœ„ë¡œ ë¶„í• 
        sentences = re.split(r"[.!?]+", text)

        chunks = []
        current_chunk = ""

        for sentence in sentences:
            sentence = sentence.strip()
            if len(sentence) < 10:  # ë„ˆë¬´ ì§§ì€ ë¬¸ì¥ ì œì™¸
                continue

            # ì²­í¬ í¬ê¸° ì œí•œ í™•ì¸
            if len(current_chunk + sentence) > self.config.chunk_size:
                if current_chunk:
                    chunks.append(current_chunk.strip())
                current_chunk = sentence
            else:
                current_chunk += " " + sentence

        if current_chunk:
            chunks.append(current_chunk.strip())

        return chunks

    async def _generate_qa_pairs(self, chunks: List[str]) -> List[Dict]:
        """ì²­í¬ì—ì„œ QA ìŒ ìƒì„±"""
        qa_pairs = []

        for i, chunk in enumerate(chunks[: self.config.max_chunks]):
            # ê°„ë‹¨í•œ QA ìƒì„± (ì‹¤ì œë¡œëŠ” ë” ì •êµí•œ ë°©ë²• ì‚¬ìš© ê°€ëŠ¥)
            question = f"ì´ ë¬¸ì„œì˜ {i+1}ë²ˆì§¸ ì„¹ì…˜ì— ëŒ€í•´ ì„¤ëª…í•´ì£¼ì„¸ìš”."
            answer = chunk

            qa_pairs.append(
                {
                    "question": question,
                    "answer": answer,
                    "chunk_id": i,
                    "source": "document",
                    "page": i // 5 + 1,  # ëŒ€ëµì ì¸ í˜ì´ì§€ ë²ˆí˜¸
                }
            )

        return qa_pairs


class RAGVectorStore:
    """RAG ë²¡í„° ìŠ¤í† ì–´ (VLLM GPU ë©”ëª¨ë¦¬ ê¸°ë°˜)"""

    def __init__(self, config: RAGConfig):
        self.config = config
        self.embedding_client = VLLMEmbeddingClient()
        self.chunks: List[Dict] = []

    async def store_qa_data(
        self, qa_data: List[Dict], source_file: str = "document.pdf"
    ) -> bool:
        """QA ë°ì´í„°ë¥¼ vLLM ì„œë²„ì˜ Milvus ë²¡í„°DBì— ì €ì¥"""
        try:
            logger.info(f"ğŸ’¾ vLLM ì„œë²„ ë²¡í„°DB ì €ì¥ ì‹œì‘: {len(qa_data)}ê°œ")

            import httpx

            # QA ìŒì„ DocumentChunk í˜•ì‹ìœ¼ë¡œ ë³€í™˜
            documents = []
            for i, qa in enumerate(qa_data):
                # ì§ˆë¬¸ê³¼ ë‹µë³€ì„ ë³„ë„ ë¬¸ì„œë¡œ ì €ì¥
                question_doc = {
                    "id": f"{source_file}_q_{qa['chunk_id']}",
                    "text": qa["question"],
                    "metadata": {
                        "type": "question",
                        "source": source_file,
                        "chunk_id": qa["chunk_id"],
                        "page": qa.get("page", 1),
                    },
                }

                answer_doc = {
                    "id": f"{source_file}_a_{qa['chunk_id']}",
                    "text": qa["answer"],
                    "metadata": {
                        "type": "answer",
                        "source": source_file,
                        "chunk_id": qa["chunk_id"],
                        "page": qa.get("page", 1),
                    },
                }

                documents.extend([question_doc, answer_doc])

            # vLLM ì„œë²„ì˜ ë²¡í„°DB API í˜¸ì¶œ
            vllm_url = settings.VLLM_BASE_URL or "http://localhost:8001"
            logger.info(f"ğŸ”— vLLM ì„œë²„ URL: {vllm_url}")
            logger.info(f"ğŸ“Š ì €ì¥í•  ë¬¸ì„œ ìˆ˜: {len(documents)}ê°œ")

            async with httpx.AsyncClient(timeout=60.0) as client:
                logger.info(
                    f"ğŸ“¤ vLLM ì„œë²„ë¡œ ìš”ì²­ ì „ì†¡: {vllm_url}/vector-db/embed-and-store"
                )
                response = await client.post(
                    f"{vllm_url}/vector-db/embed-and-store",
                    json={"documents": documents},
                )

                if response.status_code == 200:
                    result = response.json()
                    logger.info(f"âœ… vLLM ì„œë²„ ë²¡í„°DB ì €ì¥ ì™„ë£Œ: {result}")
                    return True
                else:
                    logger.error(
                        f"âŒ vLLM ì„œë²„ ë²¡í„°DB ì €ì¥ ì‹¤íŒ¨: {response.status_code}"
                    )
                    logger.error(f"âŒ ì‘ë‹µ ë‚´ìš©: {response.text}")
                    return False

        except Exception as e:
            logger.error(f"âŒ vLLM ì„œë²„ ë²¡í„°DB ì €ì¥ ì‹¤íŒ¨: {e}")
            return False

    async def search_similar(
        self, query: str, top_k: int = 5, score_threshold: float = None
    ) -> List[Dict]:
        """vLLM ì„œë²„ì˜ Milvus ë²¡í„°DBì—ì„œ ìœ ì‚¬í•œ ë¬¸ì„œ ê²€ìƒ‰"""
        try:
            import httpx

            # vLLM ì„œë²„ì˜ ë²¡í„° ê²€ìƒ‰ API í˜¸ì¶œ
            vllm_url = settings.VLLM_BASE_URL or "http://localhost:8001"

            async with httpx.AsyncClient(timeout=30.0) as client:
                # score_threshold ì„¤ì • (íŒŒë¼ë¯¸í„° ìš°ì„ , ê¸°ë³¸ê°’ ì‚¬ìš©)
                if score_threshold is None:
                    score_threshold = getattr(self.config, "score_threshold", 0.3)
                logger.info(
                    f"ğŸ” ê²€ìƒ‰ íŒŒë¼ë¯¸í„°: query='{query}', top_k={top_k}, score_threshold={score_threshold}"
                )

                response = await client.post(
                    f"{vllm_url}/vector-db/embed-and-search",
                    params={
                        "query": query,
                        "top_k": top_k,
                        "score_threshold": score_threshold,
                    },
                )

                if response.status_code == 200:
                    search_results = response.json()
                    logger.info(
                        f"ğŸ” vLLM ë²¡í„°DB ê²€ìƒ‰ ì™„ë£Œ: {len(search_results)}ê°œ ê²°ê³¼"
                    )
                    logger.info(f"ğŸ“Š ê²€ìƒ‰ ê²°ê³¼ ìƒì„¸: {search_results}")
                    return search_results
                else:
                    logger.error(f"âŒ vLLM ë²¡í„°DB ê²€ìƒ‰ ì‹¤íŒ¨: {response.status_code}")
                    logger.error(f"âŒ ì‘ë‹µ ë‚´ìš©: {response.text}")
                    return []

        except Exception as e:
            logger.error(f"âŒ vLLM ë²¡í„°DB ê²€ìƒ‰ ì‹¤íŒ¨: {e}")
            return []

    def _cosine_similarity(self, vec1: List[float], vec2: List[float]) -> float:
        """ì½”ì‚¬ì¸ ìœ ì‚¬ë„ ê³„ì‚°"""
        import numpy as np

        vec1 = np.array(vec1)
        vec2 = np.array(vec2)

        dot_product = np.dot(vec1, vec2)
        norm1 = np.linalg.norm(vec1)
        norm2 = np.linalg.norm(vec2)

        if norm1 == 0 or norm2 == 0:
            return 0.0

        return dot_product / (norm1 * norm2)


class RAGChatGenerator:
    """RAG ì±„íŒ… ìƒì„±ê¸°"""

    def __init__(self, config: RAGConfig):
        self.config = config

    async def generate_response(
        self,
        query: str,
        context: str,
        system_message: str = None,
        influencer_name: str = None,
    ) -> str:
        """ì»¨í…ìŠ¤íŠ¸ ê¸°ë°˜ ì‘ë‹µ ìƒì„±"""
        try:
            # vLLM ì„œë²„ ìƒíƒœ í™•ì¸
            vllm_manager = get_vllm_manager()
            if not await vllm_manager.health_check():
                logger.warning("âš ï¸ vLLM ì„œë²„ ì—°ê²° ë¶ˆì•ˆì •, ê¸°ë³¸ ì‘ë‹µ ì‚¬ìš©")
                return f"ì£„ì†¡í•©ë‹ˆë‹¤. í˜„ì¬ ì„œë²„ ìƒíƒœê°€ ë¶ˆì•ˆì •í•©ë‹ˆë‹¤. ì§ˆë¬¸: {query}"

            # ì‹œìŠ¤í…œ ë©”ì‹œì§€ êµ¬ì„±
            system_msg = system_message or self.config.system_message
            influencer = influencer_name or self.config.influencer_name

            # ì»¨í…ìŠ¤íŠ¸ê°€ í¬í•¨ëœ í”„ë¡¬í”„íŠ¸ êµ¬ì„±
            prompt = f"""ì‹œìŠ¤í…œ: {system_msg}

ì°¸ê³  ë¬¸ì„œ:
{context}

ì‚¬ìš©ì: {query}

{influencer}: """

            # VLLM í´ë¼ì´ì–¸íŠ¸ë¡œ ì‘ë‹µ ìƒì„±
            vllm_client = await get_vllm_client()

            response = await vllm_client.generate_response(
                user_message=query,
                system_message=system_msg,
                influencer_name=influencer,
                context=context,
                max_new_tokens=self.config.max_tokens,
                temperature=self.config.temperature,
            )

            return response

        except Exception as e:
            logger.error(f"âŒ ì‘ë‹µ ìƒì„± ì‹¤íŒ¨: {e}")
            return f"ì£„ì†¡í•©ë‹ˆë‹¤. ì‘ë‹µ ìƒì„± ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}"


class RAGService:
    """RAG ì„œë¹„ìŠ¤ ë©”ì¸ í´ë˜ìŠ¤"""

    def __init__(self, config: RAGConfig = None):
        self.config = config or RAGConfig()
        self.document_processor = RAGDocumentProcessor(self.config)
        self.vector_store = RAGVectorStore(self.config)
        self.chat_generator = RAGChatGenerator(self.config)
        self._pipelines: Dict[int, Dict] = {}  # group_idë³„ íŒŒì´í”„ë¼ì¸

    async def create_pipeline(
        self,
        group_id: int,
        pdf_path: str,
        system_message: str = None,
        influencer_name: str = None,
    ) -> bool:
        """VLLM GPU RAG íŒŒì´í”„ë¼ì¸ ìƒì„±"""
        try:
            logger.info(f"ğŸš€ VLLM GPU RAG íŒŒì´í”„ë¼ì¸ ìƒì„± ì‹œì‘: group_id={group_id}")

            # 1. ë¬¸ì„œ ì²˜ë¦¬
            qa_data = await self.document_processor.process_pdf(pdf_path)

            # 2. VLLM GPU ë©”ëª¨ë¦¬ì— ì €ì¥
            success = await self.vector_store.store_qa_data(qa_data, pdf_path)

            if success:
                # 3. íŒŒì´í”„ë¼ì¸ ì •ë³´ ì €ì¥
                self._pipelines[group_id] = {
                    "pdf_path": pdf_path,
                    "qa_count": len(qa_data),
                    "system_message": system_message or self.config.system_message,
                    "influencer_name": influencer_name or self.config.influencer_name,
                    "created_at": datetime.now().isoformat(),
                }

                logger.info(
                    f"âœ… VLLM GPU RAG íŒŒì´í”„ë¼ì¸ ìƒì„± ì™„ë£Œ: group_id={group_id}"
                )
                return True
            else:
                logger.error(
                    f"âŒ VLLM GPU RAG íŒŒì´í”„ë¼ì¸ ìƒì„± ì‹¤íŒ¨: group_id={group_id}"
                )
                return False

        except Exception as e:
            logger.error(f"âŒ VLLM GPU RAG íŒŒì´í”„ë¼ì¸ ìƒì„± ì¤‘ ì˜¤ë¥˜: {e}")
            return False

    async def chat(
        self, group_id: int, query: str, include_sources: bool = True
    ) -> Dict:
        """VLLM GPU RAG ì±„íŒ…"""
        try:
            # íŒŒì´í”„ë¼ì¸ í™•ì¸
            if group_id not in self._pipelines:
                return {
                    "error": "íŒŒì´í”„ë¼ì¸ì´ ì—†ìŠµë‹ˆë‹¤. ë¨¼ì € ë¬¸ì„œë¥¼ ì—…ë¡œë“œí•´ì£¼ì„¸ìš”.",
                    "query": query,
                    "response": "",
                    "sources": [],
                    "timestamp": datetime.now().isoformat(),
                }

            pipeline_info = self._pipelines[group_id]

            # 1. VLLM GPU ë©”ëª¨ë¦¬ì—ì„œ ìœ ì‚¬í•œ ë¬¸ì„œ ê²€ìƒ‰
            search_results = await self.vector_store.search_similar(
                query, top_k=self.config.search_top_k
            )

            # 2. ì»¨í…ìŠ¤íŠ¸ êµ¬ì„±
            context = self._build_context(search_results)

            # 3. ì‘ë‹µ ìƒì„±
            response = await self.chat_generator.generate_response(
                query=query,
                context=context,
                system_message=pipeline_info["system_message"],
                influencer_name=pipeline_info["influencer_name"],
            )

            # 4. ê²°ê³¼ êµ¬ì„±
            result = {
                "query": query,
                "response": response,
                "timestamp": datetime.now().isoformat(),
                "model_info": {
                    "influencer_name": pipeline_info["influencer_name"],
                    "system_message": pipeline_info["system_message"],
                },
            }

            # ì¶œì²˜ ì •ë³´ í¬í•¨
            if include_sources and search_results:
                result["sources"] = [
                    {
                        "text": (
                            item["text"][:200] + "..."
                            if len(item["text"]) > 200
                            else item["text"]
                        ),
                        "score": item["score"],
                        "source": item["metadata"]["source"],
                        "page": item["metadata"]["page"],
                    }
                    for item in search_results[:3]  # ìƒìœ„ 3ê°œë§Œ
                ]
                result["context_preview"] = (
                    context[:200] + "..." if len(context) > 200 else context
                )

            return result

        except Exception as e:
            logger.error(f"âŒ VLLM GPU RAG ì±„íŒ… ì‹¤íŒ¨: {e}")
            return {
                "error": f"VLLM GPU ì±„íŒ… ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}",
                "query": query,
                "response": "",
                "sources": [],
                "timestamp": datetime.now().isoformat(),
            }

    def _build_context(self, search_results: List[Dict]) -> str:
        """ê²€ìƒ‰ ê²°ê³¼ë¡œë¶€í„° ì»¨í…ìŠ¤íŠ¸ êµ¬ì„±"""
        if not search_results:
            return ""

        context_parts = []
        for item in search_results:
            context_parts.append(f"ì°¸ê³  ë¬¸ì„œ: {item['text']}")

        return "\n\n".join(context_parts)

    def get_pipeline_info(self, group_id: int) -> Optional[Dict]:
        """íŒŒì´í”„ë¼ì¸ ì •ë³´ ì¡°íšŒ"""
        return self._pipelines.get(group_id)

    def list_pipelines(self) -> List[Dict]:
        """ëª¨ë“  íŒŒì´í”„ë¼ì¸ ëª©ë¡"""
        return [
            {"group_id": group_id, **info} for group_id, info in self._pipelines.items()
        ]

    async def cleanup_pipeline(self, group_id: int) -> bool:
        """íŒŒì´í”„ë¼ì¸ ì •ë¦¬"""
        try:
            if group_id in self._pipelines:
                del self._pipelines[group_id]
                logger.info(f"âœ… íŒŒì´í”„ë¼ì¸ ì •ë¦¬ ì™„ë£Œ: group_id={group_id}")
                return True
            return False
        except Exception as e:
            logger.error(f"âŒ íŒŒì´í”„ë¼ì¸ ì •ë¦¬ ì‹¤íŒ¨: {e}")
            return False


# ì „ì—­ RAG ì„œë¹„ìŠ¤ ì¸ìŠ¤í„´ìŠ¤
_rag_service = None


def get_rag_service() -> RAGService:
    """ì „ì—­ RAG ì„œë¹„ìŠ¤ ì¸ìŠ¤í„´ìŠ¤ ë°˜í™˜"""
    global _rag_service
    if _rag_service is None:
        _rag_service = RAGService()
    return _rag_service
