"""
RAG ë¶„ê¸°ì²˜ë¦¬ í”„ë¡œì„¸ì„œ (ë‹¨ìˆœí™”ëœ ë²„ì „)
1. ë¬¸ì„œì—ì„œ ì„ê³„ê°’ì„ ë„˜ëŠ” top_k ë¬¸ì„œë“¤ì„ ê²€ìƒ‰
2. ê°€ì¥ ìœ ì‚¬ë„ê°€ ë†’ì€ ë¬¸ì„œë¥¼ í˜¸ì¶œ
3. ê²°ê³¼ í›„ì²˜ë¦¬ openaië¡œ ìì—°ìŠ¤ëŸ¬ìš´ ìì—°ì–´ë¡œ í›„ì²˜ë¦¬
4. SLLM ëª¨ë¸ë¡œ ë§íˆ¬ ìì—°ì–´ë¡œ í›„ì²˜ë¦¬
5. ì„ê³„ê°’ì„ ë„˜ëŠ” ë¬¸ì„œë“¤ì´ ì—†ìœ¼ë©´ MCPë¡œ ë„˜ì–´ê°
"""

import logging
from typing import List, Dict, Optional, Tuple, Any
from dataclasses import dataclass
from app.services.openai_service_simple import OpenAIService
from app.services.runpod_manager import get_vllm_manager
from app.services.rag_service import get_rag_service, RAGService

logger = logging.getLogger(__name__)


@dataclass
class RAGProcessorConfig:
    """RAG í”„ë¡œì„¸ì„œ ì„¤ì •"""

    top_k: int = 5
    similarity_threshold: float = 0.7  # 0.5ì—ì„œ 0.7ë¡œ ë†’ì„
    max_context_length: int = 2000
    max_tokens: int = 1024
    temperature: float = 0.8


class RAGProcessor:
    """RAG ë¶„ê¸°ì²˜ë¦¬ í”„ë¡œì„¸ì„œ (ë‹¨ìˆœí™”ëœ ë²„ì „)"""

    def __init__(self, config: RAGProcessorConfig = None):
        self.config = config or RAGProcessorConfig()
        self.openai_service = OpenAIService()
        self.rag_service = get_rag_service()

    async def process_with_rag(
        self,
        message: str,
        influencer_name: str = None,
        system_message: str = None,
    ) -> Tuple[Optional[str], List[Dict], bool]:
        """
        RAG ë¶„ê¸°ì²˜ë¦¬ ë©”ì¸ í•¨ìˆ˜ (ë‹¨ìˆœí™”ëœ ë²„ì „)
        Returns: (response, sources, should_fallback_to_mcp)
        """
        try:
            logger.info("=" * 50)
            logger.info("ğŸš€ RAG í”„ë¡œì„¸ì„œ ì²˜ë¦¬ ì‹œì‘ (ë‹¨ìˆœí™”ëœ ë²„ì „)")
            logger.info(f"ğŸ“ ì‚¬ìš©ì ë©”ì‹œì§€: {message}")
            logger.info("=" * 50)

            # 1ë‹¨ê³„: ë¬¸ì„œì—ì„œ ì„ê³„ê°’ì„ ë„˜ëŠ” top_k ë¬¸ì„œë“¤ì„ ê²€ìƒ‰
            logger.info("ğŸ” 1ë‹¨ê³„: ë¬¸ì„œ ê²€ìƒ‰ ì‹œì‘")
            search_results = await self.rag_service.vector_store.search_similar(
                query=message,
                top_k=self.config.top_k,
                score_threshold=self.config.similarity_threshold,
            )

            if not search_results:
                logger.info("âŒ ì„ê³„ê°’ì„ ë„˜ëŠ” ë¬¸ì„œê°€ ì—†ìŒ. MCPë¡œ ì „í™˜.")
                return None, [], True  # MCPë¡œ ì „í™˜

            logger.info(f"âœ… ê²€ìƒ‰ ì™„ë£Œ: {len(search_results)}ê°œ ë¬¸ì„œ ë°œê²¬")

            # 2ë‹¨ê³„: ê°€ì¥ ìœ ì‚¬ë„ê°€ ë†’ì€ ë¬¸ì„œë¥¼ í˜¸ì¶œ
            logger.info("ğŸ“„ 2ë‹¨ê³„: ìµœê³  ìœ ì‚¬ë„ ë¬¸ì„œ í˜¸ì¶œ")
            best_document = search_results[0]  # ê°€ì¥ ìœ ì‚¬ë„ê°€ ë†’ì€ ë¬¸ì„œ
            logger.info(
                f"ğŸ“„ ìµœê³  ìœ ì‚¬ë„ ë¬¸ì„œ: {best_document.get('text', '')[:100]}..."
            )
            logger.info(f"ğŸ“Š ìœ ì‚¬ë„ ì ìˆ˜: {best_document.get('score', 0)}")

            # 3ë‹¨ê³„: ê²°ê³¼ í›„ì²˜ë¦¬ openaië¡œ ìì—°ìŠ¤ëŸ¬ìš´ ìì—°ì–´ë¡œ í›„ì²˜ë¦¬
            logger.info("ğŸ¤– 3ë‹¨ê³„: OpenAI í›„ì²˜ë¦¬ ì‹œì‘")
            openai_prompt = f"""
ì•„ë˜ ë¬¸ì„œ ë‚´ìš©ì„ ë°”íƒ•ìœ¼ë¡œ ì‚¬ìš©ìì˜ ì§ˆë¬¸ì— ë‹µë³€í•´ì£¼ì„¸ìš”.

ì‚¬ìš©ì ì§ˆë¬¸: {message}

ë¬¸ì„œ ë‚´ìš©:
{best_document.get('text', '')}

ë‹µë³€ ìš”êµ¬ì‚¬í•­:
- ë¬¸ì„œ ë‚´ìš©ì„ ë°”íƒ•ìœ¼ë¡œ ì •í™•í•˜ê³  ëª…í™•í•˜ê²Œ ë‹µë³€
- ë¬¸ì„œì— ì—†ëŠ” ì •ë³´ëŠ” ì¶”ê°€í•˜ì§€ ì•ŠìŒ
- ìì—°ìŠ¤ëŸ½ê³  ì´í•´í•˜ê¸° ì‰½ê²Œ ì‘ì„±
- ë¶ˆí•„ìš”í•œ ì‚¬ì„¤ì´ë‚˜ ê°íƒ„ì€ ì œì™¸
"""

            openai_response = await self.openai_service.openai_tool_selection(
                user_prompt=openai_prompt,
                system_prompt="ë‹¹ì‹ ì€ ë¬¸ì„œ ë‚´ìš©ì„ ë°”íƒ•ìœ¼ë¡œ ì •í™•í•˜ê³  ìì—°ìŠ¤ëŸ½ê²Œ ë‹µë³€í•˜ëŠ” AIì…ë‹ˆë‹¤.",
            )
            logger.info(f"âœ… OpenAI í›„ì²˜ë¦¬ ì™„ë£Œ: {openai_response[:100]}...")

            # 4ë‹¨ê³„: ë§íˆ¬ ë³€í™˜ì€ Frontend WebSocketì—ì„œ ì²˜ë¦¬í•˜ë¯€ë¡œ OpenAI ê²°ê³¼ ê·¸ëŒ€ë¡œ ì‚¬ìš©
            logger.info("ğŸ­ 4ë‹¨ê³„: Frontend WebSocketì—ì„œ ë§íˆ¬ ë³€í™˜ ì˜ˆì •")
            final_response = openai_response

            # ê²°ê³¼ êµ¬ì„±
            sources = []
            for result in search_results:
                sources.append(
                    {
                        "text": result.get("text", ""),
                        "score": result.get("score", 0),
                        "metadata": result.get("metadata", {}),
                    }
                )

            logger.info(f"ğŸ¯ ìµœì¢… RAG ì‘ë‹µ: {final_response}")
            logger.info(f"ğŸ“‹ ì°¸ì¡° ë¬¸ì„œ: {len(sources)}ê°œ")
            logger.info("=" * 50)

            return final_response, sources, False  # RAG ì„±ê³µ, MCPë¡œ ì „í™˜í•˜ì§€ ì•ŠìŒ

        except Exception as e:
            logger.error(f"âŒ RAG í”„ë¡œì„¸ì„œ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜: {e}")
            logger.info("ğŸ”„ RAG ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ë¡œ MCPë¡œ ì „í™˜")
            logger.info("=" * 50)
            return None, [], True  # MCPë¡œ ì „í™˜


# ì „ì—­ RAG í”„ë¡œì„¸ì„œ ì¸ìŠ¤í„´ìŠ¤
rag_processor = RAGProcessor()


# RAG í”„ë¡œì„¸ì„œ í•¨ìˆ˜ë“¤ (ì™¸ë¶€ì—ì„œ ì‚¬ìš©)
async def process_with_rag(
    message: str,
    influencer_name: str = None,
    system_message: str = None,
) -> Tuple[Optional[str], List[Dict], bool]:
    """RAG ë¶„ê¸°ì²˜ë¦¬ ë©”ì¸ í•¨ìˆ˜ (ì™¸ë¶€ ì¸í„°í˜ì´ìŠ¤)"""
    return await rag_processor.process_with_rag(
        message=message,
        influencer_name=influencer_name,
        system_message=system_message,
    )
