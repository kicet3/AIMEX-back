"""
QA ìƒì„± ì„œë¹„ìŠ¤
ì¸í”Œë£¨ì–¸ì„œ íŒŒì¸íŠœë‹ì„ ìœ„í•œ QA ë°ì´í„°ì…‹ ìƒì„±
"""

import os
import json
import logging
import asyncio
from typing import List, Dict, Any, Optional
from datetime import datetime
import tempfile
from openai import OpenAI

from app.schemas.qa_generation import CharacterProfile, Gender
from app.core.encryption import decrypt_sensitive_data

logger = logging.getLogger(__name__)

# ë„ë©”ì¸ë³„ QA ìƒì„±ì„ ìœ„í•œ ì¹´í…Œê³ ë¦¬ ì •ì˜
DOMAIN_CATEGORIES = [
    "ì¼ìƒìƒí™œ",
    "ê³¼í•™ê¸°ìˆ ", 
    "ì‚¬íšŒì´ìŠˆ",
    "ì¸ë¬¸í•™",
    "ìŠ¤í¬ì¸ ",
    "ì—­ì‚¬ë¬¸í™”"
]

# ë„ë©”ì¸ë³„ íŠ¹ì„± ì„¤ëª…
DOMAIN_DESCRIPTIONS = {
    "ì¼ìƒìƒí™œ": "ì¼ìƒì˜ ì†Œì†Œí•œ ì¼ë“¤, ì·¨ë¯¸, ìŠµê´€, ìŒì‹, ì£¼ë§ í™œë™ ë“±",
    "ê³¼í•™ê¸°ìˆ ": "AI, ê¸°ìˆ  íŠ¸ë Œë“œ, ìŠ¤ë§ˆíŠ¸í°, ë¯¸ë˜ ê¸°ìˆ , ê³¼í•™ì˜ ë°œì „",
    "ì‚¬íšŒì´ìŠˆ": "ì‚¬íšŒ ë¬¸ì œ, í™˜ê²½, ë¶ˆí‰ë“±, ì„¸ëŒ€ ê°„ ì°¨ì´, ë¯¸ë˜ ì‚¬íšŒ",
    "ì¸ë¬¸í•™": "ì¸ìƒì˜ ê°€ì¹˜, ì±…, ì˜ˆìˆ , ì² í•™, ì—­ì‚¬ì˜ êµí›ˆ",
    "ìŠ¤í¬ì¸ ": "ìš´ë™, ê±´ê°•ê´€ë¦¬, ìŠ¤í¬ì¸  ê²½ê¸°, ìš´ë™ì˜ ì¦ê±°ì›€",
    "ì—­ì‚¬ë¬¸í™”": "ì „í†µë¬¸í™”, ì—­ì‚¬ì  ì¥ì†Œ, ë¬¸í™”ì˜ ë‹¤ì–‘ì„±, ì—­ì‚¬ ì¸ë¬¼"
}


class QAGenerationService:
    """QA ìƒì„± ì„œë¹„ìŠ¤"""
    
    def __init__(self):
        """ì„œë¹„ìŠ¤ ì´ˆê¸°í™”"""
        self.openai_client = None
        self._initialize_openai()
        
    def _initialize_openai(self):
        """OpenAI í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™”"""
        api_key = os.getenv("OPENAI_API_KEY")
        if api_key:
            self.openai_client = OpenAI(api_key=api_key)
            logger.info("âœ… OpenAI í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™” ì™„ë£Œ")
        else:
            logger.warning("âš ï¸ OPENAI_API_KEYê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤")
    
    async def generate_qa_for_influencer(
        self,
        character_name: str,
        character_description: str,
        personality: str,
        num_qa_pairs: int = 2000,
        domains: Optional[List[str]] = None,
        system_prompt: Optional[str] = None
    ) -> Dict[str, Any]:
        """
        ì¸í”Œë£¨ì–¸ì„œìš© ëŒ€ëŸ‰ QA ìƒì„±
        
        Args:
            character_name: ìºë¦­í„° ì´ë¦„
            character_description: ìºë¦­í„° ì„¤ëª…
            personality: ì„±ê²©
            num_qa_pairs: ìƒì„±í•  QA ìŒ ê°œìˆ˜
            domains: ë„ë©”ì¸ ë¦¬ìŠ¤íŠ¸
            system_prompt: ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸
            
        Returns:
            ë°°ì¹˜ ìš”ì²­ ë°ì´í„°
        """
        if not self.openai_client:
            raise Exception("OpenAI í´ë¼ì´ì–¸íŠ¸ê°€ ì´ˆê¸°í™”ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤")
        
        # ë„ë©”ì¸ ì„¤ì •
        if not domains:
            domains = DOMAIN_CATEGORIES
        
        # ë„ë©”ì¸ë³„ QA ê°œìˆ˜ ê³„ì‚° (ê· ë“± ë¶„ë°°)
        qa_per_domain = num_qa_pairs // len(domains)
        remaining = num_qa_pairs % len(domains)
        
        logger.info(f"ğŸ¯ QA ìƒì„± ì‹œì‘: {character_name}, ì´ {num_qa_pairs}ê°œ")
        logger.info(f"ğŸ“Š ë„ë©”ì¸ë³„ í• ë‹¹: {qa_per_domain}ê°œ (ë‚˜ë¨¸ì§€: {remaining})")
        
        # ë°°ì¹˜ ìš”ì²­ ìƒì„±
        batch_requests = []
        request_count = 0
        
        for domain_idx, domain in enumerate(domains):
            # ë§ˆì§€ë§‰ ë„ë©”ì¸ì— ë‚˜ë¨¸ì§€ í• ë‹¹
            domain_qa_count = qa_per_domain
            if domain_idx == len(domains) - 1:
                domain_qa_count += remaining
            
            domain_desc = DOMAIN_DESCRIPTIONS.get(domain, domain)
            
            # ê° ë„ë©”ì¸ì— ëŒ€í•´ ì—¬ëŸ¬ ìš”ì²­ ìƒì„± (ê° ìš”ì²­ë‹¹ 7ê°œ QA)
            requests_for_domain = domain_qa_count // 7
            remaining_qa = domain_qa_count % 7
            
            for req_idx in range(requests_for_domain):
                custom_id = f"influencer_qa_{character_name}_{domain}_{request_count}"
                
                batch_request = self._create_batch_request(
                    custom_id=custom_id,
                    character_name=character_name,
                    character_description=character_description,
                    personality=personality,
                    domain=domain,
                    domain_desc=domain_desc,
                    system_prompt=system_prompt
                )
                
                batch_requests.append(batch_request)
                request_count += 1
            
            # ë‚˜ë¨¸ì§€ QA ì²˜ë¦¬
            if remaining_qa > 0:
                custom_id = f"influencer_qa_{character_name}_{domain}_{request_count}_partial"
                
                batch_request = self._create_batch_request(
                    custom_id=custom_id,
                    character_name=character_name,
                    character_description=character_description,
                    personality=personality,
                    domain=domain,
                    domain_desc=domain_desc,
                    system_prompt=system_prompt,
                    num_qa=remaining_qa
                )
                
                batch_requests.append(batch_request)
                request_count += 1
        
        logger.info(f"âœ… ë°°ì¹˜ ìš”ì²­ ìƒì„± ì™„ë£Œ: ì´ {len(batch_requests)}ê°œ ìš”ì²­")
        
        return {
            "batch_requests": batch_requests,
            "total_requests": len(batch_requests),
            "domains": domains,
            "qa_per_domain": qa_per_domain,
            "character_name": character_name
        }
    
    def _create_batch_request(
        self,
        custom_id: str,
        character_name: str,
        character_description: str,
        personality: str,
        domain: str,
        domain_desc: str,
        system_prompt: Optional[str] = None,
        num_qa: int = 7
    ) -> Dict[str, Any]:
        """
        OpenAI Batch API í˜•ì‹ì˜ ìš”ì²­ ìƒì„±
        
        Args:
            custom_id: ìš”ì²­ ID
            character_name: ìºë¦­í„° ì´ë¦„
            character_description: ìºë¦­í„° ì„¤ëª…
            personality: ì„±ê²©
            domain: ë„ë©”ì¸
            domain_desc: ë„ë©”ì¸ ì„¤ëª…
            system_prompt: ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸
            num_qa: QA ê°œìˆ˜ (ê¸°ë³¸ 7ê°œ)
            
        Returns:
            ë°°ì¹˜ ìš”ì²­ ë”•ì…”ë„ˆë¦¬
        """
        if not system_prompt:
            system_prompt = f"ë‹¹ì‹ ì€ {character_name}ë¼ëŠ” ìºë¦­í„°ì…ë‹ˆë‹¤. {personality} ì„±ê²©ì„ ê°€ì§€ê³  ìˆìŠµë‹ˆë‹¤."
        
        user_prompt = f"""{domain}({domain_desc})ì— ê´€í•œ {num_qa}í„´ì˜ ìì—°ìŠ¤ëŸ¬ìš´ ë©€í‹°í„´ ëŒ€í™”ë¥¼ ë§Œë“¤ì–´ì£¼ì„¸ìš”.

{character_name}ì˜ ì„±ê²©ê³¼ íŠ¹ì„±ì— ë§ëŠ” í¥ë¯¸ë¡­ê³  ê¹Šì´ ìˆëŠ” ëŒ€í™”ë¥¼ ë§Œë“¤ì–´ì£¼ì„¸ìš”.
ëŒ€í™”ëŠ” í•˜ë‚˜ì˜ ì£¼ì œë¡œ ì‹œì‘í•´ì„œ ìì—°ìŠ¤ëŸ½ê²Œ ì—°ê²°ë˜ë©° ì ì°¨ ê¹Šì–´ì§€ëŠ” í˜•íƒœì—¬ì•¼ í•©ë‹ˆë‹¤.

ë°˜ë“œì‹œ ë‹¤ìŒ JSON ë°°ì—´ í˜•ì‹ìœ¼ë¡œ ë‹µë³€í•´ì£¼ì„¸ìš”:
[
  {{"q": "ì²« ë²ˆì§¸ ì§ˆë¬¸", "a": "ì²« ë²ˆì§¸ ë‹µë³€"}},
  {{"q": "ì´ì „ ë‹µë³€ì„ ë°”íƒ•ìœ¼ë¡œ í•œ í›„ì† ì§ˆë¬¸", "a": "ë” êµ¬ì²´ì ì¸ ë‹µë³€"}},
  {{"q": "ë” ê¹Šì´ íŒŒê³ ë“œëŠ” ì§ˆë¬¸", "a": "ê°œì¸ì  ê²½í—˜ì´ë‚˜ ì˜ê²¬ì´ ë‹´ê¸´ ë‹µë³€"}},
  {{"q": "ë‹¤ë¥¸ ê´€ì ì—ì„œì˜ ì§ˆë¬¸", "a": "ìƒˆë¡œìš´ ì‹œê°ì„ ì œì‹œí•˜ëŠ” ë‹µë³€"}},
  {{"q": "ì‹¤ìš©ì ì¸ ì¡°ì–¸ì„ êµ¬í•˜ëŠ” ì§ˆë¬¸", "a": "êµ¬ì²´ì ì¸ ì œì•ˆì´ ë‹´ê¸´ ë‹µë³€"}},
  {{"q": "ê°ì •ì´ë‚˜ ê°€ì¹˜ê´€ì— ëŒ€í•œ ì§ˆë¬¸", "a": "ìºë¦­í„°ì˜ ì² í•™ì´ ë“œëŸ¬ë‚˜ëŠ” ë‹µë³€"}},
  {{"q": "ë§ˆë¬´ë¦¬í•˜ë©° ì •ë¦¬ë¥¼ ìš”ì²­í•˜ëŠ” ì§ˆë¬¸", "a": "ëŒ€í™”ë¥¼ ì¢…í•©í•˜ë©° í•µì‹¬ì„ ì •ë¦¬í•˜ëŠ” ë‹µë³€"}}
]"""
        
        return {
            "custom_id": custom_id,
            "method": "POST",
            "url": "/v1/chat/completions",
            "body": {
                "model": "gpt-4o-mini",
                "messages": [
                    {
                        "role": "system", 
                        "content": system_prompt
                    },
                    {
                        "role": "user", 
                        "content": user_prompt
                    }
                ],
                "max_tokens": 500,
                "temperature": 0.7,
                "response_format": {"type": "json_object"}  # JSON í˜•ì‹ ê°•ì œ
            }
        }
    
    async def create_batch_file(
        self,
        batch_requests: List[Dict[str, Any]]
    ) -> str:
        """
        ë°°ì¹˜ ìš”ì²­ì„ JSONL íŒŒì¼ë¡œ ìƒì„±í•˜ê³  OpenAIì— ì—…ë¡œë“œ
        
        Args:
            batch_requests: ë°°ì¹˜ ìš”ì²­ ë¦¬ìŠ¤íŠ¸
            
        Returns:
            ì—…ë¡œë“œëœ íŒŒì¼ ID
        """
        if not self.openai_client:
            raise Exception("OpenAI í´ë¼ì´ì–¸íŠ¸ê°€ ì´ˆê¸°í™”ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤")
        
        # ì„ì‹œ JSONL íŒŒì¼ ìƒì„±
        with tempfile.NamedTemporaryFile(mode='w', suffix='.jsonl', delete=False) as tmp_file:
            for request in batch_requests:
                tmp_file.write(json.dumps(request) + '\n')
            tmp_file_path = tmp_file.name
        
        try:
            # OpenAIì— íŒŒì¼ ì—…ë¡œë“œ
            with open(tmp_file_path, 'rb') as file:
                response = self.openai_client.files.create(
                    file=file,
                    purpose='batch'
                )
            
            file_id = response.id
            logger.info(f"âœ… ë°°ì¹˜ íŒŒì¼ ì—…ë¡œë“œ ì™„ë£Œ: {file_id}")
            
            return file_id
            
        finally:
            # ì„ì‹œ íŒŒì¼ ì‚­ì œ
            os.unlink(tmp_file_path)
    
    async def submit_batch(
        self,
        file_id: str,
        metadata: Optional[Dict[str, Any]] = None
    ) -> str:
        """
        OpenAI Batch APIì— ì‘ì—… ì œì¶œ
        
        Args:
            file_id: ì—…ë¡œë“œëœ íŒŒì¼ ID
            metadata: ë©”íƒ€ë°ì´í„°
            
        Returns:
            ë°°ì¹˜ ID
        """
        if not self.openai_client:
            raise Exception("OpenAI í´ë¼ì´ì–¸íŠ¸ê°€ ì´ˆê¸°í™”ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤")
        
        # ë°°ì¹˜ ì‘ì—… ìƒì„±
        batch = self.openai_client.batches.create(
            input_file_id=file_id,
            endpoint="/v1/chat/completions",
            completion_window="24h",
            metadata=metadata or {}
        )
        
        logger.info(f"âœ… ë°°ì¹˜ ì‘ì—… ì œì¶œ ì™„ë£Œ: {batch.id}")
        
        return batch.id
    
    async def get_batch_status(self, batch_id: str) -> Dict[str, Any]:
        """
        ë°°ì¹˜ ì‘ì—… ìƒíƒœ ì¡°íšŒ
        
        Args:
            batch_id: ë°°ì¹˜ ID
            
        Returns:
            ë°°ì¹˜ ìƒíƒœ ì •ë³´
        """
        if not self.openai_client:
            raise Exception("OpenAI í´ë¼ì´ì–¸íŠ¸ê°€ ì´ˆê¸°í™”ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤")
        
        batch = self.openai_client.batches.retrieve(batch_id)
        
        return {
            "batch_id": batch.id,
            "status": batch.status,
            "created_at": batch.created_at,
            "completed_at": batch.completed_at,
            "request_counts": batch.request_counts,
            "output_file_id": batch.output_file_id,
            "error_file_id": batch.error_file_id
        }
    
    async def process_batch_results(
        self,
        batch_id: str,
        output_file_id: str
    ) -> Dict[str, Any]:
        """
        ë°°ì¹˜ ê²°ê³¼ ì²˜ë¦¬
        
        Args:
            batch_id: ë°°ì¹˜ ID
            output_file_id: ì¶œë ¥ íŒŒì¼ ID
            
        Returns:
            ì²˜ë¦¬ëœ QA ë°ì´í„°
        """
        if not self.openai_client:
            raise Exception("OpenAI í´ë¼ì´ì–¸íŠ¸ê°€ ì´ˆê¸°í™”ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤")
        
        # ê²°ê³¼ íŒŒì¼ ë‹¤ìš´ë¡œë“œ
        file_response = self.openai_client.files.content(output_file_id)
        
        # ì„ì‹œ íŒŒì¼ì— ì €ì¥
        with tempfile.NamedTemporaryFile(mode='w', suffix='.jsonl', delete=False) as tmp_file:
            tmp_file.write(file_response.text)
            tmp_file_path = tmp_file.name
        
        try:
            # ê²°ê³¼ íŒŒì‹±
            qa_pairs = []
            errors = []
            
            with open(tmp_file_path, 'r', encoding='utf-8') as f:
                for line in f:
                    if not line.strip():
                        continue
                    
                    try:
                        result = json.loads(line)
                        
                        # ì„±ê³µì ì¸ ì‘ë‹µ ì²˜ë¦¬
                        if result.get("response") and result["response"].get("body"):
                            choices = result["response"]["body"].get("choices", [])
                            if choices and choices[0].get("message"):
                                content = choices[0]["message"]["content"]
                                
                                # JSON íŒŒì‹±
                                try:
                                    qa_data = json.loads(content)
                                    if isinstance(qa_data, list):
                                        qa_pairs.extend(qa_data)
                                except json.JSONDecodeError:
                                    logger.warning(f"JSON íŒŒì‹± ì‹¤íŒ¨: {content[:100]}...")
                        
                        # ì—ëŸ¬ ì²˜ë¦¬
                        elif result.get("error"):
                            errors.append({
                                "custom_id": result.get("custom_id"),
                                "error": result["error"]
                            })
                    
                    except json.JSONDecodeError:
                        logger.error(f"ë¼ì¸ íŒŒì‹± ì‹¤íŒ¨: {line[:100]}...")
            
            logger.info(f"âœ… ë°°ì¹˜ ê²°ê³¼ ì²˜ë¦¬ ì™„ë£Œ: {len(qa_pairs)}ê°œ QA, {len(errors)}ê°œ ì—ëŸ¬")
            
            return {
                "qa_pairs": qa_pairs,
                "total_count": len(qa_pairs),
                "errors": errors,
                "error_count": len(errors)
            }
            
        finally:
            # ì„ì‹œ íŒŒì¼ ì‚­ì œ
            os.unlink(tmp_file_path)
    
    async def generate_tone_variations(self, character_profile: Dict[str, Any]) -> Dict[str, Any]:
        """
        ìºë¦­í„° ê¸°ë°˜ ì–´íˆ¬ ë³€í˜• ìƒì„± (í†µí•© ë²„ì „)
        vLLMì˜ generate_qa_fast ë¡œì§ì„ ë°±ì—”ë“œì— í†µí•©
        
        Args:
            character_profile: ìºë¦­í„° í”„ë¡œí•„ ì •ë³´
            
        Returns:
            ìƒì„±ëœ ì–´íˆ¬ ì‘ë‹µ
        """
        if not self.async_openai_client:
            raise Exception("OpenAI í´ë¼ì´ì–¸íŠ¸ê°€ ì´ˆê¸°í™”ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤")
        
        start_time = time.time()
        
        # ìºë¦­í„° ì´ë¦„ê³¼ ì„±ê²© ì¶”ì¶œ
        character_name = character_profile.get('name', 'ìºë¦­í„°')
        personality = character_profile.get('personality', 'ì¹œê·¼í•˜ê³  í™œë°œí•œ ì„±ê²©')
        description = character_profile.get('description', '')
        age_range = character_profile.get('age_range', 'ì•Œ ìˆ˜ ì—†ìŒ')
        gender = character_profile.get('gender', 'NON_BINARY')
        mbti = character_profile.get('mbti')
        
        logger.info(f"ğŸš€ ì–´íˆ¬ ìƒì„± ì‹œì‘: {character_name}")
        
        # 1. ì§ˆë¬¸ ìƒì„±
        question = await self._generate_question_for_character(character_profile)
        logger.info(f"ğŸ“ ìƒì„±ëœ ì§ˆë¬¸: {question}")
        
        # 2. 3ê°œì˜ ì„œë¡œ ë‹¤ë¥¸ ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ ìƒì„±
        try:
            system_prompts = await self._create_three_distinct_system_prompts(character_profile)
            logger.info("âœ… 3ê°œ ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ ìƒì„± ì„±ê³µ")
        except Exception as e:
            logger.warning(f"ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ ìƒì„± ì‹¤íŒ¨, í´ë°± ë°©ì‹ ì‚¬ìš©: {e}")
            # í´ë°±: ê¸°ì¡´ ë°©ì‹ìœ¼ë¡œ 3ê°œ ìƒì„±
            system_prompts = await asyncio.gather(*[
                self._create_character_prompt_for_tone(character_profile, i + 1)
                for i in range(3)
            ])
        
        # 3. 3ê°€ì§€ ì–´íˆ¬ë¡œ ì‘ë‹µ ìƒì„± (ë³‘ë ¬ ì²˜ë¦¬)
        try:
            # ë³‘ë ¬ë¡œ 3ê°œ ì–´íˆ¬ ìƒì„±
            response_tasks = [
                self._generate_single_tone_response(
                    character_profile=character_profile,
                    question=question,
                    system_prompt=system_prompt,
                    tone_num=i+1
                )
                for i, system_prompt in enumerate(system_prompts)
            ]
            tone_results = await asyncio.gather(*response_tasks, return_exceptions=True)
            
            # ê²°ê³¼ ì •ë¦¬
            responses = {}
            for i, result in enumerate(tone_results):
                tone_name = f"ë§íˆ¬{i+1}"
                if isinstance(result, Exception):
                    logger.error(f"ë§íˆ¬ {i+1} ìƒì„± ì‹¤íŒ¨: {result}")
                    responses[tone_name] = [{
                        "text": f"ì£„ì†¡í•©ë‹ˆë‹¤. ë§íˆ¬{i+1} ì‘ë‹µ ìƒì„±ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤.",
                        "hashtags": f"#ì˜¤ë¥˜ #ë§íˆ¬{i+1}",
                        "description": f"ìƒì„± ì‹¤íŒ¨í•œ ë§íˆ¬{i+1}",
                        "system_prompt": system_prompts[i]
                    }]
                else:
                    result['system_prompt'] = system_prompts[i]
                    responses[tone_name] = [result]
            
            generation_time = time.time() - start_time
            logger.info(f"âœ… ì–´íˆ¬ ìƒì„± ì™„ë£Œ: {generation_time:.2f}ì´ˆ")
            
            return {
                "question": question,
                "responses": responses,
                "generation_time_seconds": generation_time,
                "method": "integrated_backend"
            }
            
        except Exception as e:
            logger.error(f"âŒ ì–´íˆ¬ ìƒì„± ì‹¤íŒ¨: {e}")
            raise Exception(f"ì–´íˆ¬ ìƒì„± ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}")
    
    async def _generate_question_for_character(self, character_profile: Dict[str, Any]) -> str:
        """ìºë¦­í„°ì— ë§ëŠ” ì§ˆë¬¸ ìƒì„±"""
        prompt = f"""
        {character_profile['name']}ë¼ëŠ” ìºë¦­í„°ì—ê²Œ ë¬¼ì–´ë³¼ ë§Œí•œ í¥ë¯¸ë¡œìš´ ì§ˆë¬¸ì„ í•˜ë‚˜ ë§Œë“¤ì–´ì£¼ì„¸ìš”.
        
        ìºë¦­í„° ì •ë³´:
        - ì´ë¦„: {character_profile['name']}
        - ì„±ê²©: {character_profile.get('personality', 'ì•Œ ìˆ˜ ì—†ìŒ')}
        - ì„¤ëª…: {character_profile.get('description', 'ì—†ìŒ')}
        
        ìš”êµ¬ì‚¬í•­:
        - ìºë¦­í„°ì˜ ê°œì„±ì´ ë“œëŸ¬ë‚  ìˆ˜ ìˆëŠ” ì§ˆë¬¸
        - ì¼ìƒì ì´ë©´ì„œë„ ë‹¤ì–‘í•œ ë‹µë³€ì´ ê°€ëŠ¥í•œ ì§ˆë¬¸
        - 20ì ì´ë‚´ì˜ ê°„ë‹¨í•œ ì§ˆë¬¸
        
        ì§ˆë¬¸ë§Œ ë°˜í™˜í•˜ì„¸ìš”.
        """
        
        response = await self.async_openai_client.chat.completions.create(
            model="gpt-4o-mini",
            messages=[{"role": "user", "content": prompt}],
            temperature=0.8,
            max_tokens=50
        )
        
        return response.choices[0].message.content.strip()
    
    async def _create_three_distinct_system_prompts(self, character_profile: Dict[str, Any]) -> List[str]:
        """3ê°œì˜ ì„œë¡œ ë‹¤ë¥¸ ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ë¥¼ í•œ ë²ˆì— ìƒì„±"""
        prompt = f"""
        {character_profile['name']}ë¼ëŠ” ìºë¦­í„°ì˜ 3ê°€ì§€ ì„œë¡œ ë‹¤ë¥¸ ë§íˆ¬ ìŠ¤íƒ€ì¼ì„ ë§Œë“¤ì–´ì£¼ì„¸ìš”.
        
        ìºë¦­í„° ì •ë³´:
        - ì´ë¦„: {character_profile['name']}
        - ì„±ê²©: {character_profile.get('personality', 'ì¹œê·¼í•˜ê³  í™œë°œí•œ ì„±ê²©')}
        - ë‚˜ì´ëŒ€: {character_profile.get('age_range', 'ì•Œ ìˆ˜ ì—†ìŒ')}
        - ì„±ë³„: {character_profile.get('gender', 'ì—†ìŒ')}
        - MBTI: {character_profile.get('mbti', 'ì•Œ ìˆ˜ ì—†ìŒ')}
        - ì„¤ëª…: {character_profile.get('description', 'ì—†ìŒ')}
        
        ê° ë§íˆ¬ëŠ” ë‹¤ìŒ í˜•ì‹ìœ¼ë¡œ ì‘ì„±í•´ì£¼ì„¸ìš”:
        [ë§íˆ¬1]
        êµ¬ì²´ì ì¸ ë§íˆ¬ ì§€ì‹œì‚¬í•­...
        
        [ë§íˆ¬2]
        êµ¬ì²´ì ì¸ ë§íˆ¬ ì§€ì‹œì‚¬í•­...
        
        [ë§íˆ¬3]
        êµ¬ì²´ì ì¸ ë§íˆ¬ ì§€ì‹œì‚¬í•­...
        
        ìš”êµ¬ì‚¬í•­:
        - ê° ë§íˆ¬ëŠ” ì„œë¡œ í™•ì—°íˆ êµ¬ë³„ë˜ì–´ì•¼ í•¨
        - ìºë¦­í„°ì˜ ê°œì„±ì´ ì˜ ë“œëŸ¬ë‚˜ì•¼ í•¨
        - êµ¬ì²´ì ì¸ ì–´ë¯¸, ë§íˆ¬, íŠ¹ì§•ì„ í¬í•¨
        """
        
        response = await self.async_openai_client.chat.completions.create(
            model="gpt-4o-mini",
            messages=[{"role": "user", "content": prompt}],
            temperature=0.9,
            max_tokens=800
        )
        
        content = response.choices[0].message.content
        
        # ë§íˆ¬ íŒŒì‹±
        prompts = []
        pattern = r'\[ë§íˆ¬\d+\]\s*([^\[]+)'
        matches = re.findall(pattern, content, re.DOTALL)
        
        for match in matches[:3]:
            system_prompt = f"ë‹¹ì‹ ì€ {character_profile['name']}ì…ë‹ˆë‹¤. {match.strip()}"
            prompts.append(system_prompt)
        
        # 3ê°œê°€ ì•ˆ ë˜ë©´ ê¸°ë³¸ê°’ ì¶”ê°€
        while len(prompts) < 3:
            prompts.append(f"ë‹¹ì‹ ì€ {character_profile['name']}ì…ë‹ˆë‹¤. ì¹œê·¼í•˜ê³  í™œë°œí•œ ì„±ê²©ìœ¼ë¡œ ëŒ€í™”í•˜ì„¸ìš”.")
        
        return prompts
    
    async def _create_character_prompt_for_tone(self, character_profile: Dict[str, Any], tone_num: int) -> str:
        """íŠ¹ì • í†¤ ë²ˆí˜¸ì— ëŒ€í•œ ìºë¦­í„° í”„ë¡¬í”„íŠ¸ ìƒì„± (í´ë°±ìš©)"""
        base_prompt = f"ë‹¹ì‹ ì€ {character_profile['name']}ì…ë‹ˆë‹¤."
        
        tone_variations = [
            "ì¹œê·¼í•˜ê³  í™œë°œí•œ ë§íˆ¬ë¡œ ëŒ€í™”í•˜ì„¸ìš”.",
            "ì°¨ë¶„í•˜ê³  ì§€ì ì¸ ë§íˆ¬ë¡œ ëŒ€í™”í•˜ì„¸ìš”.",
            "ìœ ë¨¸ëŸ¬ìŠ¤í•˜ê³  ì¬ì¹˜ìˆëŠ” ë§íˆ¬ë¡œ ëŒ€í™”í•˜ì„¸ìš”."
        ]
        
        if 1 <= tone_num <= 3:
            return f"{base_prompt} {tone_variations[tone_num-1]}"
        else:
            return f"{base_prompt} ììœ ë¡­ê²Œ ëŒ€í™”í•˜ì„¸ìš”."
    
    async def _generate_single_tone_response(
        self,
        character_profile: Dict[str, Any],
        question: str,
        system_prompt: str,
        tone_num: int
    ) -> Dict[str, Any]:
        """ë‹¨ì¼ ì–´íˆ¬ë¡œ ì‘ë‹µ ìƒì„±"""
        try:
            # ì‘ë‹µ ìƒì„±
            response = await self.async_openai_client.chat.completions.create(
                model="gpt-4o-mini",
                messages=[
                    {"role": "system", "content": system_prompt},
                    {"role": "user", "content": question}
                ],
                temperature=0.8,
                max_tokens=150
            )
            
            text = response.choices[0].message.content.strip()
            
            # ìš”ì•½ ì •ë³´ ìƒì„±
            summary_prompt = f"""
            ë‹¤ìŒ ë§íˆ¬ ì§€ì‹œì‚¬í•­ì„ ë¶„ì„í•´ì„œ JSON í˜•ì‹ìœ¼ë¡œ ìš”ì•½í•´ì£¼ì„¸ìš”:
            
            ë§íˆ¬ ì§€ì‹œì‚¬í•­:
            {system_prompt}
            
            ë‹¤ìŒ í˜•ì‹ìœ¼ë¡œ ë‹µë³€í•˜ì„¸ìš”:
            {{
                "hashtags": "#í•´ì‹œíƒœê·¸1 #í•´ì‹œíƒœê·¸2 #í•´ì‹œíƒœê·¸3",
                "description": "ì´ ë§íˆ¬ì˜ íŠ¹ì§•ì„ í•œ ë¬¸ì¥ìœ¼ë¡œ ì„¤ëª…"
            }}
            """
            
            summary_response = await self.async_openai_client.chat.completions.create(
                model="gpt-4o-mini",
                messages=[{"role": "user", "content": summary_prompt}],
                temperature=0.7,
                max_tokens=100
            )
            
            summary_content = summary_response.choices[0].message.content.strip()
            
            # JSON íŒŒì‹± ì‹œë„
            try:
                summary_data = json.loads(summary_content)
                hashtags = summary_data.get('hashtags', f'#ë§íˆ¬{tone_num}')
                description = summary_data.get('description', f'ë§íˆ¬{tone_num}ì˜ íŠ¹ì§•')
            except:
                # íŒŒì‹± ì‹¤íŒ¨ ì‹œ ê¸°ë³¸ê°’
                hashtags = f'#ë§íˆ¬{tone_num} #ê°œì„±ìˆëŠ” #ìºë¦­í„°'
                description = f'{character_profile["name"]}ì˜ ë§íˆ¬{tone_num}'
            
            return {
                "text": text,
                "hashtags": hashtags,
                "description": description
            }
            
        except Exception as e:
            logger.error(f"ë§íˆ¬ {tone_num} ìƒì„± ì¤‘ ì˜¤ë¥˜: {e}")
            raise e


# ì „ì—­ ì„œë¹„ìŠ¤ ì¸ìŠ¤í„´ìŠ¤
qa_generation_service = QAGenerationService()


def get_qa_generation_service() -> QAGenerationService:
    """QA ìƒì„± ì„œë¹„ìŠ¤ ì˜ì¡´ì„± ì£¼ì…ìš© í•¨ìˆ˜"""
    return qa_generation_service