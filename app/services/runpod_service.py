"""
RunPod ì„œë²„ ê´€ë¦¬ ì„œë¹„ìŠ¤
ComfyUIê°€ ì„¤ì¹˜ëœ ì„œë²„ ì¸ìŠ¤í„´ìŠ¤ë¥¼ ë™ì ìœ¼ë¡œ ìƒì„±/ê´€ë¦¬
"""

import asyncio
import aiohttp
import logging
import json
from typing import Dict, Optional, Any
from datetime import datetime, timedelta
from pydantic import BaseModel
from app.core.config import settings

logger = logging.getLogger(__name__)


class RunPodPodRequest(BaseModel):
    """RunPod ì¸ìŠ¤í„´ìŠ¤ ìƒì„± ìš”ì²­"""
    name: str
    template_id: str
    gpu_type: str = "NVIDIA RTX A6000"
    gpu_count: int = 1
    container_disk_in_gb: int = 20  # 20GBë¡œ ê³ ì • (ì´ë¯¸ì§€ ìƒì„± ì•ˆì •ì„± í–¥ìƒ)
    volume_in_gb: int = 0
    ports: str = "8188/http"  # ComfyUI ê¸°ë³¸ í¬íŠ¸
    env: Dict[str, str] = {}


class RunPodPodResponse(BaseModel):
    """RunPod ì¸ìŠ¤í„´ìŠ¤ ì •ë³´"""
    pod_id: str
    status: str  # STARTING, RUNNING, STOPPED, FAILED
    runtime: Optional[Dict[str, Any]] = None
    endpoint_url: Optional[str] = None
    cost_per_hour: Optional[float] = None


class RunPodService:
    """RunPod API ì„œë¹„ìŠ¤"""
    
    def __init__(self):
        self.api_key = settings.RUNPOD_API_KEY
        self.base_url = "https://api.runpod.io/graphql"
        self.template_id = settings.RUNPOD_TEMPLATE_ID
        
        # Mock ëª¨ë“œ ì œê±° - ì‹¤ì œ API í‚¤ê°€ ì—†ìœ¼ë©´ ì˜¤ë¥˜ ë°œìƒ
        if not self.api_key or not self.template_id:
            raise ValueError(
                f"RunPod ì„¤ì •ì´ í•„ìš”í•©ë‹ˆë‹¤: "
                f"API_KEY={'ì„¤ì •ë¨' if self.api_key else 'ì—†ìŒ'}, "
                f"TEMPLATE_ID={'ì„¤ì •ë¨' if self.template_id else 'ì—†ìŒ'}"
            )
        
        logger.info(f"RunPod Service initialized (API Key: {'***' + self.api_key[-4:] if len(self.api_key) > 4 else '***'}, Template: {self.template_id})")
    
    def _generate_proxy_url(self, pod_id: str, internal_port: int = 8188) -> str:
        """RunPod proxy URL ìƒì„± """
        return f"https://{pod_id}-{internal_port}.proxy.runpod.net"
    
    async def create_pod(self, request_id: str) -> RunPodPodResponse:
        """ComfyUI ì„œë²„ ì¸ìŠ¤í„´ìŠ¤ ìƒì„± (GPU í´ë°± ì§€ì›)"""
        
        # GPU ë¦¬ì†ŒìŠ¤ ì‹¤ì œ ìŠ¤í™ ì„¤ì • (RunPod ì´ë¯¸ì§€ ê¸°ì¤€ - ê° GPUë³„ ê³ ìœ  ë¦¬ì†ŒìŠ¤)
        # ì»¨í…Œì´ë„ˆ ë””ìŠ¤í¬ë§Œ 20GBë¡œ ê³ ì •, ë‚˜ë¨¸ì§€ëŠ” GPUë³„ ì‹¤ì œ ìŠ¤í™ ì‚¬ìš©
        FIXED_DISK = 20     # ê³ ì • ì»´í…Œì´ë„ˆ ë””ìŠ¤í¬ (GB) - ì•ˆì •ì„±ì„ ìœ„í•´ ê³ ì •
        FIXED_VOLUME = 250  # ê³ ì • ë„¤íŠ¸ì›Œí¬ ë³¼ë¥¨ (GB)
        
        gpu_config = [
            # 1ìˆœìœ„: RTX 4090 (ì‹¤ì œ ìŠ¤í™: 24GB VRAM, 61GB RAM, 16 vCPU)
            {
                "gpu": "NVIDIA GeForce RTX 4090", 
                "vram": "24GB", 
                "bid_price": 0.69,
                "tier": "Premium",
                "vcpu": 16,   # ì‹¤ì œ RunPod ìŠ¤í™
                "memory": 61  # ì‹¤ì œ RunPod ìŠ¤í™
            },
            
            # 2ìˆœìœ„: RTX A4500 (ì‹¤ì œ ìŠ¤í™: 20GB VRAM, 54GB RAM, 12 vCPU)
            {
                "gpu": "NVIDIA RTX A4500", 
                "vram": "20GB", 
                "bid_price": 0.25,
                "tier": "Medium",
                "vcpu": 12,   # ì‹¤ì œ RunPod ìŠ¤í™
                "memory": 54  # ì‹¤ì œ RunPod ìŠ¤í™
            },
            
            # 3ìˆœìœ„: RTX 4000 Ada (ì‹¤ì œ ìŠ¤í™: 20GB VRAM, 50GB RAM, 9 vCPU)
            {
                "gpu": "NVIDIA RTX 4000 Ada Generation", 
                "vram": "20GB", 
                "bid_price": 0.26,
                "tier": "Medium",
                "vcpu": 9,    # ì‹¤ì œ RunPod ìŠ¤í™
                "memory": 50  # ì‹¤ì œ RunPod ìŠ¤í™
            },
            
            # 4ìˆœìœ„: RTX 2000 Ada (ì‹¤ì œ ìŠ¤í™: 16GB VRAM, 31GB RAM, 6 vCPU)
            {
                "gpu": "NVIDIA RTX 2000 Ada Generation", 
                "vram": "16GB", 
                "bid_price": 0.23,
                "tier": "Budget",
                "vcpu": 6,    # ì‹¤ì œ RunPod ìŠ¤í™
                "memory": 31  # ì‹¤ì œ RunPod ìŠ¤í™
            }
        ]
        
        logger.info(f"ğŸš€ ===== RunPod ì¸ìŠ¤í„´ìŠ¤ ìƒì„± ì‹œì‘ =====")
        logger.info(f"ğŸ¯ Request ID: {request_id}")
        logger.info(f"ğŸ—ºï¸ Template ID: {settings.RUNPOD_TEMPLATE_ID}")
        logger.info(f"ğŸ’¾ Volume ID: {settings.RUNPOD_VOLUME_ID}")
        logger.info(f"ğŸŒ Data Center: EU-RO-1 (ê°•ì œ ì„¤ì •)")
        logger.info(f"ğŸ”§ ë””ìŠ¤í¬ ê³ ì • ì„¤ì •: {FIXED_DISK}GB Container + {FIXED_VOLUME}GB Volume")
        logger.info(f"ğŸ” ì‚¬ìš© ê°€ëŠ¥í•œ GPU ì˜µì…˜: {len(gpu_config)}ê°œ (GPUë³„ ì‹¤ì œ ë¦¬ì†ŒìŠ¤ ìŠ¤í™)")
        logger.info(f"ğŸ¯ GPU ìš°ì„ ìˆœìœ„: RTX 4090(16C/61GB) â†’ RTX A4500(12C/54GB) â†’ RTX 4000 Ada(9C/50GB) â†’ RTX 2000 Ada(6C/31GB)")
        
        gpu_chain = [config["gpu"] for config in gpu_config]
        
        for gpu_index in range(len(gpu_config)):  # ê° GPUë³„ë¡œ ìˆœì°¨ì  ì‹œë„
            current_gpu = gpu_config[gpu_index]
            gpu_type = current_gpu["gpu"]
            base_bid_price = current_gpu["bid_price"]
            tier = current_gpu["tier"]
            vram = current_gpu["vram"]
            vcpu = current_gpu["vcpu"]
            memory = current_gpu["memory"]
            
            # RTX 4090ì˜ ê²½ìš° ë™ì¼ ì¡°ê±´ìœ¼ë¡œ 3ë²ˆ ì¬ì‹œë„, ë‹¤ë¥¸ GPUëŠ” 1ë²ˆë§Œ
            max_gpu_retries = 3 if "RTX 4090" in gpu_type or "4090" in gpu_type else 1
            
            logger.info(f"ğŸ” GPU #{gpu_index + 1}/{len(gpu_config)}: {gpu_type}")
            logger.info(f"   ğŸ¯ GPU: {gpu_type} ({vram} VRAM, {tier} ë“±ê¸‰)")
            logger.info(f"   ğŸ® ë¦¬ì†ŒìŠ¤ ê³ ì • ìŠ¤í™: {vcpu} vCPU, {memory}GB RAM (ì •í™•í•œ RunPod ìŠ¤í™)")
            logger.info(f"   ğŸ’° ê¸°ë³¸ ì…ì°°ê°€: ${base_bid_price}/hr")
            logger.info(f"   ğŸ’¾ ë””ìŠ¤í¬: {FIXED_DISK}GB Container + {FIXED_VOLUME}GB Network Volume (ê³ ì •)")
            logger.info(f"   ğŸ”„ GPUë³„ ì¬ì‹œë„: {max_gpu_retries}íšŒ (RTX 4090ì€ 3íšŒ, ë‚˜ë¨¸ì§€ëŠ” 1íšŒ)")
            
            pod_data = None
            successful_bid = None
            
            # GPUë³„ ì¬ì‹œë„ ë£¨í”„ (RTX 4090ì€ 3íšŒ, ë‚˜ë¨¸ì§€ëŠ” 1íšŒ)
            for gpu_retry in range(max_gpu_retries):
                if gpu_retry > 0:
                    logger.info(f"   ğŸ”„ GPU ì¬ì‹œë„ #{gpu_retry + 1}/{max_gpu_retries} - ë™ì¼ ì¡°ê±´({vcpu}C/{memory}GB)ìœ¼ë¡œ ì¬ìƒì„±")
                    await asyncio.sleep(3)  # GPU ì¬ì‹œë„ ê°„ 3ì´ˆ ëŒ€ê¸°
                
                # ê° GPUë³„ë¡œ ë‹¨ê³„ë³„ ì…ì°°ê°€ ì‹œë„ (ê¸°ë³¸ê°€ â†’ ìµœëŒ€ $0.6)
                bid_steps = [
                    base_bid_price,           # ê¸°ë³¸ ì‹œì¥ê°€
                    base_bid_price + 0.1,     # +$0.1
                    base_bid_price + 0.2,     # +$0.2 
                    0.6                       # ìµœëŒ€ $0.6
                ]
                # ì¤‘ë³µ ì œê±° ë° $0.6 ì´í•˜ë¡œ ì œí•œ
                bid_steps = sorted(list(set([min(price, 0.6) for price in bid_steps])))
                
                # ê° ì…ì°°ê°€ë³„ë¡œ ì‹œë„ (ì¬ì‹œë„ ë¡œì§ í¬í•¨)
                for bid_idx, bid_price in enumerate(bid_steps):
                    logger.info(f"     ğŸ’° ì…ì°°ê°€ ${bid_price}/hr ì‹œë„ ({bid_idx + 1}/{len(bid_steps)})...")
                    
                    # Spot ë¨¼ì €, ì‹¤íŒ¨ì‹œ On-Demand ì‹œë„
                    for instance_type in ["on_demand", "interruptible"]:
                        # ê° ì¸ìŠ¤í„´ìŠ¤ íƒ€ì…ë³„ë¡œ ìµœëŒ€ 2ë²ˆ ì‹œë„ (ì¦‰ì‹œ + 5ì´ˆ í›„ ì¬ì‹œë„)
                        for retry_attempt in range(2):
                            try:
                                if retry_attempt > 0:
                                    logger.info(f"       ğŸ”„ 5ì´ˆ í›„ ì¬ì‹œë„ ({retry_attempt + 1}/2)...")
                                    await asyncio.sleep(5)
                                
                                logger.info(f"       ğŸ”„ {instance_type} ì¸ìŠ¤í„´ìŠ¤ ìƒì„± ì¤‘... (ê³ ì • ë¦¬ì†ŒìŠ¤: {vcpu}C/{memory}GB)")
                                pod_data = await self._create_pod_with_type(gpu_type, request_id, instance_type, bid_price, vcpu, memory)
                                
                                if pod_data:
                                    successful_bid = bid_price
                                    logger.info(f"âœ… {instance_type} Pod ìƒì„± ì„±ê³µ! (GPU ì¬ì‹œë„ {gpu_retry + 1}/{max_gpu_retries})")
                                    logger.info(f"   ğŸ¯ GPU: {gpu_type} (${bid_price}/hr)")
                                    logger.info(f"   ğŸ® ì ìš©ëœ ë¦¬ì†ŒìŠ¤: {vcpu} vCPU, {memory}GB RAM (ê³ ì • ìŠ¤í™)")
                                    logger.info(f"   ğŸ’¾ ë””ìŠ¤í¬: {FIXED_DISK}GB Container + {FIXED_VOLUME}GB Network Volume")
                                    logger.info(f"   ğŸ†” Pod ID: {pod_data['id']}")
                                    logger.info(f"   ğŸŒ Status: {pod_data.get('desiredStatus', 'Unknown')}")
                                    break
                                    
                            except Exception as type_error:
                                error_msg = str(type_error)[:100]
                                logger.warning(f"       âŒ {instance_type} ${bid_price}/hr ì‹œë„ {retry_attempt + 1} ì‹¤íŒ¨: {error_msg}...")
                                
                                # ë§ˆì§€ë§‰ ì‹œë„ê°€ ì•„ë‹ˆë©´ ì¬ì‹œë„, ì•„ë‹ˆë©´ ë‹¤ìŒ ì¸ìŠ¤í„´ìŠ¤ íƒ€ì…ìœ¼ë¡œ
                                if retry_attempt == 1:  # ë§ˆì§€ë§‰ ì¬ì‹œë„ë„ ì‹¤íŒ¨
                                    break
                                continue
                        
                        if pod_data:  # ì„±ê³µí•˜ë©´ ì¸ìŠ¤í„´ìŠ¤ íƒ€ì… ë£¨í”„ ì¢…ë£Œ
                            break
                    
                    if pod_data:  # ì„±ê³µí•˜ë©´ ì…ì°°ê°€ ë£¨í”„ ì¢…ë£Œ
                        break
                
                if pod_data:  # ì„±ê³µí•˜ë©´ GPU ì¬ì‹œë„ ë£¨í”„ ì¢…ë£Œ
                    break
                else:
                    logger.warning(f"   âŒ GPU ì¬ì‹œë„ {gpu_retry + 1}/{max_gpu_retries} ì‹¤íŒ¨ (ëª¨ë“  ì…ì°°ê°€ ì‹œë„ ì™„ë£Œ)")
                    if gpu_retry < max_gpu_retries - 1:
                        logger.info(f"   ğŸ”„ ë™ì¼ GPUë¡œ ì¬ì‹œë„ ì˜ˆì •... (ë¦¬ì†ŒìŠ¤ ê³ ì •: {vcpu}C/{memory}GB)")
            
            if not pod_data:
                if "RTX 4090" in gpu_type or "4090" in gpu_type:
                    logger.warning(f"ğŸ’¸ RTX 4090 ({vcpu}C/{memory}GB) 3íšŒ ì¬ì‹œë„ ëª¨ë‘ ì‹¤íŒ¨")
                    logger.warning(f"   ğŸ“Š ì‹œë„ ê²°ê³¼: GPU ì¬ì‹œë„ 3íšŒ Ã— ì…ì°°ê°€ {len(bid_steps)}ê°œ Ã— ì¸ìŠ¤í„´ìŠ¤ íƒ€ì… 2ê°œ Ã— ì¬ì‹œë„ 2íšŒ")
                    logger.warning(f"   âš ï¸ ë¦¬ì†ŒìŠ¤ ë³€ê²½ ì—†ì´ ì •í™•í•œ ìŠ¤í™ìœ¼ë¡œë§Œ ì‹œë„í•¨")
                else:
                    logger.warning(f"ğŸ’¸ GPU {gpu_type} ({vcpu}C/{memory}GB) ì…ì°°ê°€ ì‹œë„ ì‹¤íŒ¨")
                
                logger.warning(f"   ì‹œë„í•œ ì…ì°°ê°€: ${bid_steps}")
                
                if gpu_index < len(gpu_config) - 1:
                    next_gpu = gpu_config[gpu_index + 1]
                    logger.info(f"   ğŸ”„ ë‹¤ìŒ GPUë¡œ í´ë°±: {next_gpu['gpu']} ({next_gpu['vcpu']}C/{next_gpu['memory']}GB)")
                continue  # ë‹¤ìŒ GPUë¡œ ì‹œë„
            
            # Pod ìƒì„± ì„±ê³µí•œ ê²½ìš°
            if pod_data:
                # RunPod proxy URL ìƒì„± (ê³µì‹ ë°©ì‹)
                pod_id = pod_data["id"]
                endpoint_url = self._generate_proxy_url(pod_id, 8188)
                
                logger.info(f"ğŸ¯ ìµœì  GPU í™•ë³´ ì„±ê³µ!")
                logger.info(f"  GPU: {gpu_type} ({vram}, {tier} ë“±ê¸‰)")
                logger.info(f"  ë¦¬ì†ŒìŠ¤: {vcpu} vCPU, {memory}GB RAM (ê³ ì •)")
                logger.info(f"  ìµœì¢… ì…ì°°ê°€: ${successful_bid}/hr")
                logger.info(f"  Pod ID: {pod_id}")
                logger.info(f"  Proxy URL: {endpoint_url}")
                logger.info(f"  ë³¼ë¥¨: {settings.RUNPOD_VOLUME_ID} â†’ /workspace")
                
                # Pod ìƒì„± í›„ ìë™ìœ¼ë¡œ ì‹œì‘
                logger.info(f"ğŸš€ Pod {pod_id} ìë™ ì‹œì‘ ì¤‘...")
                logger.info(f"   Template ID: {settings.RUNPOD_TEMPLATE_ID}")
                logger.info(f"   Volume ID: {settings.RUNPOD_VOLUME_ID}")
                logger.info(f"   Container Port: 8188 (ComfyUI)")
                logger.info(f"   Proxy URL: {endpoint_url}")
                
                start_success = await self._start_pod(pod_id)
                if start_success:
                    logger.info(f"âœ… Pod {pod_id} ìë™ ì‹œì‘ ì„±ê³µ")
                    logger.info(f"   ComfyUI ì ‘ê·¼: {endpoint_url}")
                    logger.info(f"   ì˜ˆìƒ ì¤€ë¹„ ì‹œê°„: 60-90ì´ˆ")
                else:
                    logger.warning(f"âš ï¸ Pod {pod_id} ìë™ ì‹œì‘ ì‹¤íŒ¨")
                    logger.warning(f"   ìˆ˜ë™ ì‹œì‘ì´ í•„ìš”í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤")
                
                return RunPodPodResponse(
                    pod_id=pod_id,
                    status="STARTING" if start_success else pod_data["desiredStatus"],
                    runtime=pod_data.get("runtime", {}),
                    endpoint_url=endpoint_url,
                    cost_per_hour=successful_bid  # ì‹¤ì œ ì…ì°°ê°€ ë°˜ì˜
                )
        
        # ëª¨ë“  GPU ì‹œë„ ì‹¤íŒ¨ - RTX 4090 3íšŒ ì¬ì‹œë„ í¬í•¨
        
        # ëª¨ë“  ì‹œë„ ì‹¤íŒ¨ - RTX 4090 3íšŒ ì¬ì‹œë„ í¬í•¨ ìƒì„¸ ìš”ì•½ ë¦¬í¬íŠ¸
        logger.error(f"âŒ ===== RunPod ì¸ìŠ¤í„´ìŠ¤ ìƒì„± ì‹¤íŒ¨ =====")
        logger.error(f"   ğŸ¯ ì‹œë„í•œ GPU ìˆ˜: {len(gpu_config)}ê°œ")
        logger.error(f"   ğŸ”„ RTX 4090 ì¬ì‹œë„: 3íšŒ (16C/61GB ê³ ì • ìŠ¤í™)")
        logger.error(f"   ğŸ”„ ê¸°íƒ€ GPU ì¬ì‹œë„: 1íšŒ (ê°ê° ê³ ì • ìŠ¤í™)")
        logger.error(f"   ğŸ’° ìµœëŒ€ ì…ì°°ê°€: $0.60/hr")
        logger.error(f"   ğŸ“‹ Request ID: {request_id}")
        logger.error(f"   ğŸ—‚ï¸ Template: {settings.RUNPOD_TEMPLATE_ID}")
        logger.error(f"   ğŸ’¾ Volume: {settings.RUNPOD_VOLUME_ID}")
        logger.error(f"   ğŸ“Š ì´ ì‹œë„ íšŸìˆ˜:")
        rtx_4090_total = 3 * 4 * 2 * 2  # GPUì¬ì‹œë„ 3íšŒ Ã— ì…ì°°ê°€ 4ê°œ Ã— ì¸ìŠ¤í„´ìŠ¤íƒ€ì… 2ê°œ Ã— ì¬ì‹œë„ 2íšŒ
        other_gpu_total = (len(gpu_config) - 1) * 1 * 4 * 2 * 2  # ê¸°íƒ€ GPU Ã— 1íšŒ Ã— ì…ì°°ê°€ Ã— íƒ€ì… Ã— ì¬ì‹œë„
        logger.error(f"     - RTX 4090: ìµœëŒ€ {rtx_4090_total}íšŒ ì‹œë„")
        logger.error(f"     - ê¸°íƒ€ GPU: ìµœëŒ€ {other_gpu_total}íšŒ ì‹œë„")
        logger.error(f"   âš ï¸ ë¦¬ì†ŒìŠ¤ ë³€ê²½ ì—†ì´ ì •í™•í•œ RunPod ìŠ¤í™ìœ¼ë¡œë§Œ ì‹œë„í•¨")
        logger.error(f"   ğŸ’¡ ê¶Œì¥ ì‚¬í•­:")
        logger.error(f"     1. RunPod ê³„ì • ì”ì•¡ í™•ì¸")
        logger.error(f"     2. 5-10ë¶„ í›„ ì¬ì‹œë„")
        logger.error(f"     3. ë‹¤ë¥¸ ì‹œê°„ëŒ€(ë¯¸êµ­ ë°¤ì‹œê°„)ì— ì‹œë„")
        logger.error(f"     4. GPU ë¦¬ì†ŒìŠ¤ ìš”êµ¬ì‚¬í•­ í™•ì¸")
        raise RuntimeError("ì§€ê¸ˆ ì‚¬ìš©ê°€ëŠ¥í•œ ìì›ì´ ì—†ìŠµë‹ˆë‹¤. ì ì‹œ í›„ ë‹¤ì‹œ ì‹œë„í•´ ì£¼ì„¸ìš”.")
    
    async def _create_pod_with_type(self, gpu_type: str, request_id: str, instance_type: str, bid_price: float = 0.35, vcpu: int = 8, memory: int = 32) -> dict:
        """íŠ¹ì • ì¸ìŠ¤í„´ìŠ¤ íƒ€ì…ìœ¼ë¡œ Pod ìƒì„± (ê°•ë ¥í•œ ë¦¬ì†ŒìŠ¤ ê³ ì •)"""
        
        # RTX 4090 ë¦¬ì†ŒìŠ¤ ê°•ì œ ê³ ì • - 24C/125GB ê³¼í• ë‹¹ ë°©ì§€
        if "RTX 4090" in gpu_type or "4090" in gpu_type:
            vcpu = 16    # RTX 4090 ê°•ì œ ê³ ì •: 16 vCPU
            memory = 61  # RTX 4090 ê°•ì œ ê³ ì •: 61GB RAM
            logger.info(f"ğŸ”’ RTX 4090 ë¦¬ì†ŒìŠ¤ ê°•ì œ ê³ ì •: {vcpu}C/{memory}GB (ê³¼í• ë‹¹ ë°©ì§€)")
        elif "RTX A4500" in gpu_type or "A4500" in gpu_type:
            vcpu = 12    # RTX A4500 ê°•ì œ ê³ ì •: 12 vCPU
            memory = 54  # RTX A4500 ê°•ì œ ê³ ì •: 54GB RAM
            logger.info(f"ğŸ”’ RTX A4500 ë¦¬ì†ŒìŠ¤ ê°•ì œ ê³ ì •: {vcpu}C/{memory}GB")
        elif "RTX 4000 Ada" in gpu_type or "4000 Ada" in gpu_type:
            vcpu = 9     # RTX 4000 Ada ê°•ì œ ê³ ì •: 9 vCPU
            memory = 50  # RTX 4000 Ada ê°•ì œ ê³ ì •: 50GB RAM
            logger.info(f"ğŸ”’ RTX 4000 Ada ë¦¬ì†ŒìŠ¤ ê°•ì œ ê³ ì •: {vcpu}C/{memory}GB")
        elif "RTX 2000 Ada" in gpu_type or "2000 Ada" in gpu_type:
            vcpu = 6     # RTX 2000 Ada ê°•ì œ ê³ ì •: 6 vCPU
            memory = 31  # RTX 2000 Ada ê°•ì œ ê³ ì •: 31GB RAM
            logger.info(f"ğŸ”’ RTX 2000 Ada ë¦¬ì†ŒìŠ¤ ê°•ì œ ê³ ì •: {vcpu}C/{memory}GB")
        else:
            # ê¸°íƒ€ GPUëŠ” ê¸°ë³¸ê°’ ì‚¬ìš©í•˜ë˜ ìµœëŒ€ê°’ ì œí•œ
            vcpu = min(vcpu, 16)    # ìµœëŒ€ 16 vCPUë¡œ ì œí•œ
            memory = min(memory, 64) # ìµœëŒ€ 64GB RAMìœ¼ë¡œ ì œí•œ
            logger.warning(f"âš ï¸ ì•Œ ìˆ˜ ì—†ëŠ” GPU íƒ€ì…, ë¦¬ì†ŒìŠ¤ ì œí•œ ì ìš©: {vcpu}C/{memory}GB")
        
        if instance_type == "interruptible":
            # Spot ì¸ìŠ¤í„´ìŠ¤ - í…œí”Œë¦¿ ê¸°ë°˜ ìƒì„±ìœ¼ë¡œ ë³€ê²½
            mutation = """
            mutation podRentInterruptable($input: PodRentInterruptableInput!) {
                podRentInterruptable(input: $input) {
                    id
                    desiredStatus
                    runtime {
                        uptimeInSeconds
                        ports {
                            ip
                            isIpPublic
                            privatePort
                            publicPort
                        }
                    }
                    machine {
                        podHostId
                    }
                }
            }
            """
            
            variables = {
                "input": {
                    "bidPerGpu": bid_price,  # GPUë³„ ì ì • ì…ì°°ê°€ ì‚¬ìš©
                    "gpuCount": 1,
                    "volumeInGb": 250,  # ê³ ì • ë„¤íŠ¸ì›Œí¬ ë³¼ë¥¨
                    "networkVolumeId": settings.RUNPOD_VOLUME_ID,
                    "containerDiskInGb": 20,  # ê°•ì œ ê³ ì • ì»¨í…Œì´ë„ˆ ë””ìŠ¤í¬ (ê³¼í• ë‹¹ ë°©ì§€)
                    "minVcpuCount": vcpu,   # ê°•ì œ ê³ ì •ëœ vCPU ìˆ˜
                    "minMemoryInGb": memory,  # ê°•ì œ ê³ ì •ëœ RAM ìš©ëŸ‰
                    "gpuTypeId": gpu_type,
                    "templateId": settings.RUNPOD_TEMPLATE_ID,  # í…œí”Œë¦¿ ì‚¬ìš©
                    "name": f"AIMEX_ComfyUI_{request_id}",
                    "ports": "8188/http",
                    "volumeMountPath": "/workspace",
                    "dataCenterId": "EU-RO-1"
                }
            }
        else:  # on_demand
            # On-Demand ì¸ìŠ¤í„´ìŠ¤ - í…œí”Œë¦¿ ê¸°ë°˜ìœ¼ë¡œ ë³€ê²½
            mutation = """
            mutation podFindAndDeployOnDemand($input: PodFindAndDeployOnDemandInput!) {
                podFindAndDeployOnDemand(input: $input) {
                    id
                    desiredStatus
                    runtime {
                        uptimeInSeconds
                        ports {
                            ip
                            isIpPublic
                            privatePort
                            publicPort
                        }
                    }
                    machine {
                        podHostId
                    }
                }
            }
            """
            
            variables = {
                "input": {
                    "cloudType": "ALL",  # ALL, SECURE, COMMUNITY ì˜µì…˜
                    "gpuCount": 1,
                    "volumeInGb": 200,  # ê³ ì • ë„¤íŠ¸ì›Œí¬ ë³¼ë¥¨
                    "networkVolumeId": settings.RUNPOD_VOLUME_ID,
                    "containerDiskInGb": 20,  # ê°•ì œ ê³ ì • ì»¨í…Œì´ë„ˆ ë””ìŠ¤í¬ (ê³¼í• ë‹¹ ë°©ì§€)
                    "minVcpuCount": vcpu,   # ê°•ì œ ê³ ì •ëœ vCPU ìˆ˜
                    "minMemoryInGb": memory,  # ê°•ì œ ê³ ì •ëœ RAM ìš©ëŸ‰
                    "gpuTypeId": gpu_type,
                    "templateId": settings.RUNPOD_TEMPLATE_ID,  # í…œí”Œë¦¿ ì‚¬ìš©
                    "name": f"AIMEX_ComfyUI_{request_id}",
                    "ports": "8188/http",
                    "volumeMountPath": "/workspace",
                    "dataCenterId": "EU-RO-1"
                }
            }
        
        headers = {
            "Content-Type": "application/json",
            "Authorization": f"Bearer {self.api_key}"
        }
        
        logger.info(f"   ğŸ”’ ê°•ë ¥í•œ ë¦¬ì†ŒìŠ¤ ê³ ì • API ìš”ì²­:")
        logger.info(f"     - GPU: {gpu_type}")
        logger.info(f"     - ì…ì°°ê°€: ${bid_price}/hr")
        logger.info(f"     - vCPU ê³ ì •: min={vcpu}, max={vcpu} (ê³¼í• ë‹¹ ë°©ì§€)")
        logger.info(f"     - RAM ê³ ì •: min={memory}GB, max={memory}GB (ê³¼í• ë‹¹ ë°©ì§€)")
        logger.info(f"     - ì»¨í…Œì´ë„ˆ ë””ìŠ¤í¬: 20GB (ê³ ì •)")
        logger.info(f"     - ë³¼ë¥¨: 200GB (ID: {settings.RUNPOD_VOLUME_ID})")
        logger.info(f"     - ë³¼ë¥¨ ë§ˆìš´íŠ¸: /workspace")
        logger.info(f"     - ë°ì´í„°ì„¼í„°: EU-RO-1 (ê³ ì •)")
        logger.info(f"     - í…œí”Œë¦¿: {settings.RUNPOD_TEMPLATE_ID}")
        if "RTX 4090" in gpu_type:
            logger.info(f"     âš ï¸ RTX 4090 íŠ¹ë³„ ë³´í˜¸: 24C/125GB ê³¼í• ë‹¹ ê°•ë ¥ ì°¨ë‹¨")
        
        async with aiohttp.ClientSession() as session:
            payload = {
                "query": mutation,
                "variables": variables
            }
            
            logger.info(f"   ğŸŒ RunPod GraphQL API í˜¸ì¶œ ì¤‘...")
            
            async with session.post(
                self.base_url,
                json=payload,
                headers=headers,
                timeout=aiohttp.ClientTimeout(total=30)
            ) as response:
                logger.info(f"   ğŸ” API ì‘ë‹µ ìƒíƒœ: {response.status}")
                
                if response.status != 200:
                    response_text = await response.text()
                    logger.error(f"   âŒ HTTP ì˜¤ë¥˜: {response.status}")
                    logger.error(f"   âŒ ì‘ë‹µ ë‚´ìš©: {response_text[:300]}...")
                    raise Exception(f"RunPod API í˜¸ì¶œ ì‹¤íŒ¨: {response.status} - {response_text[:100]}...")
                
                data = await response.json()
                logger.info(f"   âœ… JSON ë°ì´í„° íŒŒì‹± ì„±ê³µ")
                logger.info(data)
                
                if "errors" in data:
                    logger.error(f"   âŒ GraphQL ì˜¤ë¥˜ ë°œê²¬: {data['errors']}")
                    raise Exception(f"RunPod GraphQL ì˜¤ë¥˜: {data['errors']}")
                
                # ì„±ê³µ ë°ì´í„° ì¶”ì¶œ
                if instance_type == "interruptible":
                    result = data["data"]["podRentInterruptable"]
                else:
                    result = data["data"]["podFindAndDeployOnDemand"]
                
                if result:
                    logger.info(f"   âœ… Pod ìƒì„± ì„±ê³µ! ID: {result.get('id', 'N/A')}")
                    logger.info(f"   âœ… ìƒíƒœ: {result.get('desiredStatus', 'N/A')}")
                    
                    # ìš”ì²­í•œ ë¦¬ì†ŒìŠ¤ vs ì‹¤ì œ í• ë‹¹ ë¦¬ì†ŒìŠ¤ í™•ì¸
                    logger.info(f"   ğŸ” ìš”ì²­ ë¦¬ì†ŒìŠ¤: {vcpu}C/{memory}GB (min=max ê³ ì •)")
                    
                    # ë³¼ë¥¨ ë§ˆìš´íŠ¸ í™•ì¸
                    machine_info = result.get('machine', {})
                    if machine_info:
                        logger.info(f"   ğŸ’» ë¨¸ì‹  ì •ë³´: {machine_info.get('podHostId', 'N/A')}")
                    
                    runtime_info = result.get('runtime', {})
                    if runtime_info:
                        logger.info(f"   â±ï¸ ëŸ°íƒ€ì„: {runtime_info.get('uptimeInSeconds', 0)}ì´ˆ")
                    
                    # RTX 4090 íŠ¹ë³„ í™•ì¸
                    if "RTX 4090" in gpu_type:
                        logger.info(f"   ğŸ”’ RTX 4090 ë¦¬ì†ŒìŠ¤ ê³ ì • ì„±ê³µ: {vcpu}C/{memory}GB")
                        logger.info(f"   âœ… 24C/125GB ê³¼í• ë‹¹ ë°©ì§€ë¨")
                else:
                    logger.warning(f"   âš ï¸ ë¹ˆ ê²°ê³¼ ë°˜í™˜ - Pod ìƒì„± ì‹¤íŒ¨ ê°€ëŠ¥ì„±")
                
                return result
    
    async def get_pod_status(self, pod_id: str) -> RunPodPodResponse:
        """Pod ìƒíƒœ ì¡°íšŒ"""
        
        try:
            # GraphQL ì¿¼ë¦¬ - Pod ìƒíƒœ ì¡°íšŒ
            query = """
            query pod($input: PodFilter!) {
                pod(input: $input) {
                    id
                    desiredStatus
                    lastStatusChange
                    runtime {
                        uptimeInSeconds
                        ports {
                            ip
                            isIpPublic
                            privatePort
                            publicPort
                        }
                    }
                }
            }
            """
            
            variables = {
                "input": {
                    "podId": pod_id
                }
            }
            
            headers = {
                "Content-Type": "application/json",
                "Authorization": f"Bearer {self.api_key}"
            }
            
            async with aiohttp.ClientSession() as session:
                payload = {
                    "query": query,
                    "variables": variables
                }
                
                async with session.post(
                    self.base_url,
                    json=payload,
                    headers=headers,
                    timeout=aiohttp.ClientTimeout(total=10)
                ) as response:
                    if response.status != 200:
                        raise Exception(f"RunPod API í˜¸ì¶œ ì‹¤íŒ¨: {response.status}")
                    
                    data = await response.json()
                    pod_data = data["data"]["pod"]
                    
                    # RunPod proxy URL ìƒì„± (ê³µì‹ ë°©ì‹)
                    endpoint_url = self._generate_proxy_url(pod_data["id"], 8188)
                    
                    return RunPodPodResponse(
                        pod_id=pod_data["id"],
                        status=pod_data["desiredStatus"],
                        runtime=pod_data.get("runtime", {}),
                        endpoint_url=endpoint_url
                    )
                    
        except Exception as e:
            logger.error(f"RunPod ìƒíƒœ ì¡°íšŒ ì‹¤íŒ¨: {e}")
            raise RuntimeError(f"RunPod ìƒíƒœ ì¡°íšŒ ì‹¤íŒ¨: {e}")
    
    async def _start_pod(self, pod_id: str) -> bool:
        """Pod ì‹œì‘ (ìë™í™”ìš©)"""
        
        if not pod_id:
            logger.error("Pod IDê°€ ì œê³µë˜ì§€ ì•ŠìŒ")
            return False
        
        try:
            logger.info(f"RunPod {pod_id} ì‹œì‘ API í˜¸ì¶œ ì¤‘...")
            
            # GraphQL ë®¤í…Œì´ì…˜ - Pod ì‹œì‘ (ìƒˆë¡œ ìƒì„±ëœ Podìš©)
            mutation = """
            mutation podStart($input: PodStartInput!) {
                podStart(input: $input)
            }
            """
            
            variables = {
                "input": {
                    "podId": pod_id
                }
            }
            
            headers = {
                "Content-Type": "application/json",
                "Authorization": f"Bearer {self.api_key}"
            }
            
            async with aiohttp.ClientSession() as session:
                payload = {
                    "query": mutation,
                    "variables": variables
                }
                
                async with session.post(
                    self.base_url,
                    json=payload,
                    headers=headers,
                    timeout=aiohttp.ClientTimeout(total=15)
                ) as response:
                    response_text = await response.text()
                    logger.info(f"RunPod ì‹œì‘ API ì‘ë‹µ: {response.status} - {response_text}")
                    
                    if response.status != 200:
                        logger.error(f"RunPod ì‹œì‘ API í˜¸ì¶œ ì‹¤íŒ¨: {response.status}")
                        return False
                    
                    try:
                        data = await response.json()
                        if "errors" in data:
                            logger.error(f"RunPod ì‹œì‘ GraphQL ì˜¤ë¥˜: {data['errors']}")
                            return False
                            
                        result = data.get("data", {}).get("podStart", False)
                        
                        if result:
                            logger.info(f"âœ… RunPod {pod_id} ì‹œì‘ ìš”ì²­ ì„±ê³µ")
                            return True
                        else:
                            logger.error(f"âŒ RunPod {pod_id} ì‹œì‘ ìš”ì²­ ì‹¤íŒ¨: {data}")
                            return False
                            
                    except Exception as json_error:
                        logger.error(f"RunPod ì‹œì‘ ì‘ë‹µ JSON íŒŒì‹± ì‹¤íŒ¨: {json_error}")
                        return False
                    
        except asyncio.TimeoutError:
            logger.error(f"RunPod {pod_id} ì‹œì‘ API íƒ€ì„ì•„ì›ƒ")
            return False
        except Exception as e:
            logger.error(f"RunPod {pod_id} ì‹œì‘ ì¤‘ ì˜ˆì™¸ ë°œìƒ: {e}")
            return False

    async def terminate_pod(self, pod_id: str) -> bool:
        """Pod ì¢…ë£Œ (ê°•í™”ëœ ë¡œì§)"""
        
        if not pod_id:
            logger.error("Pod IDê°€ ì œê³µë˜ì§€ ì•ŠìŒ")
            return False
        
        try:
            logger.info(f"RunPod {pod_id} ì¢…ë£Œ API í˜¸ì¶œ ì¤‘...")
            
            # GraphQL ë®¤í…Œì´ì…˜ - Pod ì¢…ë£Œ
            mutation = """
            mutation podTerminate($input: PodTerminateInput!) {
                podTerminate(input: $input)
            }
            """
            
            variables = {
                "input": {
                    "podId": pod_id
                }
            }
            
            headers = {
                "Content-Type": "application/json",
                "Authorization": f"Bearer {self.api_key}"
            }
            
            async with aiohttp.ClientSession() as session:
                payload = {
                    "query": mutation,
                    "variables": variables
                }
                
                async with session.post(
                    self.base_url,
                    json=payload,
                    headers=headers,
                    timeout=aiohttp.ClientTimeout(total=15)  # íƒ€ì„ì•„ì›ƒ ì¦ê°€
                ) as response:
                    response_text = await response.text()
                    logger.info(f"RunPod API ì‘ë‹µ: {response.status} - {response_text}")
                    
                    if response.status != 200:
                        logger.error(f"RunPod ì¢…ë£Œ API í˜¸ì¶œ ì‹¤íŒ¨: {response.status} - {response_text}")
                        return False
                    
                    try:
                        data = await response.json()
                        result = data.get("data", {}).get("podTerminate", False)
                        
                        if result:
                            logger.info(f"âœ… RunPod {pod_id} ì¢…ë£Œ ìš”ì²­ ì„±ê³µ")
                            
                            # ì¢…ë£Œ í™•ì¸ì„ ìœ„í•´ ì ì‹œ ëŒ€ê¸° í›„ ìƒíƒœ í™•ì¸
                            await asyncio.sleep(3)
                            final_status = await self._verify_termination(pod_id)
                            
                            if final_status:
                                logger.info(f"âœ… RunPod {pod_id} ì™„ì „ ì¢…ë£Œ í™•ì¸ë¨")
                                return True
                            else:
                                logger.warning(f"âš ï¸ RunPod {pod_id} ì¢…ë£Œ ìš”ì²­ì€ ì„±ê³µí–ˆìœ¼ë‚˜ ìƒíƒœ í™•ì¸ ì‹¤íŒ¨")
                                return result  # ì¼ë‹¨ API ì‘ë‹µì„ ë¯¿ê³  True ë°˜í™˜
                        else:
                            logger.error(f"âŒ RunPod {pod_id} ì¢…ë£Œ ìš”ì²­ ì‹¤íŒ¨: {data}")
                            return False
                            
                    except Exception as json_error:
                        logger.error(f"RunPod ì‘ë‹µ JSON íŒŒì‹± ì‹¤íŒ¨: {json_error} - ì›ë³¸: {response_text}")
                        return False
                    
        except asyncio.TimeoutError:
            logger.error(f"RunPod {pod_id} ì¢…ë£Œ API íƒ€ì„ì•„ì›ƒ")
            return False
        except Exception as e:
            logger.error(f"RunPod {pod_id} ì¢…ë£Œ ì¤‘ ì˜ˆì™¸ ë°œìƒ: {e}")
            return False
    
    async def _verify_termination(self, pod_id: str) -> bool:
        """Pod ì¢…ë£Œ í™•ì¸"""
        try:
            status = await self.get_pod_status(pod_id)
            
            # STOPPED, TERMINATED ë“±ì˜ ìƒíƒœë©´ ì„±ê³µ
            terminated_states = ["STOPPED", "TERMINATED", "TERMINATING"]
            is_terminated = status.status in terminated_states
            
            logger.info(f"Pod {pod_id} ì¢…ë£Œ í™•ì¸: ìƒíƒœ={status.status}, ì¢…ë£Œë¨={is_terminated}")
            return is_terminated
            
        except Exception as e:
            logger.warning(f"Pod {pod_id} ì¢…ë£Œ í™•ì¸ ì¤‘ ì˜¤ë¥˜: {e}")
            return False  # í™•ì¸ ì‹¤íŒ¨ëŠ” ì¢…ë£Œ ì‹¤íŒ¨ë¡œ ê°„ì£¼í•˜ì§€ ì•ŠìŒ
    
    async def wait_for_ready(self, pod_id: str, max_wait_time: int = 600) -> bool:
        """Podê°€ ì‹œì‘ë˜ê³  ComfyUIê°€ ì¤€ë¹„ë  ë•Œê¹Œì§€ ëŒ€ê¸° (ë¡œë”© ì¤‘ë‹¨ ê°ì§€ ë° ë³µêµ¬ í¬í•¨)"""
        
        logger.info(f"â³ ===== Pod ì¤€ë¹„ ëŒ€ê¸° ì‹œì‘ =====")
        logger.info(f"   Pod ID: {pod_id}")
        logger.info(f"   ìµœëŒ€ ëŒ€ê¸° ì‹œê°„: {max_wait_time}ì´ˆ ({max_wait_time//60}ë¶„)")
        logger.info(f"   ë³¼ë¥¨ ë§ˆìš´íŠ¸: {settings.RUNPOD_VOLUME_ID} â†’ /workspace")
        logger.info(f"   Template ì´ˆê¸°í™” ì˜ˆìƒ ì‹œê°„: 3-5ë¶„ (ë³¼ë¥¨ ì—°ê²° í¬í•¨)")
        logger.info(f"   í™•ì¸ ì£¼ê¸°: 15ì´ˆ")
        logger.info(f"   ë¡œë”© ì¤‘ë‹¨ ê°ì§€: ì—°ì† 3íšŒ ì‹¤íŒ¨ì‹œ ì¬ì‹œì‘ ì‹œë„")
        
        check_interval = 15  # 15ì´ˆë§ˆë‹¤ í™•ì¸
        checks = max_wait_time // check_interval
        consecutive_failures = 0  # ì—°ì† ì‹¤íŒ¨ íšŸìˆ˜
        last_status = None  # ì´ì „ ìƒíƒœ ì¶”ì 
        stuck_count = 0  # ìƒíƒœ ë³€í™” ì—†ìŒ íšŸìˆ˜
        
        for i in range(checks):
            elapsed_time = i * check_interval
            remaining_time = max_wait_time - elapsed_time
            
            try:
                # ì§„í–‰ë¥  ë¡œê·¸ ë¹ˆë„ ê°ì†Œ (5íšŒë§ˆë‹¤ë§Œ ì¶œë ¥)
                if (i + 1) % 5 == 0 or i == 0:
                    logger.info(f"ğŸ” ìƒíƒœ í™•ì¸ #{i+1}/{checks} (ê²½ê³¼: {elapsed_time}ì´ˆ, ë‚¨ì€ ì‹œê°„: {remaining_time}ì´ˆ)")
                else:
                    logger.debug(f"ğŸ” ìƒíƒœ í™•ì¸ #{i+1}/{checks} (ê²½ê³¼: {elapsed_time}ì´ˆ)")
                
                status = await self.get_pod_status(pod_id)
                current_status = status.status
                logger.info(f"   Pod ìƒíƒœ: {current_status}")
                
                # ìƒíƒœ ë³€í™” ì¶”ì 
                if last_status == current_status:
                    stuck_count += 1
                else:
                    stuck_count = 0
                    consecutive_failures = 0  # ìƒíƒœ ë³€í™”ì‹œ ì‹¤íŒ¨ ì¹´ìš´íŠ¸ ë¦¬ì…‹
                
                last_status = current_status
                
                if current_status == "RUNNING" and status.endpoint_url:
                    logger.info(f"   âœ… Pod ì‹¤í–‰ ì¤‘! Endpoint: {status.endpoint_url}")
                    
                    # ë³¼ë¥¨ ë§ˆìš´íŠ¸ ìƒíƒœ ì²´í¬ (ì²˜ìŒ ëª‡ ë²ˆë§Œ)
                    if i < 3:
                        await self._check_volume_mount(status.endpoint_url)
                    
                    logger.info(f"   ğŸ” ComfyUI API ì—°ê²° í…ŒìŠ¤íŠ¸ ì¤‘...")
                    
                    # ComfyUI API ì‘ë‹µ í™•ì¸
                    if await self._check_comfyui_ready(status.endpoint_url):
                        logger.info(f"   âœ… ComfyUI API ì‘ë‹µ ì„±ê³µ!")
                        logger.info(f"âœ… ===== Pod {pod_id} ì™„ì „ ì¤€ë¹„ ì™„ë£Œ! ======")
                        logger.info(f"   ì´ ëŒ€ê¸° ì‹œê°„: {elapsed_time}ì´ˆ")
                        logger.info(f"   Endpoint URL: {status.endpoint_url}")
                        
                        # ìµœì¢… ë³¼ë¥¨ ë§ˆìš´íŠ¸ í™•ì¸
                        await self._check_volume_mount(status.endpoint_url)
                        
                        return True
                    else:
                        consecutive_failures += 1
                        logger.info(f"   âš ï¸ PodëŠ” ì‹¤í–‰ ì¤‘ì´ì§€ë§Œ ComfyUIê°€ ì•„ì§ ì¤€ë¹„ë˜ì§€ ì•ŠìŒ (ì‹¤íŒ¨ {consecutive_failures}/3)")
                        logger.info(f"   âš ï¸ Template ì´ˆê¸°í™” ì§€ì† ì¤‘... (ë³¼ë¥¨ ë§ˆìš´íŠ¸ í¬í•¨)")
                        
                        # ì—°ì† 3íšŒ ComfyUI API ì‹¤íŒ¨ì‹œ Pod ì¬ì‹œì‘ ì‹œë„
                        if consecutive_failures >= 3 and i > 10:  # ìµœì†Œ 150ì´ˆ í›„ì—ë§Œ ì¬ì‹œì‘ ê³ ë ¤
                            logger.warning(f"   ğŸ”„ ComfyUI ì‘ë‹µ ì—°ì† ì‹¤íŒ¨ ê°ì§€ - Pod ì¬ì‹œì‘ ì‹œë„...")
                            restart_success = await self._restart_stuck_pod(pod_id)
                            
                            if restart_success:
                                logger.info(f"   âœ… Pod ì¬ì‹œì‘ ì„±ê³µ - ëŒ€ê¸° ì‹œê°„ ì—°ì¥")
                                consecutive_failures = 0
                                stuck_count = 0
                                # ì¬ì‹œì‘ í›„ ì¶”ê°€ ì‹œê°„ í™•ë³´ (ìµœëŒ€ 5ë¶„)
                                additional_time = min(300, max_wait_time - elapsed_time)
                                checks += additional_time // check_interval
                            else:
                                logger.error(f"   âŒ Pod ì¬ì‹œì‘ ì‹¤íŒ¨")
                
                elif current_status in ["FAILED", "TERMINATED", "STOPPED", "EXITED"]:
                    logger.error(f"   âŒ Pod ì¢…ë£Œ ìƒíƒœ ê°ì§€: {current_status}")
                    logger.error(f"   âŒ Pod {pod_id} ì‚¬ìš© ë¶ˆê°€ - ìƒˆë¡œìš´ Pod ìƒì„± í•„ìš”")
                    return False
                
                elif current_status in ["STARTING", "CREATED"]:
                    logger.info(f"   ğŸ”„ Pod ì‹œì‘ ì¤‘... (ìƒíƒœ: {current_status})")
                    if current_status == "STARTING":
                        logger.info(f"   ğŸ”„ Template ë‹¤ìš´ë¡œë“œ ë° ComfyUI ì„¤ì¹˜ ì§„í–‰ ì¤‘...")
                    
                    # STARTING ìƒíƒœì—ì„œ ë„ˆë¬´ ì˜¤ë˜ ë©ˆì¶°ìˆìœ¼ë©´ ì¬ì‹œì‘ ê³ ë ¤
                    if stuck_count >= 8 and current_status == "STARTING":  # 2ë¶„ ì´ìƒ ê°™ì€ ìƒíƒœ
                        logger.warning(f"   âš ï¸ STARTING ìƒíƒœì—ì„œ {stuck_count * check_interval}ì´ˆê°„ ë©ˆì¶¤ - ì¬ì‹œì‘ ê³ ë ¤")
                        restart_success = await self._restart_stuck_pod(pod_id)
                        if restart_success:
                            stuck_count = 0
                            consecutive_failures = 0
                
                else:
                    logger.debug(f"   ğŸ”„ ê¸°íƒ€ ìƒíƒœ: {current_status}, ê³„ì† ëŒ€ê¸°...")
                
            except Exception as e:
                consecutive_failures += 1
                logger.warning(f"   âš ï¸ ìƒíƒœ í™•ì¸ ì¤‘ ì˜¤ë¥˜ (ì‹¤íŒ¨ {consecutive_failures}/3): {str(e)[:100]}...")
                logger.warning(f"   âš ï¸ 15ì´ˆ í›„ ì¬ì‹œë„...")
            
            if i < checks - 1:  # ë§ˆì§€ë§‰ì´ ì•„ë‹ˆë©´ ëŒ€ê¸°
                await asyncio.sleep(check_interval)
        
        logger.error(f"âŒ ===== Pod ì¤€ë¹„ ëŒ€ê¸° ì‹œê°„ ì´ˆê³¼ ======")
        logger.error(f"   Pod ID: {pod_id}")
        logger.error(f"   ìµœëŒ€ ëŒ€ê¸° ì‹œê°„: {max_wait_time}ì´ˆ ({max_wait_time//60}ë¶„)")
        logger.error(f"   ë³¼ë¥¨ ë§ˆìš´íŠ¸: {settings.RUNPOD_VOLUME_ID}")
        logger.error(f"   ì´ ì‹œë„ ìˆ˜: {checks}íšŒ")
        logger.error(f"   ì—°ì† ì‹¤íŒ¨ íšŸìˆ˜: {consecutive_failures}")
        logger.error(f"   ìƒíƒœ ë³€í™” ì—†ìŒ: {stuck_count}íšŒ")
        logger.error(f"   ê°€ëŠ¥í•œ ì›ì¸:")
        logger.error(f"     1. Template ì´ˆê¸°í™” ì‹œê°„ ì´ˆê³¼ (3-5ë¶„ ì˜ˆìƒ)")
        logger.error(f"     2. ë³¼ë¥¨ ë§ˆìš´íŠ¸ ì‹¤íŒ¨ ({settings.RUNPOD_VOLUME_ID})")
        logger.error(f"     3. ë„¤íŠ¸ì›Œí¬ ì—°ê²° ë¬¸ì œ")
        logger.error(f"     4. ComfyUI ì‹œì‘ ì‹¤íŒ¨")
        logger.error(f"   ê¶Œì¥ ì‚¬í•­: Podë¥¼ ìˆ˜ë™ìœ¼ë¡œ ì¢…ë£Œí•˜ê³  ìƒˆë¡œ ìƒì„±")
        return False
    
    async def _check_comfyui_ready(self, endpoint_url: str, max_retries: int = 3, retry_delay: int = 3) -> bool:
        """ComfyUI API ì¤€ë¹„ ìƒíƒœ í™•ì¸ (ê°•í™”ëœ ë‹¤ì¤‘ ì—”ë“œí¬ì¸íŠ¸ ì²´í¬ + ì¬ì‹œë„)"""
        
        # ComfyUI API ì—”ë“œí¬ì¸íŠ¸ë“¤ (ìš°ì„ ìˆœìœ„ ìˆœ)
        test_endpoints = [
            "/",           # ë©”ì¸ í˜ì´ì§€ (ê°€ì¥ ê¸°ë³¸ì )          # ê¸°ì¡´ ì—”ë“œí¬ì¸íŠ¸
            "/queue",             # í ìƒíƒœ
        ]
        
        # ì¬ì‹œë„ ë¡œì§ (ComfyUI ë¡œë”©ì´ ëŠë¦´ ìˆ˜ ìˆìŒ)
        for retry in range(max_retries):
            if retry > 0:
                logger.info(f"     â³ ComfyUI API ì¬ì‹œë„ {retry + 1}/{max_retries} ({retry_delay}ì´ˆ ëŒ€ê¸°)")
                await asyncio.sleep(retry_delay)
            
            for endpoint in test_endpoints:
                try:
                    test_url = f"{endpoint_url}{endpoint}"
                    logger.info(f"     ğŸŒ ComfyUI API í…ŒìŠ¤íŠ¸: {test_url}")
                    
                    async with aiohttp.ClientSession() as session:
                        async with session.get(
                            test_url,
                            timeout=aiohttp.ClientTimeout(total=10),  # íƒ€ì„ì•„ì›ƒ 10ì´ˆë¡œ ì—°ì¥
                            headers={"User-Agent": "AIMEX-Backend/1.0"},
                            ssl=False  # RunPod proxy SSL ë¬¸ì œ íšŒí”¼
                        ) as response:
                            logger.info(f"     ğŸ” API ì‘ë‹µ ({endpoint}): {response.status}")
                            
                            # 200, 201, 302 ë“± ì •ìƒ ì‘ë‹µìœ¼ë¡œ ê°„ì£¼
                            if 200 <= response.status < 400:
                                response_text = await response.text()
                                logger.info(f"     âœ… ComfyUI API ì¤€ë¹„ ì™„ë£Œ! ({endpoint}, {len(response_text)} bytes)")
                                
                                # ì‘ë‹µ ë‚´ìš©ë„ ê°„ë‹¨íˆ í™•ì¸
                                if endpoint == "/" and ("ComfyUI" in response_text or "comfyui" in response_text.lower()):
                                    logger.info(f"     ğŸ¯ ComfyUI ë©”ì¸ í˜ì´ì§€ í™•ì¸ë¨ (ì¬ì‹œë„ {retry + 1}íšŒì°¨)")
                                    return True
                                elif endpoint != "/" and response_text:
                                    logger.info(f"     ğŸ¯ API ì—”ë“œí¬ì¸íŠ¸ ì •ìƒ ì‘ë‹µ (ì¬ì‹œë„ {retry + 1}íšŒì°¨)")
                                    return True
                                elif response.status == 200:
                                    logger.info(f"     ğŸ¯ HTTP 200 ì‘ë‹µ í™•ì¸ (ì¬ì‹œë„ {retry + 1}íšŒì°¨)")
                                    return True
                            else:
                                logger.info(f"     âš ï¸ ë¹„ì •ìƒ ì‘ë‹µ: {response.status}")
                                
                except asyncio.TimeoutError:
                    logger.info(f"     âš ï¸ API íƒ€ì„ì•„ì›ƒ ({endpoint}, 10ì´ˆ)")
                    continue
                except Exception as e:
                    logger.info(f"     âš ï¸ API ì—°ê²° ì‹¤íŒ¨ ({endpoint}): {str(e)[:50]}...")
                    continue
            
            # ì´ë²ˆ ë¼ìš´ë“œì—ì„œ ì„±ê³µí•˜ì§€ ëª»í•¨
            if retry < max_retries - 1:
                logger.info(f"     â­ï¸ ì¬ì‹œë„ {retry + 1} ì‹¤íŒ¨, {retry_delay}ì´ˆ í›„ ë‹¤ì‹œ ì‹œë„...")
        
        # ëª¨ë“  ì¬ì‹œë„ ì‹¤íŒ¨
        logger.warning(f"     âŒ ComfyUI API ì¤€ë¹„ í™•ì¸ ì‹¤íŒ¨ (ì´ {max_retries}íšŒ ì¬ì‹œë„)")
        logger.info(f"     ğŸ” ì‹œë„í•œ ì—”ë“œí¬ì¸íŠ¸: {test_endpoints}")
        logger.info(f"     ğŸ’¡ ComfyUI ì„œë²„ê°€ ì‹¤í–‰ ì¤‘ì´ì§€ë§Œ APIê°€ ì•„ì§ ì¤€ë¹„ë˜ì§€ ì•Šì•˜ì„ ìˆ˜ ìˆìŒ")
        logger.info(f"     ğŸ’¡ PodëŠ” ì •ìƒì´ë¯€ë¡œ ë‚˜ì¤‘ì— ë‹¤ì‹œ í™•ì¸í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤")
        return False
    
    async def _check_volume_mount(self, endpoint_url: str) -> bool:
        """ë³¼ë¥¨ ë§ˆìš´íŠ¸ ìƒíƒœ í™•ì¸ (RunPod proxy URL ì‚¬ìš©)"""
        try:
            # RunPod proxyë¥¼ í†µí•œ JupyterLab ì ‘ê·¼ (í¬íŠ¸ 8888)
            pod_id = endpoint_url.split('://')[1].split('-')[0]  # URLì—ì„œ pod_id ì¶”ì¶œ
            jupyter_url = self._generate_proxy_url(pod_id, 8888)
            
            logger.info(f"     ğŸ’¾ ë³¼ë¥¨ ë§ˆìš´íŠ¸ ìƒíƒœ í™•ì¸: /workspace")
            logger.info(f"     ğŸ” JupyterLab URL: {jupyter_url}")
            
            # RunPod proxyë¥¼ í†µí•œ JupyterLab ì ‘ê·¼ ì‹œë„
            async with aiohttp.ClientSession() as session:
                async with session.get(
                    f"{jupyter_url}/tree",
                    timeout=aiohttp.ClientTimeout(total=5),
                    ssl=False  # RunPod proxy SSL ë¬¸ì œ íšŒí”¼
                ) as response:
                    if response.status == 200:
                        logger.info(f"     âœ… JupyterLab ì ‘ê·¼ ê°€ëŠ¥ - ë³¼ë¥¨ ë§ˆìš´íŠ¸ í™•ì¸ë¨")
                        return True
                    else:
                        logger.info(f"     âš ï¸ JupyterLab ì ‘ê·¼ ì‹¤íŒ¨: {response.status}")
                        return False
                        
        except asyncio.TimeoutError:
            logger.info(f"     âš ï¸ JupyterLab ëŒ€ê¸° ì‹œê°„ ì´ˆê³¼")
            return False
        except Exception as e:
            logger.info(f"     âš ï¸ ë³¼ë¥¨ ë§ˆìš´íŠ¸ ì²´í¬ ì‹¤íŒ¨: {str(e)[:50]}...")
            return False
    
    
    async def check_pod_health(self, pod_id: str) -> dict:
        """Pod ê±´ê°•ì„± ë° ë¦¬ì†ŒìŠ¤ ìƒíƒœ í™•ì¸"""
        if not pod_id:
            return {"error": "Pod ID not provided", "healthy": False}
        
        try:
            # Pod ìƒíƒœ ì¡°íšŒ GraphQL ì¿¼ë¦¬
            query = """
            query getPod($input: PodIdInput!) {
                pod(input: $input) {
                    id
                    name
                    desiredStatus
                    lastStatusChange
                    runtime {
                        uptimeInSeconds
                        ports {
                            ip
                            isIpPublic
                            privatePort
                            publicPort
                            type
                        }
                        gpus {
                            id
                            gpuUtilPercent
                            memoryUtilPercent
                        }
                        container {
                            cpuPercent
                            memoryPercent
                        }
                    }
                    machine {
                        gpuCount
                        vcpuCount
                        memoryInGb
                        diskInGb
                    }
                }
            }
            """
            
            headers = {
                "Content-Type": "application/json",
                "Authorization": f"Bearer {self.api_key}"
            }
            
            variables = {"input": {"podId": pod_id}}
            payload = {"query": query, "variables": variables}
            
            async with aiohttp.ClientSession() as session:
                async with session.post(
                    self.base_url,
                    json=payload,
                    headers=headers,
                    timeout=aiohttp.ClientTimeout(total=10)
                ) as response:
                    if response.status != 200:
                        logger.error(f"Pod health check failed: HTTP {response.status}")
                        return {"error": f"HTTP {response.status}", "healthy": False}
                    
                    data = await response.json()
                    
                    if "errors" in data:
                        logger.error(f"Pod health check GraphQL errors: {data['errors']}")
                        return {"error": "GraphQL errors", "healthy": False}
                    
                    pod_data = data.get("data", {}).get("pod")
                    if not pod_data:
                        logger.error(f"Pod {pod_id} not found")
                        return {"error": "Pod not found", "healthy": False}
                    
                    # ê±´ê°•ì„± íŒë‹¨ ë¡œì§
                    desired_status = pod_data.get("desiredStatus", "Unknown")
                    runtime = pod_data.get("runtime", {})
                    machine = pod_data.get("machine", {})
                    
                    # ë¦¬ì†ŒìŠ¤ í™•ì¸
                    vcpu_count = machine.get("vcpuCount", 0)
                    memory_gb = machine.get("memoryInGb", 0)
                    
                    # ê±´ê°•ì„± íŒë‹¨
                    is_healthy = (
                        desired_status == "RUNNING" and
                        runtime is not None and
                        vcpu_count >= 4 and  # ìµœì†Œ 4 vCPU
                        memory_gb >= 16      # ìµœì†Œ 16GB RAM
                    )
                    
                    health_info = {
                        "pod_id": pod_id,
                        "status": desired_status,
                        "healthy": is_healthy,
                        "uptime": runtime.get("uptimeInSeconds", 0) if runtime else 0,
                        "vcpu_count": vcpu_count,
                        "memory_gb": memory_gb,
                        "disk_gb": machine.get("diskInGb", 0),
                        "gpu_count": machine.get("gpuCount", 0),
                        "last_status_change": pod_data.get("lastStatusChange"),
                        "ports": runtime.get("ports", []) if runtime else []
                    }
                    
                    # ë¦¬ì†ŒìŠ¤ ë¶€ì¡± ê°ì§€
                    if vcpu_count < 4 or memory_gb < 16:
                        health_info["resource_issue"] = f"Insufficient resources: {vcpu_count}C/{memory_gb}GB"
                        health_info["needs_restart"] = True
                        logger.warning(f"Pod {pod_id} has insufficient resources: {vcpu_count}C/{memory_gb}GB")
                    
                    return health_info
                    
        except Exception as e:
            logger.error(f"Failed to check Pod {pod_id} health: {e}")
            return {"error": str(e), "healthy": False}
    
    async def force_restart_pod(self, pod_id: str, request_id: str) -> dict:
        """ë¦¬ì†ŒìŠ¤ ë¶€ì¡± ë˜ëŠ” ì˜¤ë¥˜ Pod ê°•ì œ ì¬ì‹œì‘"""
        try:
            logger.info(f"ğŸ”„ Pod {pod_id} ê°•ì œ ì¬ì‹œì‘ ì‹œì‘...")
            
            # 1. í˜„ì¬ Pod ê±´ê°•ì„± í™•ì¸
            health_check = await self.check_pod_health(pod_id)
            logger.info(f"   ğŸ“Š í˜„ì¬ Pod ìƒíƒœ: {health_check}")
            
            # 2. Pod ê°•ì œ ì¢…ë£Œ
            logger.info(f"   ğŸ—‘ï¸ Pod {pod_id} ê°•ì œ ì¢…ë£Œ ì¤‘...")
            terminate_success = await self.terminate_pod(pod_id)
            
            if not terminate_success:
                logger.warning(f"   âš ï¸ Pod ì¢…ë£Œ ì‹¤íŒ¨í–ˆì§€ë§Œ ì¬ìƒì„± ì§„í–‰...")
            else:
                logger.info(f"   âœ… Pod ì¢…ë£Œ ì™„ë£Œ")
                
                # ì¢…ë£Œ ì™„ë£Œê¹Œì§€ ì ì‹œ ëŒ€ê¸°
                await asyncio.sleep(5)
            
            # 3. ìƒˆ Pod ìƒì„±
            logger.info(f"   ğŸš€ ìƒˆë¡œìš´ Pod ìƒì„± ì¤‘...")
            new_pod_response = await self.create_pod(request_id)
            
            if new_pod_response and new_pod_response.pod_id:
                logger.info(f"   âœ… ìƒˆ Pod ìƒì„± ì„±ê³µ: {new_pod_response.pod_id}")
                return {
                    "success": True,
                    "old_pod_id": pod_id,
                    "new_pod_id": new_pod_response.pod_id,
                    "status": new_pod_response.status,
                    "endpoint_url": new_pod_response.endpoint_url
                }
            else:
                logger.error(f"   âŒ ìƒˆ Pod ìƒì„± ì‹¤íŒ¨")
                return {
                    "success": False,
                    "error": "Failed to create new pod",
                    "old_pod_id": pod_id
                }
                
        except Exception as e:
            logger.error(f"Pod ê°•ì œ ì¬ì‹œì‘ ì‹¤íŒ¨: {e}")
            return {
                "success": False,
                "error": str(e),
                "old_pod_id": pod_id
            }

    async def check_volume_status(self, volume_id: str = None) -> dict:
        """ë³¼ë¥¨ ìƒíƒœ ë° ë‚´ìš© í™•ì¸ (ê°„ë‹¨í•œ ë²„ì „)"""
        if not volume_id:
            volume_id = settings.RUNPOD_VOLUME_ID
        
        # ì˜¬ë°”ë¥¸ ë³¼ë¥¨ ì¡°íšŒ ì¿¼ë¦¬ (introspectionìœ¼ë¡œ í™•ì¸ëœ ì •í™•í•œ í•„ë“œëª…)
        query = """
        query {
            myself {
                networkVolumes {
                    id
                    name
                    size
                    dataCenterId
                }
            }
        }
        """
        
        try:
            headers = {
                "Content-Type": "application/json",
                "Authorization": f"Bearer {self.api_key}"
            }
            
            async with aiohttp.ClientSession() as session:
                payload = {"query": query}
                
                logger.info(f"Checking volume {volume_id} status...")
                
                async with session.post(
                    self.base_url,
                    json=payload,
                    headers=headers,
                    timeout=aiohttp.ClientTimeout(total=10)
                ) as response:
                    if response.status != 200:
                        response_text = await response.text()
                        logger.error(f"Volume query failed: {response.status} - {response_text[:200]}")
                        return {"error": f"API call failed: {response.status}"}
                    
                    data = await response.json()
                    
                    if "errors" in data:
                        logger.error(f"Volume query GraphQL errors: {data['errors']}")
                        return {"error": f"GraphQL errors: {data['errors']}"}
                    
                    volumes = data.get("data", {}).get("myself", {}).get("networkVolumes", [])
                    
                    # íŠ¹ì • ë³¼ë¥¨ ID ì°¾ê¸°
                    target_volume = None
                    for volume in volumes:
                        if volume.get("id") == volume_id:
                            target_volume = volume
                            break
                    
                    if target_volume:
                        logger.info(f"Found volume {volume_id}:")
                        logger.info(f"  Name: {target_volume.get('name', 'N/A')}")
                        logger.info(f"  Size: {target_volume.get('size', 'N/A')}GB")
                        logger.info(f"  Data Center: {target_volume.get('dataCenterId', 'N/A')}")
                        
                        return {
                            "volume_id": target_volume.get("id"),
                            "name": target_volume.get("name"),
                            "size": target_volume.get("size"),
                            "data_center": target_volume.get("dataCenterId"),
                            "status": "found"
                        }
                    else:
                        logger.warning(f"Volume {volume_id} not found in {len(volumes)} volumes")
                        return {
                            "error": "Volume not found",
                            "available_volumes": [v.get("id") for v in volumes[:5]]  # ì²˜ìŒ 5ê°œë§Œ
                        }
                        
        except Exception as e:
            logger.error(f"Volume status check error: {e}")
            return {"error": str(e)}
    
    async def _restart_stuck_pod(self, pod_id: str) -> bool:
        """ë¡œë”©ì´ ë©ˆì¶˜ Pod ì¬ì‹œì‘ ì‹œë„"""
        try:
            logger.info(f"ğŸ”„ Pod {pod_id} ì¬ì‹œì‘ ì‹œë„ ì¤‘...")
            
            # RunPod APIì˜ podResume ì‚¬ìš©
            mutation = """
            mutation podResume($input: PodResumeInput!) {
                podResume(input: $input) {
                    id
                    desiredStatus
                    imageName
                }
            }
            """
            
            variables = {
                "input": {
                    "podId": pod_id,
                    "gpuCount": 1
                }
            }
            
            headers = {
                "Content-Type": "application/json",
                "Authorization": f"Bearer {self.api_key}"
            }
            
            async with aiohttp.ClientSession() as session:
                payload = {
                    "query": mutation,
                    "variables": variables
                }
                
                async with session.post(
                    self.base_url,
                    json=payload,
                    headers=headers,
                    timeout=aiohttp.ClientTimeout(total=15)
                ) as response:
                    if response.status != 200:
                        response_text = await response.text()
                        logger.error(f"Pod restart API failed: {response.status} - {response_text}")
                        return False
                    
                    data = await response.json()
                    
                    if "errors" in data:
                        logger.error(f"Pod restart GraphQL errors: {data['errors']}")
                        return False
                    
                    result = data.get("data", {}).get("podResume")
                    
                    if result:
                        logger.info(f"âœ… Pod {pod_id} ì¬ì‹œì‘ ì„±ê³µ")
                        logger.info(f"   ìƒˆ ìƒíƒœ: {result.get('desiredStatus', 'Unknown')}")
                        
                        # ì¬ì‹œì‘ í›„ ì ì‹œ ëŒ€ê¸°
                        await asyncio.sleep(10)
                        return True
                    else:
                        logger.error(f"âŒ Pod restart returned empty result")
                        return False
                        
        except Exception as e:
            logger.error(f"Pod restart failed: {e}")
            return False
    
    async def _check_gpu_availability(self) -> dict:
        """GPU ê°€ìš©ì„± í™•ì¸"""
        
        # RunPod GPU íƒ€ì… ì¡°íšŒ ì¿¼ë¦¬ (stockStatus í•„ë“œëŠ” ì¡´ì¬í•˜ì§€ ì•Šìœ¼ë¯€ë¡œ ì œê±°)
        query = """
        query {
            gpuTypes {
                id
                displayName
                memoryInGb
                secureCloud
                communityCloud
                lowestPrice(input: {gpuCount: 1}) {
                    minimumBidPrice
                    uninterruptablePrice
                }
            }
        }
        """
        
        try:
            headers = {
                "Content-Type": "application/json",
                "Authorization": f"Bearer {self.api_key}"
            }
            
            async with aiohttp.ClientSession() as session:
                payload = {"query": query}
                
                async with session.post(
                    self.base_url,
                    json=payload,
                    headers=headers,
                    timeout=aiohttp.ClientTimeout(total=10)
                ) as response:
                    if response.status != 200:
                        response_text = await response.text()
                        logger.error(f"GPU ê°€ìš©ì„± ì¡°íšŒ ì‹¤íŒ¨: {response.status}")
                        logger.error(f"ì‘ë‹µ ë‚´ìš©: {response_text}")
                        return {}
                    
                    data = await response.json()
                    gpu_types = data.get("data", {}).get("gpuTypes", [])
                    
                    availability = {}
                    for gpu in gpu_types:
                        display_name = gpu.get("displayName", "")
                        lowest_price = gpu.get("lowestPrice", {})
                        
                        # ê°€ê²© ì •ë³´ê°€ ìˆìœ¼ë©´ ê°€ìš©í•˜ë‹¤ê³  íŒë‹¨
                        min_bid_price = lowest_price.get("minimumBidPrice")
                        uninterruptable_price = lowest_price.get("uninterruptablePrice", 0)
                        
                        has_price = (
                            (min_bid_price is not None and min_bid_price > 0) or 
                            (uninterruptable_price is not None and uninterruptable_price > 0)
                        )
                        
                        # GPU íƒ€ì…ë³„ í™•ì¸ (ìš°ì„ ìˆœìœ„ ìˆœ)
                        if "RTX 4090" in display_name or "4090" in display_name:
                            availability["RTX_4090"] = has_price
                        elif "RTX A6000" in display_name or "A6000" in display_name:
                            availability["RTX_A6000"] = has_price
                        elif "RTX A5000" in display_name or "A5000" in display_name:
                            availability["RTX_A5000"] = has_price
                        elif "A40" in display_name:
                            availability["RTX_A40"] = has_price
                    
                    logger.info(f"GPU ê°€ìš©ì„±: {availability}")
                    return availability
                    
        except Exception as e:
            logger.error(f"GPU ê°€ìš©ì„± ì¡°íšŒ ì¤‘ ì˜¤ë¥˜: {e}")
            return {}

    async def get_remaining_credits(self) -> Optional[Dict[str, Any]]:
        """RunPod ë‚¨ì€ í¬ë ˆë”§ ì¡°íšŒ"""
        try:
            if not self.api_key:
                logger.warning("RunPod API í‚¤ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤")
                return None

            # GraphQL ì¿¼ë¦¬ (ë‹¨ìˆœí™”)
            query = """
            query myself {
                myself {
                    clientBalance
                }
            }
            """

            headers = {
                "Authorization": f"Bearer {self.api_key}",
                "Content-Type": "application/json",
            }

            payload = {
                "query": query
            }

            async with aiohttp.ClientSession() as session:
                async with session.post(
                    self.base_url,
                    headers=headers,
                    json=payload,
                    timeout=30
                ) as response:
                    if response.status == 200:
                        data = await response.json()
                        
                        if "data" in data and "myself" in data["data"]:
                            client_balance = data["data"]["myself"].get("clientBalance", 0)
                            
                            result = {
                                "remaining_credits": client_balance,
                                "last_updated": datetime.utcnow().isoformat()
                            }
                            
                            logger.info(f"RunPod í¬ë ˆë”§ ì¡°íšŒ ì„±ê³µ: {client_balance} í¬ë ˆë”§ ë‚¨ìŒ")
                            return result
                        else:
                            logger.error(f"RunPod API ì‘ë‹µ í˜•ì‹ ì˜¤ë¥˜: {data}")
                            return None
                    else:
                        logger.error(f"RunPod API ìš”ì²­ ì‹¤íŒ¨: {response.status}")
                        return None

        except aiohttp.ClientError as e:
            logger.error(f"RunPod API ë„¤íŠ¸ì›Œí¬ ì˜¤ë¥˜: {e}")
            return None
        except Exception as e:
            logger.error(f"RunPod í¬ë ˆë”§ ì¡°íšŒ ì‹¤íŒ¨: {e}")
            return None


# ì‹±ê¸€í†¤ íŒ¨í„´
_runpod_service_instance = None

def get_runpod_service() -> RunPodService:
    """RunPod ì„œë¹„ìŠ¤ ì¸ìŠ¤í„´ìŠ¤ ë°˜í™˜"""
    global _runpod_service_instance
    if _runpod_service_instance is None:
        _runpod_service_instance = RunPodService()
    return _runpod_service_instance