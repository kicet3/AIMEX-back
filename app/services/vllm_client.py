"""
VLLM ì„œë²„ í´ë¼ì´ì–¸íŠ¸ ì„œë¹„ìŠ¤
FastAPI ë°±ì—”ë“œì—ì„œ VLLM ì„œë²„ë¡œ ìš”ì²­ì„ ë¼ìš°íŒ…í•˜ëŠ” í´ë¼ì´ì–¸íŠ¸
"""

import asyncio
import json
import logging
import os
import httpx
import websockets
from typing import Optional, Dict, List, Any, AsyncIterator
from datetime import datetime
from dataclasses import dataclass
from enum import Enum

from app.core.config import settings

logger = logging.getLogger(__name__)


class VLLMClientError(Exception):
    """VLLM í´ë¼ì´ì–¸íŠ¸ ì˜¤ë¥˜"""
    pass


@dataclass
class VLLMServerConfig:
    """VLLM ì„œë²„ ì„¤ì •"""
    base_url: str
    timeout: int = 300
    
    @property
    def ws_url(self) -> str:
        # HTTP -> WS ë³€í™˜ (http:// -> ws://, https:// -> wss://)
        return self.base_url.replace("http://", "ws://").replace("https://", "wss://")


class VLLMClient:
    """VLLM ì„œë²„ í´ë¼ì´ì–¸íŠ¸"""
    
    def __init__(self, config: Optional[VLLMServerConfig] = None):
        self.config = config or VLLMServerConfig()
        self.client = httpx.AsyncClient(
            base_url=self.config.base_url,
            timeout=httpx.Timeout(self.config.timeout)
        )
        
    async def __aenter__(self):
        return self
    
    async def __aexit__(self, exc_type, exc_val, exc_tb):
        await self.client.aclose()
    
    async def health_check(self) -> bool:
        """VLLM ì„œë²„ ìƒíƒœ í™•ì¸"""
        try:
            logger.info(f"ğŸ” VLLM ì„œë²„ health check ì‹œì‘: {self.config.base_url}")
            response = await self.client.get("/")
            logger.info(f"âœ… VLLM ì„œë²„ ì‘ë‹µ ì„±ê³µ: ìƒíƒœ ì½”ë“œ {response.status_code}")
            if response.status_code != 200:
                logger.warning(f"âš ï¸ VLLM ì„œë²„ê°€ 200ì´ ì•„ë‹Œ ìƒíƒœ ì½”ë“œ ë°˜í™˜: {response.status_code}")
                logger.warning(f"   - ì‘ë‹µ ë‚´ìš©: {response.text[:500]}")
            return response.status_code == 200
        except httpx.ConnectError as e:
            logger.error(f"âŒ VLLM ì„œë²„ ì—°ê²° ì‹¤íŒ¨ (ConnectError): {self.config.base_url}")
            logger.error(f"   - ìƒì„¸ ì˜¤ë¥˜: {str(e)}")
            logger.error(f"   - í™˜ê²½ ë³€ìˆ˜ í™•ì¸: VLLM_ENABLED={os.getenv('VLLM_ENABLED')}, VLLM_SERVER_URL={os.getenv('VLLM_SERVER_URL')}")
            return False
        except httpx.TimeoutException as e:
            logger.error(f"âŒ VLLM ì„œë²„ ì—°ê²° ì‹œê°„ ì´ˆê³¼ (TimeoutException): {self.config.base_url}")
            logger.error(f"   - ì‹œê°„ ì´ˆê³¼ ì„¤ì •: {self.config.timeout}ì´ˆ")
            return False
        except Exception as e:
            logger.error(f"âŒ VLLM ì„œë²„ ìƒíƒœ í™•ì¸ ì‹¤íŒ¨ (ê¸°íƒ€ ì˜¤ë¥˜): {type(e).__name__}")
            logger.error(f"   - ìƒì„¸ ì˜¤ë¥˜: {str(e)}")
            import traceback
            logger.error(f"   - ìŠ¤íƒ íŠ¸ë ˆì´ìŠ¤:\n{traceback.format_exc()}")
            return False
    
    async def get_stats(self) -> Dict[str, Any]:
        """VLLM ì„œë²„ í†µê³„ ì¡°íšŒ"""
        try:
            response = await self.client.get("/stats")
            response.raise_for_status()
            return response.json()
        except Exception as e:
            logger.error(f"VLLM ì„œë²„ í†µê³„ ì¡°íšŒ ì‹¤íŒ¨: {e}")
            raise VLLMClientError(f"ì„œë²„ í†µê³„ ì¡°íšŒ ì‹¤íŒ¨: {e}")
    
    async def load_adapter(self, model_id: str, hf_repo_name: str, 
                          hf_token: Optional[str] = None, 
                          base_model_override: Optional[str] = None) -> Dict[str, Any]:
        """LoRA ì–´ëŒ‘í„° ë¡œë“œ"""
        try:
            payload = {
                "model_id": model_id,
                "hf_repo_name": hf_repo_name
            }
            if hf_token:
                payload["hf_token"] = hf_token
            if base_model_override:
                payload["base_model_override"] = base_model_override
            
            response = await self.client.post("/lora/load_adapter", json=payload)
            response.raise_for_status()
            
            result = response.json()
            logger.info(f"âœ… ì–´ëŒ‘í„° ë¡œë“œ ì„±ê³µ: {model_id}")
            return result
            
        except Exception as e:
            logger.error(f"âŒ ì–´ëŒ‘í„° ë¡œë“œ ì‹¤íŒ¨: {model_id}, {e}")
            raise VLLMClientError(f"ì–´ëŒ‘í„° ë¡œë“œ ì‹¤íŒ¨: {e}")
    
    async def generate_response(self, user_message: str, system_message: str = None,
                              influencer_name: str = "ì–´ì‹œìŠ¤í„´íŠ¸", model_id: str = None,
                              max_new_tokens: int = 150, temperature: float = 0.7) -> Dict[str, Any]:
        """ì‘ë‹µ ìƒì„±"""
        try:
            payload = {
                "user_message": user_message,
                "system_message": system_message or "ë‹¹ì‹ ì€ ë„ì›€ì´ ë˜ëŠ” AI ì–´ì‹œìŠ¤í„´íŠ¸ì…ë‹ˆë‹¤.",
                "influencer_name": influencer_name,
                "max_new_tokens": max_new_tokens,
                "temperature": temperature,
                "do_sample": True,
                "use_chat_template": True
            }
            if model_id:
                payload["model_id"] = model_id
            
            logger.debug(f"ğŸ”„ vLLM ì„œë²„ ìš”ì²­: {payload}")
            response = await self.client.post("/generate", json=payload)
            response.raise_for_status()
            
            result = response.json()
            logger.debug(f"âœ… ì‘ë‹µ ìƒì„± ì„±ê³µ: {influencer_name}")
            return result
            
        except httpx.HTTPStatusError as e:
            error_detail = ""
            try:
                error_detail = e.response.text if e.response else "No response"
            except:
                error_detail = "Cannot read response"
            
            logger.error(f"âŒ vLLM ì„œë²„ HTTP ì˜¤ë¥˜: {e.response.status_code} - {error_detail}")
            raise VLLMClientError(f"ì‘ë‹µ ìƒì„± ì‹¤íŒ¨: {e.response.status_code} - {error_detail}")
        except Exception as e:
            logger.error(f"âŒ ì‘ë‹µ ìƒì„± ì‹¤íŒ¨: {e}")
            raise VLLMClientError(f"ì‘ë‹µ ìƒì„± ì‹¤íŒ¨: {e}")
    
    async def generate_response_stream(self, user_message: str, system_message: str = None,
                                     influencer_name: str = "ì–´ì‹œìŠ¤í„´íŠ¸", model_id: str = None,
                                     max_new_tokens: int = 150, temperature: float = 0.7) -> AsyncIterator[str]:
        """ìŠ¤íŠ¸ë¦¬ë° ì‘ë‹µ ìƒì„±"""
        try:
            payload = {
                "user_message": user_message,
                "system_message": system_message or "ë‹¹ì‹ ì€ ë„ì›€ì´ ë˜ëŠ” AI ì–´ì‹œìŠ¤í„´íŠ¸ì…ë‹ˆë‹¤.",
                "influencer_name": influencer_name,
                "max_new_tokens": max_new_tokens,
                "temperature": temperature,
                "do_sample": True,
                "use_chat_template": True
            }
            if model_id:
                payload["model_id"] = model_id
            
            logger.debug(f"ğŸ”„ vLLM ì„œë²„ ìŠ¤íŠ¸ë¦¬ë° ìš”ì²­: {payload}")
            
            # íƒ€ì„ì•„ì›ƒì„ ëŠ˜ë ¤ì„œ ìŠ¤íŠ¸ë¦¬ë° ì‘ë‹µì„ ì™„ì „íˆ ë°›ì„ ìˆ˜ ìˆë„ë¡ í•¨
            async with httpx.AsyncClient(timeout=httpx.Timeout(300.0)) as client:
                async with client.stream("POST", f"{self.config.base_url}/generate/stream", json=payload) as response:
                    response.raise_for_status()
                    
                    async for line in response.aiter_lines():
                        if line.startswith("data: "):
                            try:
                                data = json.loads(line[6:])  # "data: " ì œê±°
                                if "text" in data:
                                    logger.debug(f"ğŸ”„ VLLM í† í° ìˆ˜ì‹ : {repr(data['text'])}")
                                    yield data["text"]
                                elif "error" in data:
                                    logger.error(f"âŒ VLLM ìŠ¤íŠ¸ë¦¬ë° ì˜¤ë¥˜: {data['error']}")
                                    yield f"ì˜¤ë¥˜: {data['error']}"
                                    break
                                elif "done" in data:
                                    break  # ìŠ¤íŠ¸ë¦¬ë° ì™„ë£Œ
                            except json.JSONDecodeError:
                                logger.warning(f"âš ï¸ JSON íŒŒì‹± ì‹¤íŒ¨: {line}")
                                continue
                    
        except httpx.HTTPStatusError as e:
            error_detail = ""
            try:
                error_detail = e.response.text if e.response else "No response"
            except:
                error_detail = "Cannot read response"
            
            logger.error(f"âŒ vLLM ì„œë²„ HTTP ì˜¤ë¥˜: {e.response.status_code} - {error_detail}")
            yield f"ì‘ë‹µ ìƒì„± ì‹¤íŒ¨: {e.response.status_code} - {error_detail}"
        except httpx.ReadTimeout:
            logger.error(f"âŒ vLLM ì„œë²„ ìŠ¤íŠ¸ë¦¬ë° íƒ€ì„ì•„ì›ƒ")
            yield "ì‘ë‹µ ìƒì„± ì‹œê°„ì´ ì´ˆê³¼ë˜ì—ˆìŠµë‹ˆë‹¤. ë‹¤ì‹œ ì‹œë„í•´ì£¼ì„¸ìš”."
        except httpx.ConnectError:
            logger.error(f"âŒ vLLM ì„œë²„ ì—°ê²° ì‹¤íŒ¨")
            yield "VLLM ì„œë²„ì— ì—°ê²°í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ì„œë²„ ìƒíƒœë¥¼ í™•ì¸í•´ì£¼ì„¸ìš”."
        except Exception as e:
            logger.error(f"âŒ ìŠ¤íŠ¸ë¦¬ë° ì‘ë‹µ ìƒì„± ì‹¤íŒ¨: {e}")
            yield f"ì‘ë‹µ ìƒì„± ì‹¤íŒ¨: {e}"
    
    async def list_adapters(self) -> Dict[str, Any]:
        """ë¡œë“œëœ ì–´ëŒ‘í„° ëª©ë¡ ì¡°íšŒ"""
        try:
            response = await self.client.get("/lora/adapters")
            response.raise_for_status()
            return response.json()
        except httpx.HTTPStatusError as e:
            if e.response.status_code == 404:
                logger.warning(f"âš ï¸ ì–´ëŒ‘í„° ì—”ë“œí¬ì¸íŠ¸ê°€ ì§€ì›ë˜ì§€ ì•ŠìŠµë‹ˆë‹¤: {e}")
                return {"adapters": []}  # ë¹ˆ ì–´ëŒ‘í„° ëª©ë¡ ë°˜í™˜
            else:
                logger.error(f"ì–´ëŒ‘í„° ëª©ë¡ ì¡°íšŒ ì‹¤íŒ¨: {e}")
                raise VLLMClientError(f"ì–´ëŒ‘í„° ëª©ë¡ ì¡°íšŒ ì‹¤íŒ¨: {e}")
        except Exception as e:
            logger.error(f"ì–´ëŒ‘í„° ëª©ë¡ ì¡°íšŒ ì‹¤íŒ¨: {e}")
            raise VLLMClientError(f"ì–´ëŒ‘í„° ëª©ë¡ ì¡°íšŒ ì‹¤íŒ¨: {e}")
    
    async def unload_adapter(self, model_id: str) -> Dict[str, Any]:
        """ì–´ëŒ‘í„° ì–¸ë¡œë“œ"""
        try:
            response = await self.client.delete(f"/lora/adapter/{model_id}")
            response.raise_for_status()
            
            result = response.json()
            logger.info(f"âœ… ì–´ëŒ‘í„° ì–¸ë¡œë“œ ì„±ê³µ: {model_id}")
            return result
            
        except Exception as e:
            logger.error(f"âŒ ì–´ëŒ‘í„° ì–¸ë¡œë“œ ì‹¤íŒ¨: {model_id}, {e}")
            raise VLLMClientError(f"ì–´ëŒ‘í„° ì–¸ë¡œë“œ ì‹¤íŒ¨: {e}")
    
    async def start_finetuning(self, influencer_id: str, influencer_name: str,
                             personality: str, qa_data: List[Dict], hf_repo_id: str,
                             hf_token: str, training_epochs: int = 5,
                             style_info: str = "", is_converted: bool = False,
                             system_prompt: str = "",
                             task_id: Optional[str] = None) -> Dict[str, Any]:
        """íŒŒì¸íŠœë‹ ì‹œì‘"""
        try:
            payload = {
                "influencer_id": influencer_id,
                "influencer_name": influencer_name,
                "personality": personality,
                "qa_data": qa_data,
                "hf_repo_id": hf_repo_id,
                "hf_token": hf_token,
                "training_epochs": training_epochs,
                "style_info": style_info,
                "is_converted": is_converted,
                "task_id": task_id,
                "system_prompt": system_prompt
            }
            
            response = await self.client.post("/finetuning/start", json=payload)
            response.raise_for_status()
            
            result = response.json()
            logger.info(f"âœ… íŒŒì¸íŠœë‹ ì‹œì‘ ì„±ê³µ: {influencer_id}")
            return result
            
        except Exception as e:
            logger.error(f"âŒ íŒŒì¸íŠœë‹ ì‹œì‘ ì‹¤íŒ¨: {influencer_id}, {e}")
            raise VLLMClientError(f"íŒŒì¸íŠœë‹ ì‹œì‘ ì‹¤íŒ¨: {e}")
    
    async def get_finetuning_status(self, task_id: str) -> Dict[str, Any]:
        """íŒŒì¸íŠœë‹ ìƒíƒœ ì¡°íšŒ"""
        try:
            response = await self.client.get(f"/finetuning/status/{task_id}")
            response.raise_for_status()
            return response.json()
        except Exception as e:
            logger.error(f"íŒŒì¸íŠœë‹ ìƒíƒœ ì¡°íšŒ ì‹¤íŒ¨: {task_id}, {e}")
            raise VLLMClientError(f"íŒŒì¸íŠœë‹ ìƒíƒœ ì¡°íšŒ ì‹¤íŒ¨: {e}")
    
    async def list_finetuning_tasks(self) -> Dict[str, Any]:
        """íŒŒì¸íŠœë‹ ì‘ì—… ëª©ë¡ ì¡°íšŒ"""
        try:
            response = await self.client.get("/finetuning/tasks")
            response.raise_for_status()
            return response.json()
        except Exception as e:
            logger.error(f"íŒŒì¸íŠœë‹ ì‘ì—… ëª©ë¡ ì¡°íšŒ ì‹¤íŒ¨: {e}")
            raise VLLMClientError(f"íŒŒì¸íŠœë‹ ì‘ì—… ëª©ë¡ ì¡°íšŒ ì‹¤íŒ¨: {e}")
    
    async def generate_voice(self, text: str, base_voice_url: str = None, influencer_id: str = None) -> Dict[str, Any]:
        """ìŒì„± ìƒì„± (ë² ì´ìŠ¤ ìŒì„±ì„ ì‚¬ìš©í•œ í´ë¡œë‹)"""
        try:
            if not base_voice_url:
                raise ValueError("ë² ì´ìŠ¤ ìŒì„± URLì´ í•„ìš”í•©ë‹ˆë‹¤")
            
            # base_voice_urlì—ì„œ íŒŒì¼ ë‹¤ìš´ë¡œë“œ ë° base64 ì¸ì½”ë”©
            import base64
            import httpx
            
            async with httpx.AsyncClient() as client:
                response = await client.get(base_voice_url)
                response.raise_for_status()
                voice_data = response.content
                voice_data_base64 = base64.b64encode(voice_data).decode('utf-8')
            
            payload = {
                "text": text,
                "voice_data_base64": voice_data_base64,
                "upload_to_s3": True,
                "s3_folder_prefix": f"tts/{influencer_id}" if influencer_id else "tts",
                "async_mode": True,  # ë¹„ë™ê¸° ëª¨ë“œ ì‚¬ìš©
                "language": "ko",
                "speaking_rate": 22.0,
                "pitch_std": 40.0,
                "cfg_scale": 4.0,
                "emotion": [0.3077, 0.0256, 0.0256, 0.0256, 0.0256, 0.0256, 0.2564, 0.3077]  # ì¤‘ë¦½ ê°ì •
            }

            response = await self.client.post("/zonos/generate_tts_with_voice", json=payload)
            response.raise_for_status()
            
            result = response.json()
            logger.info(f"ìŒì„± ìƒì„± ì‘ë‹µ: {result}")
            return result
        except Exception as e:
            logger.error(f"ìŒì„± ìƒì„± ì‹¤íŒ¨: {e}")
            raise VLLMClientError(f"ìŒì„± ìƒì„± ì‹¤íŒ¨: {e}")

    async def generate_qa_for_character(self, character_data: Dict[str, Any]) -> Dict[str, Any]:
        """ìºë¦­í„°ì— ëŒ€í•œ QA ìƒì„± (vLLM ì„œë²„ì˜ /speech/generate_qa_fast ì—”ë“œí¬ì¸íŠ¸ ì‚¬ìš©)"""
        try:
            # vLLM ì„œë²„ê°€ ê¸°ëŒ€í•˜ëŠ” í˜•ì‹ìœ¼ë¡œ í˜ì´ë¡œë“œ êµ¬ì„± (character í‚¤ë¡œ ê°ì‹¸ê¸°)
            payload = {
                "character": {
                    "name": character_data.get("name", ""),
                    "description": character_data.get("description", ""),
                    "age_range": character_data.get("age_range", ""),
                    "gender": character_data.get("gender", "NON_BINARY"),
                    "personality": character_data.get("personality", ""),
                    "mbti": character_data.get("mbti")
                }
            }
            
            logger.info(f"vLLM ì„œë²„ë¡œ QA ìƒì„± ìš”ì²­ (ê³ ì† ì—”ë“œí¬ì¸íŠ¸): {payload}")
            # ì˜¬ë°”ë¥¸ ì—”ë“œí¬ì¸íŠ¸ ì‚¬ìš© (/speech/generate_qa_fast)
            response = await self.client.post("/speech/generate_qa_fast", json=payload)
            response.raise_for_status()
            
            result = response.json()
            logger.debug(f"âœ… QA ìƒì„± ì„±ê³µ: {character_data.get('name', 'Unknown')}")
            
            # ìƒì„± ì‹œê°„ ì •ë³´ê°€ ìˆìœ¼ë©´ ë¡œê¹…
            if 'generation_time_seconds' in result:
                logger.info(f"âš¡ ìƒì„± ì†Œìš” ì‹œê°„: {result['generation_time_seconds']:.2f}ì´ˆ")
            
            return result
            
        except Exception as e:
            logger.error(f"âŒ QA ìƒì„± ì‹¤íŒ¨: {e}")
            if hasattr(e, 'response') and e.response is not None:
                logger.error(f"ì‘ë‹µ ìƒíƒœ: {e.response.status_code}")
                logger.error(f"ì‘ë‹µ ë‚´ìš©: {e.response.text}")
            raise VLLMClientError(f"QA ìƒì„± ì‹¤íŒ¨: {e}")


class VLLMWebSocketClient:
    """VLLM WebSocket í´ë¼ì´ì–¸íŠ¸"""
    
    def __init__(self, config: Optional[VLLMServerConfig] = None):
        self.config = config or VLLMServerConfig()
        self.websocket = None
    
    async def connect(self, lora_repo: str):
        """WebSocket ì—°ê²°"""
        try:
            # Base64 ì¸ì½”ë”© (ê¸°ì¡´ í˜¸í™˜ì„± ìœ ì§€)
            import base64
            encoded_repo = base64.b64encode(lora_repo.encode('utf-8')).decode('utf-8')
            
            ws_url = f"{self.config.ws_url}/ws/chat/{encoded_repo}"
            self.websocket = await websockets.connect(ws_url)
            logger.info(f"ğŸ”— VLLM WebSocket ì—°ê²° ì„±ê³µ: {lora_repo}")
            
        except Exception as e:
            logger.error(f"âŒ VLLM WebSocket ì—°ê²° ì‹¤íŒ¨: {e}")
            raise VLLMClientError(f"WebSocket ì—°ê²° ì‹¤íŒ¨: {e}")
    
    async def send_message(self, message: str, system_message: str = None,
                          influencer_name: str = "ì–´ì‹œìŠ¤í„´íŠ¸"):
        """ë©”ì‹œì§€ ì „ì†¡"""
        if not self.websocket:
            raise VLLMClientError("WebSocket ì—°ê²°ì´ ì—†ìŠµë‹ˆë‹¤.")
        
        try:
            payload = {
                "message": message,
                "system_message": system_message or "ë‹¹ì‹ ì€ ë„ì›€ì´ ë˜ëŠ” AI ì–´ì‹œìŠ¤í„´íŠ¸ì…ë‹ˆë‹¤.",
                "influencer_name": influencer_name
            }
            
            await self.websocket.send(json.dumps(payload))
            logger.debug(f"ğŸ“¤ ë©”ì‹œì§€ ì „ì†¡: {message[:50]}...")
            
        except Exception as e:
            logger.error(f"âŒ ë©”ì‹œì§€ ì „ì†¡ ì‹¤íŒ¨: {e}")
            raise VLLMClientError(f"ë©”ì‹œì§€ ì „ì†¡ ì‹¤íŒ¨: {e}")
    
    async def receive_response(self) -> Dict[str, Any]:
        """ì‘ë‹µ ìˆ˜ì‹ """
        if not self.websocket:
            raise VLLMClientError("WebSocket ì—°ê²°ì´ ì—†ìŠµë‹ˆë‹¤.")
        
        try:
            response = await self.websocket.recv()
            data = json.loads(response)
            logger.debug(f"ğŸ“¥ ì‘ë‹µ ìˆ˜ì‹ : {data.get('type', 'unknown')}")
            return data
            
        except Exception as e:
            logger.error(f"âŒ ì‘ë‹µ ìˆ˜ì‹  ì‹¤íŒ¨: {e}")
            raise VLLMClientError(f"ì‘ë‹µ ìˆ˜ì‹  ì‹¤íŒ¨: {e}")
    
    async def close(self):
        """ì—°ê²° ì¢…ë£Œ"""
        if self.websocket:
            await self.websocket.close()
            self.websocket = None
            logger.info("ğŸ”Œ VLLM WebSocket ì—°ê²° ì¢…ë£Œ")


_vllm_config = VLLMServerConfig(
    base_url=settings.VLLM_BASE_URL,
    timeout=getattr(settings, 'VLLM_TIMEOUT', 300)
)


async def get_vllm_client() -> VLLMClient:
    """VLLM í´ë¼ì´ì–¸íŠ¸ ì˜ì¡´ì„± ì£¼ì…ìš© í•¨ìˆ˜"""
    return VLLMClient(_vllm_config)


# í¸ì˜ í•¨ìˆ˜ë“¤
async def vllm_health_check() -> bool:
    """VLLM ì„œë²„ ìƒíƒœ í™•ì¸"""
    async with VLLMClient(_vllm_config) as client:
        return await client.health_check()


async def vllm_generate_response(user_message: str, system_message: str = None,
                                influencer_name: str = "ì–´ì‹œìŠ¤í„´íŠ¸", 
                                model_id: str = None, **kwargs) -> str:
    """VLLMì—ì„œ ì‘ë‹µ ìƒì„± (í¸ì˜ í•¨ìˆ˜)"""
    async with VLLMClient(_vllm_config) as client:
        result = await client.generate_response(
            user_message=user_message,
            system_message=system_message,
            influencer_name=influencer_name,
            model_id=model_id,
            **kwargs
        )
        return result.get("response", "")


async def vllm_load_adapter_if_needed(model_id: str, hf_repo_name: str,
                                     hf_token: str = None, 
                                     base_model_override: str = None) -> bool:
    """í•„ìš”ì‹œ ì–´ëŒ‘í„° ë¡œë“œ"""
    async with VLLMClient(_vllm_config) as client:
        try:
            # ë¡œë“œëœ ì–´ëŒ‘í„° ëª©ë¡ í™•ì¸
            adapters = await client.list_adapters()
            loaded_adapters = adapters.get("loaded_adapters", {})
            
            if model_id in loaded_adapters:
                logger.info(f"â™»ï¸ ì–´ëŒ‘í„° {model_id}ëŠ” ì´ë¯¸ ë¡œë“œë˜ì–´ ìˆìŠµë‹ˆë‹¤.")
                return True
            
            # ì–´ëŒ‘í„° ë¡œë“œ
            await client.load_adapter(model_id, hf_repo_name, hf_token, base_model_override)
            return True
            
        except Exception as e:
            logger.error(f"âŒ ì–´ëŒ‘í„° ë¡œë“œ ì‹¤íŒ¨: {model_id}, {e}")
            return False


async def vllm_generate_qa_for_character(character_data: Dict[str, Any]) -> Dict[str, Any]:
    """vLLMì—ì„œ ìºë¦­í„° QA ìƒì„± (í¸ì˜ í•¨ìˆ˜)"""
    async with VLLMClient(_vllm_config) as client:
        return await client.generate_qa_for_character(character_data)

async def vllm_generate_voice(text: str, base_voice_url: str = None, influencer_id: str = None) -> Dict[str, Any]:
    """vLLMì—ì„œ ìŒì„± ìƒì„± (í¸ì˜ í•¨ìˆ˜)"""
    async with VLLMClient(_vllm_config) as client:
        return await client.generate_voice(text, base_voice_url, influencer_id)