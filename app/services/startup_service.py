#!/usr/bin/env python3
"""
μ• ν”λ¦¬μΌ€μ΄μ… μ‹μ‘μ‹ μ‹¤ν–‰λλ” μ„λΉ„μ¤
QA λ°μ΄ν„°κ°€ μμ§€λ§ νμΈνλ‹μ΄ μ‹μ‘λμ§€ μ•μ€ μ‘μ—…λ“¤μ„ μλ™μΌλ΅ μ²λ¦¬
μ±—λ΄‡ μµμ…μ΄ ν™μ„±ν™”λ μΈν”λ£¨μ–Έμ„λ“¤μ vLLM μ–΄λ‘ν„° μλ™ λ΅λ“
"""

import asyncio
import logging
import os
from typing import List
from sqlalchemy.orm import Session
from datetime import datetime

from app.database import get_db

# batch_job_service μ κ±°λ¨ - BatchKey λ¨λΈ μ§μ ‘ μ‚¬μ©
from app.services.finetuning_service import get_finetuning_service
from app.models.influencer import BatchKey as BatchJob, AIInfluencer
from app.models.user import HFTokenManage
from app.services.influencers.qa_generator import QAGenerationStatus
from app.services.vllm_client import vllm_load_adapter_if_needed, vllm_health_check
from app.core.encryption import decrypt_sensitive_data
from app.utils.timezone_utils import get_current_kst

# λ΅κΉ… μ„¤μ •
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)


class StartupService:
    """μ• ν”λ¦¬μΌ€μ΄μ… μ‹μ‘μ‹ μ‹¤ν–‰λλ” μ„λΉ„μ¤"""

    def __init__(self):
        # batch_service μ κ±°λ¨
        self.finetuning_service = get_finetuning_service()

    async def check_and_restart_finetuning(self) -> int:
        """
        QA λ°μ΄ν„°κ°€ μ—…λ΅λ“λμ—μ§€λ§ νμΈνλ‹μ΄ μ‹μ‘λμ§€ μ•μ€ μ‘μ—…λ“¤μ„ μ°Ύμ•„ μλ™ μ‹μ‘
        Returns:
            μ¬μ‹μ‘λ νμΈνλ‹ μ‘μ—… μ
        """
        # ν™κ²½λ³€μλ΅ μλ™ νμΈνλ‹ λΉ„ν™μ„±ν™” ν™•μΈ
        auto_finetuning_enabled = (
            os.getenv("AUTO_FINETUNING_ENABLED", "true").lower() == "true"
        )

        if not auto_finetuning_enabled:
            logger.info(
                "π”’ μλ™ νμΈνλ‹μ΄ λΉ„ν™μ„±ν™”λμ–΄ μμµλ‹λ‹¤ (AUTO_FINETUNING_ENABLED=false)"
            )
            return 0

        try:
            logger.info("π” μ‹μ‘μ‹ μ„λΉ„μ¤ μ‹¤ν–‰...")
            db: Session = next(get_db())

            try:
                # 1. μ§„ν–‰ μ¤‘μ΄λ λ°°μΉ μ‘μ—… μƒνƒ ν™•μΈ λ° μ²λ¦¬
                logger.info("π” μ§„ν–‰ μ¤‘μ΄λ λ°°μΉ μ‘μ—… μƒνƒ ν™•μΈ μ¤‘...")
                in_progress_jobs = (
                    db.query(BatchJob)
                    .filter(
                        BatchJob.status.in_(["batch_submitted", "batch_processing"]),
                        BatchJob.openai_batch_id.isnot(None),
                    )
                    .all()
                )

                if in_progress_jobs:
                    logger.info(f"π― μ²λ¦¬ μ¤‘μ΄μ—λ μ‘μ—… {len(in_progress_jobs)}κ° λ°κ²¬")
                    from app.services.batch_monitor import (
                        BatchMonitor,
                    )  # μν™ μ°Έμ΅° λ°©μ§€λ¥Ό μ„ν•΄ μ—¬κΈ°μ„ import

                    monitor = BatchMonitor()
                    for job in in_progress_jobs:
                        try:
                            await monitor.check_and_update_single_job(job, db)
                            db.commit()  # κ°λ³„ μ‘μ—… μ²λ¦¬ ν›„ μ»¤λ°‹
                        except Exception as e:
                            logger.error(
                                f"β μ§„ν–‰ μ¤‘μ΄λ λ°°μΉ μ‘μ—… {job.batch_key_id} μ²λ¦¬ μ¤‘ μ¤λ¥: {e}",
                                exc_info=True,
                            )
                            db.rollback()  # μ¤λ¥ λ°μƒ μ‹ λ΅¤λ°±
                else:
                    logger.info("β… μ§„ν–‰ μ¤‘μ΄λ λ°°μΉ μ‘μ—… μ—†μ")

                # 2. S3 μ—…λ΅λ“ λ„λ½ μ‘μ—… μ²λ¦¬
                logger.info("π” S3 μ—…λ΅λ“ λ„λ½ μ‘μ—… κ²€μƒ‰ μ¤‘...")
                upload_candidates = (
                    db.query(BatchJob)
                    .filter(
                        BatchJob.status == "completed",
                        BatchJob.is_uploaded_to_s3 == False,
                        BatchJob.output_file_id.isnot(None),
                    )
                    .all()
                )

                if upload_candidates:
                    logger.info(f"π― S3 μ—…λ΅λ“ λ€μƒ {len(upload_candidates)}κ° λ°κ²¬")
                    from app.services.s3_service import get_s3_service
                    from openai import OpenAI
                    import tempfile

                    s3_service = get_s3_service()
                    openai_client = OpenAI(api_key=os.getenv("OPENAI_API_KEY"))

                    for job in upload_candidates:
                        tmp_file_path = None
                        try:
                            logger.info(
                                f"β¬‡οΈ OpenAI κ²°κ³Ό λ‹¤μ΄λ΅λ“ μ¤‘: output_file_id={job.output_file_id}"
                            )
                            file_content = openai_client.files.content(
                                job.output_file_id
                            ).read()

                            with tempfile.NamedTemporaryFile(
                                delete=False, mode="wb", suffix=".jsonl"
                            ) as tmp_file:
                                tmp_file.write(file_content)
                                tmp_file_path = tmp_file.name

                            s3_key = f"influencers/{job.influencer_id}/qa_results/{job.task_id}/generated_qa_results.jsonl"
                            logger.info(f"β¬†οΈ S3 μ—…λ΅λ“ μ¤‘: {s3_key}")
                            s3_url = s3_service.upload_file(tmp_file_path, s3_key)

                            if s3_url:
                                job.s3_qa_file_url = s3_url
                                job.is_uploaded_to_s3 = True
                                job.updated_at = get_current_kst()
                                db.commit()  # κ°λ³„ μ‘μ—… μ²λ¦¬ ν›„ μ»¤λ°‹
                                logger.info(f"β… S3 μ—…λ΅λ“ μ„±κ³µ: {s3_url}")
                            else:
                                logger.error(
                                    f"β S3 μ—…λ΅λ“ μ‹¤ν¨: task_id={job.task_id}"
                                )

                        except Exception as e:
                            logger.error(
                                f"β S3 μ—…λ΅λ“ μ²λ¦¬ μ¤‘ μ¤λ¥: task_id={job.task_id}, error={e}"
                            )
                            db.rollback()  # μ¤λ¥ λ°μƒ μ‹ λ΅¤λ°±
                        finally:
                            if tmp_file_path and os.path.exists(tmp_file_path):
                                os.remove(tmp_file_path)

                else:
                    logger.info("β… S3 μ—…λ΅λ“ λ„λ½ μ‘μ—… μ—†μ")

                # 3. νμΈνλ‹ μ¬μ‹μ‘ λ€μƒ κ²€μƒ‰
                logger.info("π” νμΈνλ‹ μ¬μ‹μ‘ λ€μƒ κ²€μƒ‰...")
                restart_candidates = (
                    db.query(BatchJob)
                    .filter(
                        BatchJob.status == "completed",
                        BatchJob.is_uploaded_to_s3 == True,
                        BatchJob.is_finetuning_started == False,
                        BatchJob.s3_qa_file_url.isnot(None),
                    )
                    .all()
                )

                if not restart_candidates:
                    logger.info("β… μ¬μ‹μ‘ν•  νμΈνλ‹ μ‘μ—…μ΄ μ—†μµλ‹λ‹¤")
                    return 0

                logger.info(
                    f"π― μ¬μ‹μ‘ λ€μƒ νμΈνλ‹ μ‘μ—… {len(restart_candidates)}κ° λ°κ²¬"
                )

                restarted_count = 0

                for batch_job in restart_candidates:
                    try:
                        logger.info(
                            f"π” νμΈνλ‹ μƒνƒ ν™•μΈ: task_id={batch_job.task_id}, influencer_id={batch_job.influencer_id}"
                        )

                        # μΈν”λ£¨μ–Έμ„ μ •λ³΄ μ΅°ν
                        from app.services.influencers.crud import get_influencer_by_id

                        influencer_data = get_influencer_by_id(
                            db, "system", batch_job.influencer_id
                        )

                        if not influencer_data:
                            logger.warning(
                                f"β οΈ μΈν”λ£¨μ–Έμ„λ¥Ό μ°Ύμ„ μ μ—†μ: {batch_job.influencer_id}"
                            )
                            continue

                        # μ΄λ―Έ νμΈνλ‹μ΄ μ™„λ£λμ—λ”μ§€ ν™•μΈ
                        if self.finetuning_service.is_influencer_finetuned(
                            influencer_data, db
                        ):
                            logger.info(
                                f"β… μ΄λ―Έ νμΈνλ‹ μ™„λ£λ¨: influencer_id={batch_job.influencer_id}"
                            )
                            # νμΈνλ‹ μ‹μ‘ ν”λκ·Έ μ—…λ°μ΄νΈ (μ¤‘λ³µ μ‹μ‘ λ°©μ§€)
                            batch_job.is_finetuning_started = True
                            db.commit()  # κ°λ³„ μ‘μ—… μ²λ¦¬ ν›„ μ»¤λ°‹
                            continue

                        logger.info(
                            f"π€ νμΈνλ‹ μλ™ μ¬μ‹μ‘: task_id={batch_job.task_id}, influencer_id={batch_job.influencer_id}"
                        )

                        # S3 URL ν™•μΈ λ° μμ •
                        s3_qa_url = batch_job.s3_qa_file_url

                        # μλ»λ URLμΈ κ²½μ° μμ • (μ„μ‹ μ΅°μΉ)
                        if s3_qa_url and "generated_qa_results.jsonl" in s3_qa_url:
                            logger.warning(
                                f"β οΈ μλ»λ S3 URL κ°μ§€, μ²λ¦¬λ QA URLλ΅ λ³€κ²½ μ‹λ„"
                            )
                            # processed_qa νμΌ URLλ΅ λ³€κ²½
                            s3_qa_url = s3_qa_url.replace(
                                "qa_results/", "qa_pairs/"
                            ).replace(
                                "generated_qa_results.jsonl",
                                f'processed_qa_{batch_job.task_id.split("_")[-1]}.json',
                            )
                            logger.info(f"π“ μμ •λ S3 URL: {s3_qa_url}")

                        # νμΈνλ‹ μ‹μ‘ (task_id μ „λ‹¬)
                        success = await self.finetuning_service.start_finetuning_for_influencer(
                            influencer_id=batch_job.influencer_id,
                            s3_qa_file_url=s3_qa_url,
                            db=db,
                            task_id=batch_job.task_id,
                        )

                        if success:
                            # νμΈνλ‹ μ‹μ‘ ν‘μ‹ (BatchKey λ¨λΈ μ§μ ‘ μ‚¬μ©)
                            batch_job.is_finetuning_started = True
                            batch_job.status = (
                                QAGenerationStatus.FINALIZED.value
                            )  # μµμΆ… μ™„λ£ μƒνƒλ΅ μ—…λ°μ΄νΈ
                            db.commit()  # κ°λ³„ μ‘μ—… μ²λ¦¬ ν›„ μ»¤λ°‹

                            restarted_count += 1
                            logger.info(
                                f"β… νμΈνλ‹ μλ™ μ¬μ‹μ‘ μ™„λ£: task_id={batch_job.task_id}"
                            )
                        else:
                            logger.warning(
                                f"β οΈ νμΈνλ‹ μλ™ μ¬μ‹μ‘ μ‹¤ν¨: task_id={batch_job.task_id}"
                            )

                    except Exception as e:
                        logger.error(
                            f"β νμΈνλ‹ μ¬μ‹μ‘ μ¤‘ μ¤λ¥: task_id={batch_job.task_id}, error={str(e)}"
                        )
                        db.rollback()  # μ¤λ¥ λ°μƒ μ‹ λ΅¤λ°±
                        continue

                if restarted_count > 0:
                    logger.info(
                        f"π‰ μ΄ {restarted_count}κ°μ νμΈνλ‹ μ‘μ—… μλ™ μ¬μ‹μ‘ μ™„λ£"
                    )
                else:
                    logger.warning("β οΈ μ¬μ‹μ‘ λ€μƒμ΄ μμ—μ§€λ§ λ¨λ‘ μ‹¤ν¨ν–μµλ‹λ‹¤")

                return restarted_count

            finally:
                db.close()

        except Exception as e:
            logger.error(f"β νμΈνλ‹ μ¬μ‹μ‘ κ²€μ‚¬ μ¤‘ μ¤λ¥: {str(e)}", exc_info=True)
            return 0

    async def cleanup_old_batch_jobs(self) -> int:
        """μ¤λλ λ°°μΉ μ‘μ—… μ •λ¦¬"""
        try:
            logger.info("π§Ή μ¤λλ λ°°μΉ μ‘μ—… μ •λ¦¬ μ¤‘...")

            db: Session = next(get_db())

            try:
                # 7μΌ μ΄μƒ λ μ‹¤ν¨ μ‘μ—… μ •λ¦¬ (BatchKey λ¨λΈ μ§μ ‘ μ‚¬μ©)
                from datetime import timedelta

                cutoff_date = datetime.now() - timedelta(days=7)

                old_failed_jobs = (
                    db.query(BatchJob)
                    .filter(
                        BatchJob.status == "failed", BatchJob.updated_at < cutoff_date
                    )
                    .all()
                )

                cleaned_count = len(old_failed_jobs)
                for job in old_failed_jobs:
                    try:
                        db.delete(job)
                        db.commit()  # κ°λ³„ μ‚­μ  ν›„ μ»¤λ°‹
                    except Exception as e:
                        logger.error(
                            f"β μ¤λλ λ°°μΉ μ‘μ—… {job.batch_key_id} μ •λ¦¬ μ¤‘ μ¤λ¥: {e}",
                            exc_info=True,
                        )
                        db.rollback()  # μ¤λ¥ λ°μƒ μ‹ λ΅¤λ°±

                if cleaned_count > 0:
                    logger.info(f"π—‘οΈ {cleaned_count}κ°μ μ¤λλ μ‹¤ν¨ μ‘μ—… μ •λ¦¬ μ™„λ£")

                return cleaned_count

            finally:
                db.close()

        except Exception as e:
            logger.error(f"β λ°°μΉ μ‘μ—… μ •λ¦¬ μ¤‘ μ¤λ¥: {str(e)}", exc_info=True)
            return 0

    async def run_startup_tasks(self):
        """μ‹μ‘μ‹ μ‹¤ν–‰ν•  λ¨λ“  μ‘μ—…λ“¤"""
        logger.info("π€ μ• ν”λ¦¬μΌ€μ΄μ… μ‹μ‘μ‹ μ‘μ—… μ‹¤ν–‰ μ¤‘...")

        try:
            # 1. νμΈνλ‹ μ¬μ‹μ‘ κ²€μ‚¬
            restarted_count = await self.check_and_restart_finetuning()

            # 2. μ¤λλ λ°°μΉ μ‘μ—… μ •λ¦¬
            cleaned_count = await self.cleanup_old_batch_jobs()

            # 3. ν—κΉ…νμ΄μ¤μ— μ—…λ΅λ“λ λ¨λ“  μΈν”λ£¨μ–Έμ„ λ¨λΈλ“¤μ vLLM μ–΄λ‘ν„° λ΅λ“
            loaded_count = await self.load_all_huggingface_models()

            logger.info(
                f"β… μ‹μ‘μ‹ μ‘μ—… μ™„λ£ - μ¬μ‹μ‘: {restarted_count}κ°, μ •λ¦¬: {cleaned_count}κ°, μ–΄λ‘ν„° λ΅λ“: {loaded_count}κ°"
            )

            # 4. μ±—λ΄‡ μµμ… ν™μ„±ν™”λ μΈν”λ£¨μ–Έμ„λ“¤μ vLLM μ–΄λ‘ν„° λ΅λ“ (ν•μ„ νΈν™μ„±)
            # await self.load_adapters_for_chat_enabled_influencers()

        except Exception as e:
            logger.error(f"β μ‹μ‘μ‹ μ‘μ—… μ‹¤ν–‰ μ¤‘ μ¤λ¥: {str(e)}", exc_info=True)

    async def load_adapters_for_chat_enabled_influencers(self):
        """μ±—λ΄‡ μµμ…μ΄ ν™μ„±ν™”λ μΈν”λ£¨μ–Έμ„λ“¤μ vLLM μ–΄λ‘ν„° λ΅λ“"""
        logger.info("π’¬ μ±—λ΄‡ ν™μ„±ν™”λ μΈν”λ£¨μ–Έμ„λ“¤μ vLLM μ–΄λ‘ν„° λ΅λ“ μ‹μ‘...")

        try:
            db = next(get_db())
            try:
                # μ±—λ΄‡ μµμ…μ΄ ν™μ„±ν™”λκ³  νμΈνλ‹λ λ¨λΈμ„ κ°€μ§„ μΈν”λ£¨μ–Έμ„ μ΅°ν
                chat_enabled_influencers = (
                    db.query(AIInfluencer)
                    .filter(
                        AIInfluencer.chatbot_option == True,
                        AIInfluencer.influencer_model_repo.isnot(None),
                        AIInfluencer.influencer_model_repo != "",
                    )
                    .all()
                )

                if not chat_enabled_influencers:
                    logger.info("π’¬ μ±—λ΄‡ ν™μ„±ν™”λ μΈν”λ£¨μ–Έμ„κ°€ μ—†μµλ‹λ‹¤.")
                    return

                logger.info(
                    f"π’¬ μ±—λ΄‡ ν™μ„±ν™”λ μΈν”λ£¨μ–Έμ„ {len(chat_enabled_influencers)}κ° λ°κ²¬"
                )

                loaded_count = 0
                for influencer in chat_enabled_influencers:
                    try:
                        # μΈν”λ£¨μ–Έμ„μ— μ§μ ‘ μ—°κ²°λ HF ν† ν° μ΅°ν
                        hf_token_record = None
                        if influencer.hf_manage_id:
                            # 1. μΈν”λ£¨μ–Έμ„μ— μ§μ ‘ ν• λ‹Ήλ ν† ν° μ΅°ν
                            hf_token_record = (
                                db.query(HFTokenManage)
                                .filter(
                                    HFTokenManage.hf_manage_id
                                    == influencer.hf_manage_id
                                )
                                .first()
                            )

                        if not hf_token_record:
                            # 2. κ°™μ€ κ·Έλ£Ήμ μ²« λ²μ§Έ ν† ν° μ‚¬μ©
                            hf_token_record = (
                                db.query(HFTokenManage)
                                .filter(HFTokenManage.group_id == influencer.group_id)
                                .first()
                            )

                        if not hf_token_record:
                            logger.warning(
                                f"β οΈ μΈν”λ£¨μ–Έμ„ {influencer.influencer_id}μ HF ν† ν°μ„ μ°Ύμ„ μ μ—†μµλ‹λ‹¤."
                            )
                            continue

                        # ν† ν° λ³µνΈν™”
                        try:
                            decrypted_token = decrypt_sensitive_data(
                                hf_token_record.hf_token_value
                            )
                            if not decrypted_token:
                                logger.warning(
                                    f"β οΈ μΈν”λ£¨μ–Έμ„ {influencer.influencer_id}μ HF ν† ν° λ³µνΈν™”μ— μ‹¤ν¨ν–μµλ‹λ‹¤."
                                )
                                continue
                        except Exception as decrypt_error:
                            logger.warning(
                                f"β οΈ μΈν”λ£¨μ–Έμ„ {influencer.influencer_id}μ HF ν† ν° λ³µνΈν™” μ¤‘ μ¤λ¥: {decrypt_error}"
                            )
                            continue

                        # vLLM μ–΄λ‘ν„° λ΅λ“
                        logger.info(
                            f"π”„ μ–΄λ‘ν„° λ΅λ“ μ¤‘: {influencer.influencer_id}"
                        )
                        success = await vllm_load_adapter_if_needed(
                            model_id=influencer.influencer_id,
                            hf_repo_name=influencer.influencer_model_repo,
                            hf_token=decrypted_token,
                            base_model_override="LGAI-EXAONE/EXAONE-3.5-2.4B-Instruct",  # κΈ°λ³Έ λ² μ΄μ¤ λ¨λΈ μ§€μ •
                        )

                        if success:
                            loaded_count += 1
                            logger.info(
                                f"β… μ–΄λ‘ν„° λ΅λ“ μ„±κ³µ: {influencer.influencer_model_repo}"
                            )
                        else:
                            logger.warning(
                                f"β οΈ μ–΄λ‘ν„° λ΅λ“ μ‹¤ν¨: {influencer.influencer_model_repo}"
                            )

                    except Exception as e:
                        logger.error(
                            f"β μΈν”λ£¨μ–Έμ„ {influencer.influencer_id} μ–΄λ‘ν„° λ΅λ“ μ¤‘ μ¤λ¥: {str(e)}"
                        )
                        continue

                logger.info(
                    f"π’¬ μ±—λ΄‡ μΈν”λ£¨μ–Έμ„ μ–΄λ‘ν„° λ΅λ“ μ™„λ£: {loaded_count}/{len(chat_enabled_influencers)}κ° μ„±κ³µ"
                )

            finally:
                db.close()

        except Exception as e:
            logger.error(
                f"β μ±—λ΄‡ μΈν”λ£¨μ–Έμ„ μ–΄λ‘ν„° λ΅λ“ μ¤‘ μ¤λ¥: {str(e)}", exc_info=True
            )

    async def load_all_huggingface_models(self) -> int:
        """ν—κΉ…νμ΄μ¤μ— μ—…λ΅λ“λ λ¨λ“  μΈν”λ£¨μ–Έμ„ λ¨λΈλ“¤μ vLLM μ–΄λ‘ν„° λ΅λ“"""
        logger.info("π¤— ν—κΉ…νμ΄μ¤μ— μ—…λ΅λ“λ λ¨λ“  μΈν”λ£¨μ–Έμ„ λ¨λΈ λ΅λ“ μ‹μ‘...")

        try:
            # VLLM μ„λ²„ μƒνƒ ν™•μΈ
            if not await vllm_health_check():
                logger.warning("β οΈ VLLM μ„λ²„κ°€ λΉ„ν™μ„±ν™”λμ—κ±°λ‚ μ—°κ²°ν•  μ μ—†μµλ‹λ‹¤.")
                return 0

            db = next(get_db())
            try:
                # ν—κΉ…νμ΄μ¤ λ¨λΈ repoκ°€ μλ” λ¨λ“  μΈν”λ£¨μ–Έμ„ μ΅°ν
                influencers_with_models = (
                    db.query(AIInfluencer)
                    .filter(
                        AIInfluencer.influencer_model_repo.isnot(None),
                        AIInfluencer.influencer_model_repo != "",
                    )
                    .all()
                )

                if not influencers_with_models:
                    logger.info("π¤— ν—κΉ…νμ΄μ¤μ— μ—…λ΅λ“λ λ¨λΈμ΄ μ—†μµλ‹λ‹¤.")
                    return 0

                logger.info(
                    f"π¤— μ΄ {len(influencers_with_models)}κ°μ ν—κΉ…νμ΄μ¤ λ¨λΈμ„ λ°κ²¬ν–μµλ‹λ‹¤."
                )
                loaded_count = 0

                # κ° μΈν”λ£¨μ–Έμ„μ μ–΄λ‘ν„° λ΅λ“
                for influencer in influencers_with_models:
                    print(influencer.group_id)
                    try:
                        # HF ν† ν° μ •λ³΄ κ°€μ Έμ¤κΈ°
                        hf_token_record = (
                            db.query(HFTokenManage)
                            .filter(HFTokenManage.group_id == influencer.group_id)
                            .first()
                        )

                        if not hf_token_record:
                            logger.warning(
                                f"β οΈ μΈν”λ£¨μ–Έμ„ {influencer.influencer_name}μ HF ν† ν°μ„ μ°Ύμ„ μ μ—†μµλ‹λ‹¤."
                            )
                            continue

                        # ν† ν° λ³µνΈν™”
                        try:
                            decrypted_token = decrypt_sensitive_data(
                                hf_token_record.hf_token_value
                            )
                            if not decrypted_token:
                                logger.warning(
                                    f"β οΈ μΈν”λ£¨μ–Έμ„ {influencer.influencer_name}μ HF ν† ν° λ³µνΈν™”μ— μ‹¤ν¨ν–μµλ‹λ‹¤."
                                )
                                continue
                        except Exception as decrypt_error:
                            logger.warning(
                                f"β οΈ μΈν”λ£¨μ–Έμ„ {influencer.influencer_name}μ HF ν† ν° λ³µνΈν™” μ¤‘ μ¤λ¥: {decrypt_error}"
                            )
                            continue

                        # vLLM μ–΄λ‘ν„° λ΅λ“
                        logger.info(
                            f"π”„ μ–΄λ‘ν„° λ΅λ“ μ¤‘: {influencer.influencer_name} - {influencer.influencer_model_repo}"
                        )
                        success = await vllm_load_adapter_if_needed(
                            model_id=influencer.influencer_id,
                            hf_repo_name=influencer.influencer_model_repo,
                            hf_token=decrypted_token,
                            base_model_override="LGAI-EXAONE/EXAONE-3.5-2.4B-Instruct",  # κΈ°λ³Έ λ² μ΄μ¤ λ¨λΈ μ§€μ •
                        )

                        if success:
                            loaded_count += 1
                            logger.info(
                                f"β… μ–΄λ‘ν„° λ΅λ“ μ„±κ³µ: {influencer.influencer_name} ({influencer.influencer_model_repo})"
                            )
                        else:
                            logger.warning(
                                f"β οΈ μ–΄λ‘ν„° λ΅λ“ μ‹¤ν¨: {influencer.influencer_name} ({influencer.influencer_model_repo})"
                            )

                    except Exception as e:
                        logger.error(
                            f"β μΈν”λ£¨μ–Έμ„ {influencer.influencer_name} μ–΄λ‘ν„° λ΅λ“ μ¤‘ μ¤λ¥: {str(e)}"
                        )
                        continue

                logger.info(
                    f"π¤— ν—κΉ…νμ΄μ¤ λ¨λΈ μ–΄λ‘ν„° λ΅λ“ μ™„λ£: {loaded_count}/{len(influencers_with_models)}κ° μ„±κ³µ"
                )
                return loaded_count

            finally:
                db.close()

        except Exception as e:
            logger.error(f"β ν—κΉ…νμ΄μ¤ λ¨λΈ λ΅λ“ μ¤‘ μ¤λ¥: {str(e)}", exc_info=True)
            return 0


# κΈ€λ΅λ² μ‹μ‘μ‹ μ„λΉ„μ¤ μΈμ¤ν„΄μ¤
startup_service = StartupService()


def get_startup_service() -> StartupService:
    """μ‹μ‘μ‹ μ„λΉ„μ¤ μμ΅΄μ„± μ£Όμ…"""
    return startup_service


async def run_startup_tasks():
    """μ• ν”λ¦¬μΌ€μ΄μ… μ‹μ‘μ‹ μ‹¤ν–‰ν•  μ‘μ—…λ“¤"""
    service = get_startup_service()
    await service.run_startup_tasks()


async def load_adapters_for_chat_enabled_influencers(db: Session):
    """μ±—λ΄‡ μµμ…μ΄ ν™μ„±ν™”λ μΈν”λ£¨μ–Έμ„λ“¤μ vLLM μ–΄λ‘ν„° λ΅λ“ (main.pyμ—μ„ μ‚¬μ©)"""
    service = get_startup_service()
    await service.load_adapters_for_chat_enabled_influencers()
