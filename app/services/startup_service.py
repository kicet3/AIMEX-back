#!/usr/bin/env python3
"""
ì• í”Œë¦¬ì¼€ì´ì…˜ ì‹œì‘ì‹œ ì‹¤í–‰ë˜ëŠ” ì„œë¹„ìŠ¤
QA ë°ì´í„°ê°€ ìˆì§€ë§Œ íŒŒì¸íŠœë‹ì´ ì‹œì‘ë˜ì§€ ì•Šì€ ì‘ì—…ë“¤ì„ ìë™ìœ¼ë¡œ ì²˜ë¦¬
"""

import asyncio
import logging
import os
from typing import List
from sqlalchemy.orm import Session
from datetime import datetime

from app.database import get_db

# batch_job_service ì œê±°ë¨ - BatchKey ëª¨ë¸ ì§ì ‘ ì‚¬ìš©
from app.services.finetuning_service import get_finetuning_service
from app.models.influencer import BatchKey as BatchJob, AIInfluencer
from app.models.user import HFTokenManage
from app.services.influencers.qa_generator import QAGenerationStatus
from app.core.encryption import decrypt_sensitive_data
from app.utils.timezone_utils import get_current_kst
from app.services.hf_token_resolver import get_token_for_influencer

# ë¡œê¹… ì„¤ì •
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)


class StartupService:
    """ì• í”Œë¦¬ì¼€ì´ì…˜ ì‹œì‘ì‹œ ì‹¤í–‰ë˜ëŠ” ì„œë¹„ìŠ¤"""

    def __init__(self):
        # batch_service ì œê±°ë¨
        self.finetuning_service = get_finetuning_service()

    async def check_and_restart_finetuning(self) -> int:
        """
        QA ë°ì´í„°ê°€ ì—…ë¡œë“œë˜ì—ˆì§€ë§Œ íŒŒì¸íŠœë‹ì´ ì‹œì‘ë˜ì§€ ì•Šì€ ì‘ì—…ë“¤ì„ ì°¾ì•„ ìë™ ì‹œì‘
        Returns:
            ì¬ì‹œì‘ëœ íŒŒì¸íŠœë‹ ì‘ì—… ìˆ˜
        """
        # í™˜ê²½ë³€ìˆ˜ë¡œ ìë™ íŒŒì¸íŠœë‹ ë¹„í™œì„±í™” í™•ì¸
        auto_finetuning_enabled = (
            os.getenv("AUTO_FINETUNING_ENABLED", "true").lower() == "true"
        )

        if not auto_finetuning_enabled:
            logger.info(
                "ğŸ”’ ìë™ íŒŒì¸íŠœë‹ì´ ë¹„í™œì„±í™”ë˜ì–´ ìˆìŠµë‹ˆë‹¤ (AUTO_FINETUNING_ENABLED=false)"
            )
            return 0

        try:
            logger.info("ğŸ” ì‹œì‘ì‹œ ì„œë¹„ìŠ¤ ì‹¤í–‰...")
            db: Session = next(get_db())

            try:
                # 1. ì§„í–‰ ì¤‘ì´ë˜ ë°°ì¹˜ ì‘ì—… ìƒíƒœ í™•ì¸ ë° ì²˜ë¦¬
                logger.info("ğŸ” ì§„í–‰ ì¤‘ì´ë˜ ë°°ì¹˜ ì‘ì—… ìƒíƒœ í™•ì¸ ì¤‘...")
                in_progress_jobs = (
                    db.query(BatchJob)
                    .filter(
                        BatchJob.status.in_(["batch_submitted", "batch_processing"]),
                        BatchJob.openai_batch_id.isnot(None),
                    )
                    .all()
                )

                if in_progress_jobs:
                    logger.info(f"ğŸ¯ ì²˜ë¦¬ ì¤‘ì´ì—ˆë˜ ì‘ì—… {len(in_progress_jobs)}ê°œ ë°œê²¬")
                    from app.services.batch_monitor import (
                        BatchMonitor,
                    )  # ìˆœí™˜ ì°¸ì¡° ë°©ì§€ë¥¼ ìœ„í•´ ì—¬ê¸°ì„œ import

                    monitor = BatchMonitor()
                    for job in in_progress_jobs:
                        try:
                            await monitor.check_and_update_single_job(job, db)
                            db.commit()  # ê°œë³„ ì‘ì—… ì²˜ë¦¬ í›„ ì»¤ë°‹
                        except Exception as e:
                            logger.error(
                                f"âŒ ì§„í–‰ ì¤‘ì´ë˜ ë°°ì¹˜ ì‘ì—… {job.batch_key_id} ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜: {e}",
                                exc_info=True,
                            )
                            db.rollback()  # ì˜¤ë¥˜ ë°œìƒ ì‹œ ë¡¤ë°±
                else:
                    logger.info("âœ… ì§„í–‰ ì¤‘ì´ë˜ ë°°ì¹˜ ì‘ì—… ì—†ìŒ")

                # 2. S3 ì—…ë¡œë“œ ëˆ„ë½ ì‘ì—… ì²˜ë¦¬
                logger.info("ğŸ” S3 ì—…ë¡œë“œ ëˆ„ë½ ì‘ì—… ê²€ìƒ‰ ì¤‘...")
                upload_candidates = (
                    db.query(BatchJob)
                    .filter(
                        BatchJob.status == "completed",
                        BatchJob.is_uploaded_to_s3 == False,
                        BatchJob.output_file_id.isnot(None),
                    )
                    .all()
                )

                if upload_candidates:
                    logger.info(f"ğŸ¯ S3 ì—…ë¡œë“œ ëŒ€ìƒ {len(upload_candidates)}ê°œ ë°œê²¬")
                    from app.services.s3_service import get_s3_service
                    from openai import OpenAI
                    import tempfile

                    s3_service = get_s3_service()
                    openai_client = OpenAI(api_key=os.getenv("OPENAI_API_KEY"))

                    for job in upload_candidates:
                        tmp_file_path = None
                        try:
                            logger.info(
                                f"â¬‡ï¸ OpenAI ê²°ê³¼ ë‹¤ìš´ë¡œë“œ ì¤‘: output_file_id={job.output_file_id}"
                            )
                            file_content = openai_client.files.content(
                                job.output_file_id
                            ).read()

                            with tempfile.NamedTemporaryFile(
                                delete=False, mode="wb", suffix=".jsonl"
                            ) as tmp_file:
                                tmp_file.write(file_content)
                                tmp_file_path = tmp_file.name

                            s3_key = f"influencers/{job.influencer_id}/qa_results/{job.task_id}/generated_qa_results.jsonl"
                            logger.info(f"â¬†ï¸ S3 ì—…ë¡œë“œ ì¤‘: {s3_key}")
                            s3_url = s3_service.upload_file(tmp_file_path, s3_key)

                            if s3_url:
                                job.s3_qa_file_url = s3_url
                                job.is_uploaded_to_s3 = True
                                job.updated_at = get_current_kst()
                                db.commit()  # ê°œë³„ ì‘ì—… ì²˜ë¦¬ í›„ ì»¤ë°‹
                                logger.info(f"âœ… S3 ì—…ë¡œë“œ ì„±ê³µ: {s3_url}")
                            else:
                                logger.error(
                                    f"âŒ S3 ì—…ë¡œë“œ ì‹¤íŒ¨: task_id={job.task_id}"
                                )

                        except Exception as e:
                            logger.error(
                                f"âŒ S3 ì—…ë¡œë“œ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜: task_id={job.task_id}, error={e}"
                            )
                            db.rollback()  # ì˜¤ë¥˜ ë°œìƒ ì‹œ ë¡¤ë°±
                        finally:
                            if tmp_file_path and os.path.exists(tmp_file_path):
                                os.remove(tmp_file_path)

                else:
                    logger.info("âœ… S3 ì—…ë¡œë“œ ëˆ„ë½ ì‘ì—… ì—†ìŒ")

                # 3. íŒŒì¸íŠœë‹ ì¬ì‹œì‘ ëŒ€ìƒ ê²€ìƒ‰
                logger.info("ğŸ” íŒŒì¸íŠœë‹ ì¬ì‹œì‘ ëŒ€ìƒ ê²€ìƒ‰...")
                restart_candidates = (
                    db.query(BatchJob)
                    .filter(
                        BatchJob.status == "completed",
                        BatchJob.is_uploaded_to_s3 == True,
                        BatchJob.is_finetuning_started == False,
                        BatchJob.s3_qa_file_url.isnot(None),
                    )
                    .all()
                )

                if not restart_candidates:
                    logger.info("âœ… ì¬ì‹œì‘í•  íŒŒì¸íŠœë‹ ì‘ì—…ì´ ì—†ìŠµë‹ˆë‹¤")
                    return 0

                logger.info(
                    f"ğŸ¯ ì¬ì‹œì‘ ëŒ€ìƒ íŒŒì¸íŠœë‹ ì‘ì—… {len(restart_candidates)}ê°œ ë°œê²¬"
                )

                restarted_count = 0

                for batch_job in restart_candidates:
                    try:
                        logger.info(
                            f"ğŸ” íŒŒì¸íŠœë‹ ìƒíƒœ í™•ì¸: task_id={batch_job.task_id}, influencer_id={batch_job.influencer_id}"
                        )

                        # ì¸í”Œë£¨ì–¸ì„œ ì •ë³´ ì¡°íšŒ (ê¶Œí•œ ì²´í¬ ì—†ì´ ì§ì ‘ ì¡°íšŒ)
                        influencer_data = (
                            db.query(AIInfluencer)
                            .filter(
                                AIInfluencer.influencer_id == batch_job.influencer_id
                            )
                            .first()
                        )

                        if not influencer_data:
                            logger.warning(
                                f"âš ï¸ ì¸í”Œë£¨ì–¸ì„œë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŒ: {batch_job.influencer_id}"
                            )
                            continue

                        # ì¸í”Œë£¨ì–¸ì„œì˜ ê·¸ë£¹ì— ì†í•œ ì‚¬ìš©ì ì°¾ê¸° (ê¶Œí•œ ì²´í¬ìš©)
                        from app.models.user import User, Team

                        # user_idë¥¼ ì‚¬ìš©í•˜ëŠ” ê²½ìš°
                        if influencer_data.user_id:
                            user_id_for_check = influencer_data.user_id
                        # group_idë¥¼ ì‚¬ìš©í•˜ëŠ” ê²½ìš°
                        elif influencer_data.group_id:
                            # ê·¸ë£¹ì— ì†í•œ ì²« ë²ˆì§¸ ì‚¬ìš©ì ì°¾ê¸°
                            group_user = (
                                db.query(User)
                                .join(User.teams)
                                .filter(Team.group_id == influencer_data.group_id)
                                .first()
                            )
                            if group_user:
                                user_id_for_check = group_user.user_id
                            else:
                                logger.warning(
                                    f"âš ï¸ ê·¸ë£¹ {influencer_data.group_id}ì— ì‚¬ìš©ìê°€ ì—†ìŒ"
                                )
                                continue
                        else:
                            logger.warning(
                                f"âš ï¸ ì¸í”Œë£¨ì–¸ì„œ {batch_job.influencer_id}ì— user_idë‚˜ group_idê°€ ì—†ìŒ"
                            )
                            continue

                        # get_influencer_by_idë¥¼ ì‚¬ìš©í•˜ì—¬ ê¶Œí•œ ì²´í¬
                        from app.services.influencers.crud import get_influencer_by_id

                        try:
                            influencer_data = await get_influencer_by_id(
                                db, user_id_for_check, batch_job.influencer_id
                            )
                        except HTTPException:
                            logger.warning(
                                f"âš ï¸ ì‚¬ìš©ì {user_id_for_check}ëŠ” ì¸í”Œë£¨ì–¸ì„œ {batch_job.influencer_id}ì— ì ‘ê·¼ ê¶Œí•œì´ ì—†ìŒ"
                            )
                            continue

                        if not influencer_data:
                            logger.warning(
                                f"âš ï¸ ì¸í”Œë£¨ì–¸ì„œë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŒ: {batch_job.influencer_id}"
                            )
                            continue

                        # ì´ë¯¸ íŒŒì¸íŠœë‹ì´ ì™„ë£Œë˜ì—ˆëŠ”ì§€ í™•ì¸
                        if self.finetuning_service.is_influencer_finetuned(
                            influencer_data, db
                        ):
                            logger.info(
                                f"âœ… ì´ë¯¸ íŒŒì¸íŠœë‹ ì™„ë£Œë¨: influencer_id={batch_job.influencer_id}"
                            )
                            # íŒŒì¸íŠœë‹ ì‹œì‘ í”Œë˜ê·¸ ì—…ë°ì´íŠ¸ (ì¤‘ë³µ ì‹œì‘ ë°©ì§€)
                            batch_job.is_finetuning_started = True
                            db.commit()  # ê°œë³„ ì‘ì—… ì²˜ë¦¬ í›„ ì»¤ë°‹
                            continue

                        logger.info(
                            f"ğŸš€ íŒŒì¸íŠœë‹ ìë™ ì¬ì‹œì‘: task_id={batch_job.task_id}, influencer_id={batch_job.influencer_id}"
                        )

                        # S3 URL í™•ì¸
                        s3_qa_url = batch_job.s3_qa_file_url

                        # ì›ë³¸ URL ê·¸ëŒ€ë¡œ ì‚¬ìš© (íŒŒì¼ì´ ì‹¤ì œë¡œ ì¡´ì¬í•˜ëŠ” ê²½ë¡œ)
                        if s3_qa_url and "generated_qa_results.jsonl" in s3_qa_url:
                            logger.info(f"âœ… ì›ë³¸ QA íŒŒì¼ URL ì‚¬ìš©: {s3_qa_url}")

                        # íŒŒì¸íŠœë‹ ì‹œì‘ (task_id ì „ë‹¬)
                        try:
                            success = await self.finetuning_service.start_finetuning_for_influencer(
                                influencer_id=batch_job.influencer_id,
                                s3_qa_file_url=s3_qa_url,
                                db=db,
                                task_id=batch_job.task_id,
                            )
                        except Exception as fe:
                            logger.error(
                                f"âŒ íŒŒì¸íŠœë‹ ì‹œì‘ ì¤‘ ì˜ˆì™¸ ë°œìƒ: {type(fe).__name__}: {str(fe)}",
                                exc_info=True,
                            )
                            success = False

                        if success:
                            # íŒŒì¸íŠœë‹ ì‹œì‘ í‘œì‹œ (BatchKey ëª¨ë¸ ì§ì ‘ ì‚¬ìš©)
                            batch_job.is_finetuning_started = True
                            batch_job.status = (
                                QAGenerationStatus.FINALIZED.value
                            )  # ìµœì¢… ì™„ë£Œ ìƒíƒœë¡œ ì—…ë°ì´íŠ¸
                            db.commit()  # ê°œë³„ ì‘ì—… ì²˜ë¦¬ í›„ ì»¤ë°‹

                            restarted_count += 1
                            logger.info(
                                f"âœ… íŒŒì¸íŠœë‹ ìë™ ì¬ì‹œì‘ ì™„ë£Œ: task_id={batch_job.task_id}"
                            )
                        else:
                            logger.warning(
                                f"âš ï¸ íŒŒì¸íŠœë‹ ìë™ ì¬ì‹œì‘ ì‹¤íŒ¨: task_id={batch_job.task_id}"
                            )

                    except Exception as e:
                        logger.error(
                            f"âŒ íŒŒì¸íŠœë‹ ì¬ì‹œì‘ ì¤‘ ì˜¤ë¥˜: task_id={batch_job.task_id}, error={str(e)}",
                            exc_info=True,  # ì „ì²´ ìŠ¤íƒ íŠ¸ë ˆì´ìŠ¤ ì¶œë ¥
                        )
                        db.rollback()  # ì˜¤ë¥˜ ë°œìƒ ì‹œ ë¡¤ë°±
                        continue

                if restarted_count > 0:
                    logger.info(
                        f"ğŸ‰ ì´ {restarted_count}ê°œì˜ íŒŒì¸íŠœë‹ ì‘ì—… ìë™ ì¬ì‹œì‘ ì™„ë£Œ"
                    )
                else:
                    logger.warning("âš ï¸ ì¬ì‹œì‘ ëŒ€ìƒì´ ìˆì—ˆì§€ë§Œ ëª¨ë‘ ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤")

                return restarted_count

            finally:
                db.close()

        except Exception as e:
            logger.error(f"âŒ íŒŒì¸íŠœë‹ ì¬ì‹œì‘ ê²€ì‚¬ ì¤‘ ì˜¤ë¥˜: {str(e)}", exc_info=True)
            return 0

    async def cleanup_old_batch_jobs(self) -> int:
        """ì˜¤ë˜ëœ ë°°ì¹˜ ì‘ì—… ì •ë¦¬"""
        try:
            logger.info("ğŸ§¹ ì˜¤ë˜ëœ ë°°ì¹˜ ì‘ì—… ì •ë¦¬ ì¤‘...")

            db: Session = next(get_db())

            try:
                # 7ì¼ ì´ìƒ ëœ ì‹¤íŒ¨ ì‘ì—… ì •ë¦¬ (BatchKey ëª¨ë¸ ì§ì ‘ ì‚¬ìš©)
                from datetime import timedelta

                cutoff_date = datetime.now() - timedelta(days=7)

                old_failed_jobs = (
                    db.query(BatchJob)
                    .filter(
                        BatchJob.status == "failed", BatchJob.updated_at < cutoff_date
                    )
                    .all()
                )

                cleaned_count = len(old_failed_jobs)
                for job in old_failed_jobs:
                    try:
                        db.delete(job)
                        db.commit()  # ê°œë³„ ì‚­ì œ í›„ ì»¤ë°‹
                    except Exception as e:
                        logger.error(
                            f"âŒ ì˜¤ë˜ëœ ë°°ì¹˜ ì‘ì—… {job.batch_key_id} ì •ë¦¬ ì¤‘ ì˜¤ë¥˜: {e}",
                            exc_info=True,
                        )
                        db.rollback()  # ì˜¤ë¥˜ ë°œìƒ ì‹œ ë¡¤ë°±

                if cleaned_count > 0:
                    logger.info(f"ğŸ—‘ï¸ {cleaned_count}ê°œì˜ ì˜¤ë˜ëœ ì‹¤íŒ¨ ì‘ì—… ì •ë¦¬ ì™„ë£Œ")

                return cleaned_count

            finally:
                db.close()

        except Exception as e:
            logger.error(f"âŒ ë°°ì¹˜ ì‘ì—… ì •ë¦¬ ì¤‘ ì˜¤ë¥˜: {str(e)}", exc_info=True)
            return 0

    async def run_startup_tasks(self):
        """ì‹œì‘ì‹œ ì‹¤í–‰í•  ëª¨ë“  ì‘ì—…ë“¤"""
        logger.info("ğŸš€ ì• í”Œë¦¬ì¼€ì´ì…˜ ì‹œì‘ì‹œ ì‘ì—… ì‹¤í–‰ ì¤‘...")

        try:
            # 1. íŒŒì¸íŠœë‹ ì¬ì‹œì‘ ê²€ì‚¬
            restarted_count = await self.check_and_restart_finetuning()

            # 2. ì˜¤ë˜ëœ ë°°ì¹˜ ì‘ì—… ì •ë¦¬
            cleaned_count = await self.cleanup_old_batch_jobs()

            logger.info(
                f"âœ… ì‹œì‘ì‹œ ì‘ì—… ì™„ë£Œ - ì¬ì‹œì‘: {restarted_count}ê°œ, ì •ë¦¬: {cleaned_count}ê°œ"
            )


        except Exception as e:
            logger.error(f"âŒ ì‹œì‘ì‹œ ì‘ì—… ì‹¤í–‰ ì¤‘ ì˜¤ë¥˜: {str(e)}", exc_info=True)


# ê¸€ë¡œë²Œ ì‹œì‘ì‹œ ì„œë¹„ìŠ¤ ì¸ìŠ¤í„´ìŠ¤
startup_service = StartupService()


def get_startup_service() -> StartupService:
    """ì‹œì‘ì‹œ ì„œë¹„ìŠ¤ ì˜ì¡´ì„± ì£¼ì…"""
    return startup_service


async def run_startup_tasks():
    """ì• í”Œë¦¬ì¼€ì´ì…˜ ì‹œì‘ì‹œ ì‹¤í–‰í•  ì‘ì—…ë“¤"""
    service = get_startup_service()
    await service.run_startup_tasks()

