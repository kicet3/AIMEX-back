#!/usr/bin/env python3
"""
ì¸í”Œë£¨ì–¸ì„œ ì „ìš© QA ìƒì„± ì„œë¹„ìŠ¤
speech_generatorì™€ generate_qa ë¡œì§ì„ í™œìš©í•˜ì—¬ ì¸í”Œë£¨ì–¸ì„œë³„ 2000ìŒì˜ QA ìƒì„±
"""

import json
import os
import time
import random
import tempfile
import logging
import asyncio
from typing import List, Dict, Optional
from openai import OpenAI
from datetime import datetime
from dataclasses import dataclass
from enum import Enum
from sqlalchemy.orm import Session

from app.database import get_db
from app.services.influencers.crud import get_influencer_by_id
from app.models.influencer import BatchKey
from app.core.config import settings
# Backend ë‚´ë¶€ ëª¨ë¸ ì‚¬ìš©
from app.models.vllm_models import Gender, VLLMCharacterProfile
from dotenv import load_dotenv
load_dotenv()

# í•˜ìœ„ í˜¸í™˜ì„±ì„ ìœ„í•œ ë³„ì¹­
CharacterProfile = VLLMCharacterProfile

class SpeechGenerator:
    """vLLM ì„œë²„ ëŒ€ì‹  HTTP API í´ë¼ì´ì–¸íŠ¸ ì‚¬ìš©"""
    def __init__(self, *args, **kwargs):
        pass
    
    def generate_character_random_tones_sync(self, *args, **kwargs):
        raise RuntimeError("ì´ ë©”ì„œë“œëŠ” ë” ì´ìƒ ì‚¬ìš©ë˜ì§€ ì•ŠìŠµë‹ˆë‹¤. vLLM ì„œë²„ APIë¥¼ ì‚¬ìš©í•˜ì„¸ìš”.")


class QAGenerationStatus(Enum):
    PENDING = "pending"
    TONE_GENERATION = "tone_generation"      # ì–´íˆ¬ ìƒì„± ì¤‘
    DOMAIN_PREPARATION = "domain_preparation" # ë„ë©”ì¸ë³„ ì§ˆë¬¸ ì¤€ë¹„
    PROCESSING = "processing"  
    BATCH_SUBMITTED = "batch_submitted"
    BATCH_PROCESSING = "batch_processing"
    BATCH_COMPLETED = "batch_completed"
    BATCH_UPLOAD = "batch_upload"            # S3 ì—…ë¡œë“œ ì¤‘
    PROCESSING_RESULTS = "processing_results"
    COMPLETED = "completed"
    FINALIZED = "finalized"
    FAILED = "failed"


class InfluencerQAGenerator:
    def __init__(self, api_key: Optional[str] = None):
        """
        ì¸í”Œë£¨ì–¸ì„œìš© QA ìƒì„±ê¸°
        Args:
            api_key: OpenAI API í‚¤
        """
        self.client = OpenAI(api_key=api_key or os.getenv('OPENAI_API_KEY'))
        self.speech_generator = SpeechGenerator(api_key)
        
    def influencer_to_character_profile(self, influencer_data: dict, style_preset: dict = None, mbti: dict = None) -> CharacterProfile:
        """
        ì¸í”Œë£¨ì–¸ì„œ ë°ì´í„°ë¥¼ CharacterProfileë¡œ ë³€í™˜
        Args:
            influencer_data: DBì—ì„œ ê°€ì ¸ì˜¨ ì¸í”Œë£¨ì–¸ì„œ ë°ì´í„°
            style_preset: ìŠ¤íƒ€ì¼ í”„ë¦¬ì…‹ ë°ì´í„°
            mbti: MBTI ë°ì´í„°
        Returns:
            CharacterProfile ê°ì²´
        """
        # ì„±ë³„ ë§¤í•‘ (influencer_gender: 1=ë‚¨ì„±, 2=ì—¬ì„±, 3=ì¤‘ì„±)
        gender_map = {
            1: Gender.MALE,
            2: Gender.FEMALE, 
            3: Gender.NON_BINARY
        }
        
        # ë‚˜ì´ëŒ€ ë§¤í•‘ (influencer_age_group: 1=10ëŒ€, 2=20ëŒ€, 3=30ëŒ€, 4=40ëŒ€, 5=50ëŒ€+)
        age_group_map = {
            1: 15,  # 10ëŒ€
            2: 25,  # 20ëŒ€  
            3: 35,  # 30ëŒ€
            4: 45,  # 40ëŒ€
            5: 55   # 50ëŒ€+
        }
        
        # ê¸°ë³¸ê°’ ì„¤ì •
        name = influencer_data.get('influencer_name', 'ì¸í”Œë£¨ì–¸ì„œ')
        description = influencer_data.get('influencer_description', '')
        
        # ìŠ¤íƒ€ì¼ í”„ë¦¬ì…‹ì—ì„œ ì •ë³´ ì¶”ì¶œ
        if style_preset:
            gender = gender_map.get(style_preset.get('influencer_gender'), Gender.NON_BINARY)
            age_group = style_preset.get('influencer_age_group')
            age_range = f"{age_group_map.get(age_group, 25)}ëŒ€" if age_group else "ì•Œ ìˆ˜ ì—†ìŒ"
            personality = style_preset.get('influencer_personality', 'ì¹œê·¼í•˜ê³  í™œë°œí•œ ì„±ê²©')
            
            # ì„¤ëª…ì— ìŠ¤íƒ€ì¼ ì •ë³´ ì¶”ê°€
            if not description:
                hairstyle = style_preset.get('influencer_hairstyle', '')
                style = style_preset.get('influencer_style', '')
                speech = style_preset.get('influencer_speech', '')
                description = f"í—¤ì–´ìŠ¤íƒ€ì¼: {hairstyle}, ìŠ¤íƒ€ì¼: {style}, ë§íˆ¬: {speech}"
        else:
            gender = Gender.NON_BINARY
            age_range = "ì•Œ ìˆ˜ ì—†ìŒ"
            personality = 'ì¹œê·¼í•˜ê³  í™œë°œí•œ ì„±ê²©'
            
        # MBTI ì •ë³´ ì¶”ì¶œ
        if mbti:
            mbti_type = mbti.get('mbti_name')
            if not mbti_type:
                mbti_type = "ì•Œ ìˆ˜ ì—†ìŒ"
            # ì„±ê²©ì— MBTI íŠ¹ì„± ì¶”ê°€
            mbti_traits = mbti.get('mbti_traits', '')
            if mbti_traits:
                personality += f" ({mbti_traits})"
        else:
            mbti_type = None
            
        return CharacterProfile(
            name=name,
            description=description,
            age_range=age_range,
            gender=gender,
            personality=personality,
            mbti=mbti_type
        )
    
    def create_qa_batch_requests(self, character: CharacterProfile, num_requests: int = None, system_prompt: str = None) -> List[Dict]:
        """
        ì¸í”Œë£¨ì–¸ì„œ ìºë¦­í„°ë¥¼ ìœ„í•œ QA ìƒì„± ë°°ì¹˜ ìš”ì²­ ìƒì„±
        ë¡œì»¬ì—ì„œ ì§ì ‘ OpenAI Batch API í˜•ì‹ì˜ JSONLì„ ìƒì„±
        Args:
            character: ìºë¦­í„° í”„ë¡œí•„
            num_requests: ìƒì„±í•  QA ê°œìˆ˜ (Noneì´ë©´ í™˜ê²½ë³€ìˆ˜ QA_GENERATION_COUNT ì‚¬ìš©)
            system_prompt: ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸
        Returns:
            ë°°ì¹˜ ìš”ì²­ ë¦¬ìŠ¤íŠ¸ (OpenAI Batch API í˜•ì‹)
        """
        if num_requests is None:
            num_requests = settings.QA_GENERATION_COUNT
        
        print(f"ë¡œì»¬ì—ì„œ {num_requests}ê°œ QA ìƒì„± ì‹œì‘...")
        
        # ë„ë©”ì¸ ì„¤ì •
        domains = ["ì¼ìƒìƒí™œ", "ê³¼í•™ê¸°ìˆ ", "ì‚¬íšŒì´ìŠˆ", "ì¸ë¬¸í•™", "ìŠ¤í¬ì¸ ", "ì—­ì‚¬ë¬¸í™”"]
        
        # ë„ë©”ì¸ë³„ íŠ¹ì„± ì„¤ëª…
        domain_descriptions = {
            "ì¼ìƒìƒí™œ": "ì¼ìƒì˜ ì†Œì†Œí•œ ì¼ë“¤, ì·¨ë¯¸, ìŠµê´€, ìŒì‹, ì£¼ë§ í™œë™ ë“±",
            "ê³¼í•™ê¸°ìˆ ": "AI, ê¸°ìˆ  íŠ¸ë Œë“œ, ìŠ¤ë§ˆíŠ¸í°, ë¯¸ë˜ ê¸°ìˆ , ê³¼í•™ì˜ ë°œì „",
            "ì‚¬íšŒì´ìŠˆ": "ì‚¬íšŒ ë¬¸ì œ, í™˜ê²½, ë¶ˆí‰ë“±, ì„¸ëŒ€ ê°„ ì°¨ì´, ë¯¸ë˜ ì‚¬íšŒ",
            "ì¸ë¬¸í•™": "ì¸ìƒì˜ ê°€ì¹˜, ì±…, ì˜ˆìˆ , ì² í•™, ì—­ì‚¬ì˜ êµí›ˆ",
            "ìŠ¤í¬ì¸ ": "ìš´ë™, ê±´ê°•ê´€ë¦¬, ìŠ¤í¬ì¸  ê²½ê¸°, ìš´ë™ì˜ ì¦ê±°ì›€",
            "ì—­ì‚¬ë¬¸í™”": "ì „í†µë¬¸í™”, ì—­ì‚¬ì  ì¥ì†Œ, ë¬¸í™”ì˜ ë‹¤ì–‘ì„±, ì—­ì‚¬ ì¸ë¬¼"
        }
        
        # ë„ë©”ì¸ë³„ QA ê°œìˆ˜ ê³„ì‚° (ê· ë“± ë¶„ë°°)
        qa_per_domain = num_requests // len(domains)
        
        batch_requests = []
        
        # ìºë¦­í„° ì •ë³´ ë¬¸ìì—´ë¡œ ë³€í™˜
        gender_str = character.gender.value if hasattr(character.gender, 'value') else str(character.gender) if character.gender else "ì—†ìŒ"
        
        # ë„ë©”ì¸ë³„ë¡œ QA ìƒì„±
        for domain_idx, domain in enumerate(domains):
            current_domain_qa = qa_per_domain
            
            # ë§ˆì§€ë§‰ ë„ë©”ì¸ì—ëŠ” ë‚˜ë¨¸ì§€ QA ëª¨ë‘ í• ë‹¹
            if domain_idx == len(domains) - 1:
                current_domain_qa = num_requests - len(batch_requests)
            
            print(f"ë„ë©”ì¸ '{domain}' QA ìƒì„± ì¤‘: {current_domain_qa}ê°œ")
            
            for i in range(current_domain_qa):
                domain_desc = domain_descriptions.get(domain, domain)
                
                # OpenAI Batch API í˜•ì‹ìœ¼ë¡œ ë³€í™˜
                custom_id = f"influencer_qa_{character.name}_{domain}_{i}"
                batch_request = {
                    "custom_id": custom_id,
                    "method": "POST",
                    "url": "/v1/chat/completions",
                    "body": {
                        "model": "gpt-4o-mini",
                        "messages": [
                            {
                                "role": "system",
                                "content": system_prompt or f"ë‹¹ì‹ ì€ {character.name}ë¼ëŠ” ì¸í”Œë£¨ì–¸ì„œì…ë‹ˆë‹¤. {character.personality} ì„±ê²©ì„ ê°€ì§€ê³  ìˆìŠµë‹ˆë‹¤."
                            },
                            {
                                "role": "user",
                                "content": f"""{domain}({domain_desc})ì— ê´€í•œ QA ìŒì„ í•˜ë‚˜ ë§Œë“¤ì–´ì£¼ì„¸ìš”.
{character.name}ì˜ ì„±ê²©ê³¼ íŠ¹ì„±ì— ë§ëŠ” ìì—°ìŠ¤ëŸ½ê³  í¥ë¯¸ë¡œìš´ ì§ˆë¬¸ì„ ë§Œë“¤ê³ , ê·¸ì— ëŒ€í•´ ìºë¦­í„°ë‹µê²Œ ë‹µë³€í•´ì£¼ì„¸ìš”.
ë°˜ë“œì‹œ JSON í˜•ì‹ìœ¼ë¡œ ë‹µë³€í•´ì£¼ì„¸ìš”:
{{"q": "ì§ˆë¬¸ ë‚´ìš©", "a": "ë‹µë³€ ë‚´ìš©"}}"""
                            }
                        ],
                        "max_tokens": 500,
                        "temperature": 0.8,
                        "response_format": {"type": "json_object"}  # JSON í˜•ì‹ ê°•ì œ
                    }
                }
                
                batch_requests.append(batch_request)
                
                # ì§„í–‰ìƒí™© ë¡œê¹… (100ê°œë§ˆë‹¤)
                if (i + 1) % 100 == 0:
                    print(f"ë„ë©”ì¸ '{domain}' ì§„í–‰: {i + 1}/{current_domain_qa}")
        
        print(f"QA ìƒì„± ì™„ë£Œ: ì´ {len(batch_requests)}ê°œ ë°°ì¹˜ ìš”ì²­ ìƒì„±")
        return batch_requests
    
    # _wait_for_qa_completion ë©”ì„œë“œëŠ” ë” ì´ìƒ í•„ìš”í•˜ì§€ ì•ŠìŒ (ë¡œì»¬ ìƒì„±ìœ¼ë¡œ ë³€ê²½ë¨)

    # í´ë°± QA ìš”ì²­ ìƒì„± ë©”ì„œë“œëŠ” ì œê±°ë¨ - vLLM ì„œë²„ ì‹¤íŒ¨ ì‹œ ì˜ˆì™¸ ë°œìƒ

    def _create_qa_batch_requests_from_results(self, qa_results: List[Dict], character: CharacterProfile, system_prompt: str) -> List[Dict]:
        """
        VLLM ì„œë²„ì—ì„œ ìƒì„±ëœ QA ê²°ê³¼ë¥¼ ë°°ì¹˜ ìš”ì²­ìœ¼ë¡œ ë³€í™˜
        Args:
            qa_results: VLLM ì„œë²„ì—ì„œ ìƒì„±ëœ QA ê²°ê³¼ ë¦¬ìŠ¤íŠ¸
            character: ìºë¦­í„° í”„ë¡œí•„
            system_prompt: ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸
        Returns:
            ë°°ì¹˜ ìš”ì²­ ë¦¬ìŠ¤íŠ¸
        """
        batch_requests = []
        
        for i, qa_result in enumerate(qa_results):
            question = qa_result.get('question', '')
            responses = qa_result.get('responses', {})
            
            # ê° ë§íˆ¬ë³„ ì‘ë‹µì„ ê°œë³„ ë°°ì¹˜ ìš”ì²­ìœ¼ë¡œ ìƒì„±
            for tone_name, tone_responses in responses.items():
                if tone_responses and len(tone_responses) > 0:
                    tone_response = tone_responses[0]  # ì²« ë²ˆì§¸ ì‘ë‹µ ì‚¬ìš©
                    answer_text = tone_response.get('text', '')
                    
                    # ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ì™€ ìºë¦­í„° ì •ë³´ë¥¼ ê²°í•©
                    enhanced_system_prompt = f"{system_prompt}\n\nìºë¦­í„° ì •ë³´:\n- ì´ë¦„: {character.name}\n- ì„±ê²©: {character.personality}\n- ë§íˆ¬: {tone_name}"
                    
                    request = {
                        "custom_id": f"influencer_qa_{character.name}_{i + 1}_{tone_name}",
                        "method": "POST",
                        "url": "/v1/chat/completions",
                        "body": {
                            "model": "gpt-4o-mini",
                            "messages": [
                                {
                                    "role": "system",
                                    "content": enhanced_system_prompt
                                },
                                {
                                    "role": "user",
                                    "content": f"Q: {question}\nA: {answer_text}\n\nìœ„ QA ìŒì„ ê²€í† í•˜ê³  ê°œì„ í•´ì£¼ì„¸ìš”."
                                }
                            ],
                            "max_tokens": 500,
                            "temperature": 0.7
                        }
                    }
                    
                    batch_requests.append(request)
        
        return batch_requests
    
    def save_batch_file(self, requests: List[Dict], task_id: str) -> str:
        """ë°°ì¹˜ ìš”ì²­ì„ JSONL íŒŒì¼ë¡œ ì €ì¥"""
        filename = f"influencer_qa_batch_{task_id}.jsonl"
        # OSì— ë§ëŠ” ì„ì‹œ ë””ë ‰í† ë¦¬ ì‚¬ìš©
        temp_dir = tempfile.gettempdir()
        filepath = os.path.join(temp_dir, filename)
        
        # ë””ë ‰í† ë¦¬ê°€ ì¡´ì¬í•˜ëŠ”ì§€ í™•ì¸í•˜ê³  ìƒì„±
        os.makedirs(temp_dir, exist_ok=True)
        
        with open(filepath, 'w', encoding='utf-8') as f:
            for request in requests:
                f.write(json.dumps(request, ensure_ascii=False) + '\n')
        
        print(f"ë°°ì¹˜ íŒŒì¼ ì €ì¥ ì™„ë£Œ: {filepath}")
        return filepath
    
    def submit_batch_job(self, batch_file_path: str, task_id: str) -> str:
        """OpenAI ë°°ì¹˜ ì‘ì—… ì œì¶œ"""
        print(f"ë°°ì¹˜ íŒŒì¼ ì—…ë¡œë“œ ì¤‘: {batch_file_path}")
        
        # íŒŒì¼ ì—…ë¡œë“œ
        with open(batch_file_path, 'rb') as f:
            batch_input_file = self.client.files.create(
                file=f,
                purpose="batch"
            )
        
        print(f"íŒŒì¼ ì—…ë¡œë“œ ì™„ë£Œ: {batch_input_file.id}")
        
        # ëª¨ë‹ˆí„°ë§ ë°©ì‹ í™•ì¸
        use_webhook = settings.OPENAI_MONITORING_MODE == 'webhook'
        
        batch_create_params = {
            "input_file_id": batch_input_file.id,
            "endpoint": "/v1/chat/completions",
            "completion_window": "24h",
            "metadata": {
                "description": f"Influencer QA pairs generation - Task ID: {task_id}",
                "task_id": task_id
            }
        }
        
        # ì›¹í›… ëª¨ë“œì¼ ë•Œë§Œ ì›¹í›… URL ì¶”ê°€
        if use_webhook:
            batch_create_params["metadata"]["webhook_url"] = settings.OPENAI_WEBHOOK_URL
            print(f"ğŸ¯ ì›¹í›… ëª¨ë“œë¡œ ë°°ì¹˜ ì‘ì—… ìƒì„± ì¤‘... (URL: {settings.OPENAI_WEBHOOK_URL})")
        else:
            print(f"ğŸ”„ í´ë§ ëª¨ë“œë¡œ ë°°ì¹˜ ì‘ì—… ìƒì„± ì¤‘... (ê°„ê²©: {settings.OPENAI_POLLING_INTERVAL_MINUTES}ë¶„)")
        
        # ë°°ì¹˜ ì‘ì—… ìƒì„±
        batch = self.client.batches.create(**batch_create_params)
        
        print(f"ë°°ì¹˜ ì‘ì—… ìƒì„± ì™„ë£Œ: {batch.id}")
        return batch.id
    
    def check_batch_status(self, batch_id: str) -> Dict:
        """ë°°ì¹˜ ì‘ì—… ìƒíƒœ í™•ì¸"""
        batch = self.client.batches.retrieve(batch_id)
        return {
            "id": batch.id,
            "status": batch.status,
            "request_counts": batch.request_counts.__dict__ if batch.request_counts else None,
            "created_at": batch.created_at,
            "completed_at": batch.completed_at,
            "output_file_id": batch.output_file_id if hasattr(batch, 'output_file_id') else None,
            "error_file_id": batch.error_file_id if hasattr(batch, 'error_file_id') else None
        }
    
    def download_batch_results(self, batch_id: str, task_id: str) -> Optional[str]:
        """ë°°ì¹˜ ê²°ê³¼ ë‹¤ìš´ë¡œë“œ"""
        batch = self.client.batches.retrieve(batch_id)
        
        if batch.status != "completed":
            print(f"ë°°ì¹˜ ì‘ì—…ì´ ì•„ì§ ì™„ë£Œë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. í˜„ì¬ ìƒíƒœ: {batch.status}")
            return None
        
        if not batch.output_file_id:
            print("ì¶œë ¥ íŒŒì¼ IDê°€ ì—†ìŠµë‹ˆë‹¤.")
            return None
        
        # ê²°ê³¼ íŒŒì¼ ë‹¤ìš´ë¡œë“œ
        result_file_name = f"influencer_qa_results_{task_id}.jsonl"
        temp_dir = tempfile.gettempdir()
        result_file_path = os.path.join(temp_dir, result_file_name)
        
        file_response = self.client.files.content(batch.output_file_id)
        
        with open(result_file_path, 'wb') as f:
            f.write(file_response.content)
        
        print(f"ê²°ê³¼ íŒŒì¼ ë‹¤ìš´ë¡œë“œ ì™„ë£Œ: {result_file_path}")
        return result_file_path
    
    def process_qa_results(self, result_file_path: str) -> List[Dict]:
        """ê²°ê³¼ íŒŒì¼ì—ì„œ QA ìŒ ì¶”ì¶œ"""
        qa_pairs = []
        
        with open(result_file_path, 'r', encoding='utf-8') as f:
            for line in f:
                result = json.loads(line)
                
                if result.get('response', {}).get('status_code') == 200:
                    # ì‘ë‹µ ì»¨í…ì¸  ê°€ì ¸ì˜¤ê¸°
                    content = result['response']['body']['choices'][0]['message']['content'].strip()
                    
                    try:
                        # JSON í˜•ì‹ìœ¼ë¡œ íŒŒì‹± ì‹œë„ ({"q": "...", "a": "..."} í˜•ì‹)
                        qa_data = json.loads(content)
                        if isinstance(qa_data, dict) and 'q' in qa_data and 'a' in qa_data:
                            qa_pairs.append({
                                "question": qa_data['q'],
                                "answer": qa_data['a'],
                                "custom_id": result.get('custom_id')
                            })
                            continue
                    except json.JSONDecodeError:
                        # JSON íŒŒì‹± ì‹¤íŒ¨ ì‹œ ê¸°ì¡´ ë°©ì‹ìœ¼ë¡œ í´ë°±
                        pass
                    
                    # ê¸°ì¡´ í˜•ì‹ (Q: ... A: ...) ì²˜ë¦¬
                    if 'Q:' in content and 'A:' in content:
                        parts = content.split('A:', 1)
                        if len(parts) == 2:
                            question = parts[0].replace('Q:', '').strip()
                            answer = parts[1].strip()
                            qa_pairs.append({
                                "question": question,
                                "answer": answer,
                                "custom_id": result.get('custom_id')
                            })
                    else:
                        # JSONë„ ì•„ë‹ˆê³  Q:A: í˜•ì‹ë„ ì•„ë‹Œ ê²½ìš°
                        print(f"QA íŒŒì‹± ì‹¤íŒ¨ - ì»¨í…ì¸ : {content[:100]}...")
        
        return qa_pairs
    
    def save_qa_pairs_to_db(self, influencer_id: str, qa_pairs: List[Dict], db: Session):
        """ìƒì„±ëœ QA ìŒì„ ë°ì´í„°ë² ì´ìŠ¤ì— ì €ì¥"""
        # TODO: QA ìŒì„ ì €ì¥í•  í…Œì´ë¸”ì´ í•„ìš” (ì˜ˆ: influencer_qa_pairs)
        # í˜„ì¬ëŠ” JSON íŒŒì¼ë¡œ ì„ì‹œ ì €ì¥
        filename = f"influencer_{influencer_id}_qa_pairs.json"
        temp_dir = tempfile.gettempdir()
        filepath = os.path.join(temp_dir, filename)
        
        with open(filepath, 'w', encoding='utf-8') as f:
            json.dump(qa_pairs, f, ensure_ascii=False, indent=2)
        
        print(f"QA ìŒ {len(qa_pairs)}ê°œê°€ {filepath}ì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")
    
    async def start_qa_generation(self, influencer_id: str, db: Session, user_id: str = None) -> str:
        """
        ì¸í”Œë£¨ì–¸ì„œë¥¼ ìœ„í•œ QA ìƒì„± ì‹œì‘
        Args:
            influencer_id: ì¸í”Œë£¨ì–¸ì„œ ID
            db: ë°ì´í„°ë² ì´ìŠ¤ ì„¸ì…˜
            user_id: ì‚¬ìš©ì ID (ê¶Œí•œ í™•ì¸ìš©)
        Returns:
            ì‘ì—… ID
        """
        print(f"ğŸ¨ QA Generator: start_qa_generation í•¨ìˆ˜ ì‹œì‘ - influencer_id={influencer_id}, user_id={user_id}")
        
        # ì‘ì—… ID ìƒì„±
        task_id = f"qa_{influencer_id}_{int(time.time())}"
        print(f"ğŸ¨ QA Generator: ì‘ì—… ì‹œì‘ - task_id={task_id}, influencer_id={influencer_id}")

        # BatchKey ëª¨ë¸ì„ ì‚¬ìš©í•˜ì—¬ DBì— ì‘ì—… ê¸°ë¡
        import uuid
        batch_key_entry = BatchKey(
            batch_key_id=str(uuid.uuid4()),
            batch_key=task_id,  # batch_key í•„ë“œì— task_id ê°’ ì„¤ì •
            task_id=task_id,
            influencer_id=influencer_id,
            status=QAGenerationStatus.PENDING.value,
            total_qa_pairs=settings.QA_GENERATION_COUNT
        )
        db.add(batch_key_entry)
        
        try:
            db.commit()
            db.refresh(batch_key_entry)

            # ì¸í”Œë£¨ì–¸ì„œ ë°ì´í„° ê°€ì ¸ì˜¤ê¸° (ì‚¬ìš©ì ê¶Œí•œ í™•ì¸)
            if user_id:
                # ì‚¬ìš©ì ê¶Œí•œìœ¼ë¡œ ì¸í”Œë£¨ì–¸ì„œ ì¡°íšŒ
                influencer_data = await get_influencer_by_id(db, user_id, influencer_id)
            else:
                # ë°±ê·¸ë¼ìš´ë“œ ì‘ì—…ì˜ ê²½ìš° ì§ì ‘ ì¡°íšŒ (ê¶Œí•œ ìš°íšŒ)
                from app.models.influencer import AIInfluencer
                influencer_data = db.query(AIInfluencer).filter(AIInfluencer.influencer_id == influencer_id).first()
            
            if not influencer_data:
                raise Exception(f"ì¸í”Œë£¨ì–¸ì„œë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {influencer_id}")

            # ìƒíƒœ ì—…ë°ì´íŠ¸: PROCESSING
            batch_key_entry.status = QAGenerationStatus.PROCESSING.value
            db.commit()
            
            # ì¸í”Œë£¨ì–¸ì„œ â†’ ìºë¦­í„° í”„ë¡œí•„ ë³€í™˜
            character = self.influencer_to_character_profile(
                influencer_data.__dict__,
                influencer_data.style_preset.__dict__ if influencer_data.style_preset else None,
                influencer_data.mbti.__dict__ if influencer_data.mbti else None
            )
            
            # ì €ì¥ëœ ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ ê°€ì ¸ì˜¤ê¸°
            system_prompt = getattr(influencer_data, 'system_prompt', None)
            
            # ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ê°€ ì—†ê³  tone_dataê°€ ìˆìœ¼ë©´ ë¶„ì„í•˜ì—¬ ìƒì„±
            if not system_prompt:
                # influencer_tone í•„ë“œì—ì„œ ëŒ€ì‚¬ ë°ì´í„° ê°€ì ¸ì˜¤ê¸°
                tone_data = getattr(influencer_data, 'influencer_tone', None)
                if tone_data:
                    print("ğŸ” tone_data ë¶„ì„ì„ í†µí•œ ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ ìƒì„± ì‹œì‘")
                    try:
                        # vLLM í´ë¼ì´ì–¸íŠ¸ ì‚¬ìš© ì¤‘ë‹¨ - RunPodë¡œ ëŒ€ì²´
                        print("âš ï¸ vLLM ì„œë²„ê°€ ì•„ë‹Œ RunPod Serverlessë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤")
                        print("ğŸ”„ tone_data ë¶„ì„ ê¸°ëŠ¥ì€ í˜„ì¬ ì§€ì›ë˜ì§€ ì•ŠìŠµë‹ˆë‹¤")
                        
                        # ê¸°ë³¸ ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ ìƒì„±
                        character_name = influencer_data.influencer_name
                        personality = getattr(influencer_data, 'influencer_personality', 'ì¹œê·¼í•˜ê³  í™œë°œí•œ ì„±ê²©')
                        
                        system_prompt = f"""ë‹¹ì‹ ì€ {character_name}ì…ë‹ˆë‹¤. 
{personality}ì„ ê°€ì§€ê³  ìˆìœ¼ë©°, ì‚¬ìš©ìì™€ ì¹œê·¼í•˜ê³  ìì—°ìŠ¤ëŸ½ê²Œ ëŒ€í™”í•©ë‹ˆë‹¤.
ëŒ€í™”í•  ë•ŒëŠ” ìƒëŒ€ë°©ì„ ì¡´ì¤‘í•˜ê³ , ê³µê°í•˜ë©°, ë„ì›€ì´ ë˜ëŠ” ë‹µë³€ì„ í•˜ë ¤ê³  ë…¸ë ¥í•©ë‹ˆë‹¤."""
                        
                        print(f"âœ… ê¸°ë³¸ ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ ìƒì„± ì™„ë£Œ: {system_prompt[:100]}...")
                        
                        # DBì— ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ ì €ì¥
                        influencer_data.system_prompt = system_prompt
                        db.commit()
                        print("ğŸ’¾ ìƒì„±ëœ ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ë¥¼ DBì— ì €ì¥í–ˆìŠµë‹ˆë‹¤")
                        
                    except Exception as e:
                        print(f"âŒ tone_data ë¶„ì„ ì‹¤íŒ¨: {e}")
                        print("âš ï¸ ê¸°ë³¸ í”„ë¡¬í”„íŠ¸ ì‚¬ìš©")
                else:
                    print("âš ï¸ ì €ì¥ëœ ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ì™€ tone_dataê°€ ëª¨ë‘ ì—†ì–´ ê¸°ë³¸ í”„ë¡¬í”„íŠ¸ ì‚¬ìš©")
            else:
                print(f"âœ… ì €ì¥ëœ ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ ì‚¬ìš©: {system_prompt[:100]}...")
            
            # ë°°ì¹˜ ìš”ì²­ ìƒì„± (ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ í¬í•¨)
            batch_requests = self.create_qa_batch_requests(character, system_prompt=system_prompt)
            
            # ë°°ì¹˜ íŒŒì¼ ì €ì¥
            batch_file_path = self.save_batch_file(batch_requests, task_id)
            
            # ë°°ì¹˜ ì‘ì—… ì œì¶œ
            batch_id = self.submit_batch_job(batch_file_path, task_id)
            
            # DBì— ë°°ì¹˜ ì •ë³´ ì—…ë°ì´íŠ¸
            batch_key_entry.openai_batch_id = batch_id
            batch_key_entry.input_file_id = batch_file_path
            batch_key_entry.status = QAGenerationStatus.BATCH_SUBMITTED.value
            db.commit()
            
            print(f"âœ… ë°°ì¹˜ ì‘ì—… DBì— ì €ì¥ ë° ìƒíƒœ ì—…ë°ì´íŠ¸: task_id={task_id}, batch_id={batch_id}")
            
            print(f"ğŸ‰ QA Generator: ì‘ì—… ì™„ë£Œ - Task ID: {task_id}, Batch ID: {batch_id}, QA ê°œìˆ˜: {settings.QA_GENERATION_COUNT}")
            return task_id
            
        except Exception as e:
            error_msg = str(e)
            print(f"âŒ QA Generator: ì‘ì—… ì‹¤íŒ¨ - {error_msg}")
            import traceback
            print(f"ğŸ” QA Generator: ìƒì„¸ ì—ëŸ¬ ì •ë³´ - {traceback.format_exc()}")
            
            # DBì— ì˜¤ë¥˜ ìƒíƒœ ì—…ë°ì´íŠ¸
            db.rollback() # ì˜¤ë¥˜ ë°œìƒ ì‹œ ë¡¤ë°±
            batch_key_entry.status = QAGenerationStatus.FAILED.value
            batch_key_entry.error_message = error_msg
            db.commit()
            
            return task_id
    
    def complete_qa_generation(self, task_id: str, db: Session) -> bool:
        """QA ìƒì„± ì™„ë£Œ ì²˜ë¦¬ - í´ë§/ì›¹í›… ëª¨ë“œ ëª¨ë‘ ì§€ì›"""
        logger = logging.getLogger(__name__)
        logger.info(f"ğŸ”„ QA ìƒì„± ì™„ë£Œ ì²˜ë¦¬ ì‹œì‘: task_id={task_id}")
        
        try:
            # BatchKeyì—ì„œ ë°°ì¹˜ ì •ë³´ ì¡°íšŒ
            batch_key = db.query(BatchKey).filter(BatchKey.task_id == task_id).first()
            if not batch_key:
                logger.error(f"âŒ BatchKeyë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŒ: task_id={task_id}")
                return False
            
            if not batch_key.openai_batch_id:
                logger.error(f"âŒ OpenAI ë°°ì¹˜ IDê°€ ì—†ìŒ: task_id={task_id}")
                return False
            
            logger.info(f"ğŸ“¦ ë°°ì¹˜ ì •ë³´ í™•ì¸: batch_id={batch_key.openai_batch_id}, influencer_id={batch_key.influencer_id}")
            
            # ë°°ì¹˜ ê²°ê³¼ ë‹¤ìš´ë¡œë“œ
            result_file_path = self.download_batch_results(batch_key.openai_batch_id, task_id)
            if not result_file_path:
                raise Exception("ê²°ê³¼ íŒŒì¼ ë‹¤ìš´ë¡œë“œ ì‹¤íŒ¨")
            
            logger.info(f"ğŸ“¥ ë°°ì¹˜ ê²°ê³¼ ë‹¤ìš´ë¡œë“œ ì™„ë£Œ: {result_file_path}")
            
            # QA ìŒ ì²˜ë¦¬
            qa_pairs = self.process_qa_results(result_file_path)
            logger.info(f"ğŸ” QA ìŒ ì²˜ë¦¬ ì™„ë£Œ: {len(qa_pairs)}ê°œ")
            
            # DBì— ì €ì¥
            self.save_qa_pairs_to_db(batch_key.influencer_id, qa_pairs, db)
            logger.info(f"ğŸ’¾ QA ìŒ DB ì €ì¥ ì™„ë£Œ")
            
            # S3ì— ì—…ë¡œë“œ
            logger.info(f"â˜ï¸ S3 ì—…ë¡œë“œ ì‹œì‘: influencer_id={batch_key.influencer_id}, task_id={task_id}")
            try:
                from app.services.s3_service import get_s3_service
                s3_service = get_s3_service()
                
                if s3_service.is_available():
                    # S3ì— QA ê²°ê³¼ ì—…ë¡œë“œ
                    s3_urls = s3_service.upload_qa_results(
                        influencer_id=batch_key.influencer_id,
                        task_id=task_id,
                        qa_pairs=qa_pairs,
                        raw_results_file=result_file_path
                    )
                    
                    # S3 URL ì €ì¥
                    if s3_urls:
                        batch_key.s3_qa_file_url = s3_urls.get('processed_qa_url')
                        batch_key.s3_processed_file_url = s3_urls.get('raw_results_url')
                        batch_key.is_uploaded_to_s3 = True
                        logger.info(f"âœ… S3 ì—…ë¡œë“œ ì„±ê³µ: QA URL={batch_key.s3_qa_file_url}")
                    else:
                        logger.warning(f"âš ï¸ S3 ì—…ë¡œë“œ ì‹¤íŒ¨: URLì´ ë°˜í™˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤")
                        batch_key.is_uploaded_to_s3 = False
                else:
                    logger.warning(f"âš ï¸ S3 ì„œë¹„ìŠ¤ë¥¼ ì‚¬ìš©í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ë¡œì»¬ íŒŒì¼ë§Œ ì‚¬ìš©í•©ë‹ˆë‹¤.")
                    batch_key.is_uploaded_to_s3 = False
                    
            except Exception as s3_error:
                logger.error(f"âŒ S3 ì—…ë¡œë“œ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {s3_error}", exc_info=True)
                # S3 ì—…ë¡œë“œ ì‹¤íŒ¨í•´ë„ ì „ì²´ í”„ë¡œì„¸ìŠ¤ëŠ” ê³„ì† ì§„í–‰
                batch_key.is_uploaded_to_s3 = False
            
            # BatchKey ìƒíƒœ ì—…ë°ì´íŠ¸
            batch_key.status = QAGenerationStatus.COMPLETED.value
            batch_key.generated_qa_pairs = len(qa_pairs)
            batch_key.completed_at = datetime.now()
            batch_key.is_processed = True
            db.commit()
            logger.info(f"ğŸ§  BatchKey ìƒíƒœ ì—…ë°ì´íŠ¸ ì™„ë£Œ (DB)")
            
            logger.info(f"âœ… QA ìƒì„± ì™„ë£Œ - Task ID: {task_id}, QA ìŒ: {len(qa_pairs)}ê°œ, S3 ì—…ë¡œë“œ: {batch_key.is_uploaded_to_s3}")
            return True
            
        except Exception as e:
            logger.error(f"âŒ QA ìƒì„± ì™„ë£Œ ì²˜ë¦¬ ì‹¤íŒ¨: task_id={task_id}, error={e}", exc_info=True)
            
            # DBì— ì˜¤ë¥˜ ìƒíƒœ ì—…ë°ì´íŠ¸
            if batch_key:
                db.rollback() # ì˜¤ë¥˜ ë°œìƒ ì‹œ ë¡¤ë°±
                batch_key.status = QAGenerationStatus.FAILED.value
                batch_key.error_message = f"ê²°ê³¼ ì²˜ë¦¬ ì˜¤ë¥˜: {str(e)}"
                db.commit()
            
            return False