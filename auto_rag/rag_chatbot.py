from fastapi import APIRouter, WebSocket, WebSocketDisconnect, Query, Depends, HTTPException
from pydantic import BaseModel, Field
from sqlalchemy.orm import Session
from typing import Optional, Dict, List
import json
import logging
import base64
import asyncio
from datetime import datetime
from pathlib import Path
import sys
import unicodedata
import re

# backend ê²½ë¡œ ì¶”ê°€
backend_path = Path(__file__).parent.parent
sys.path.append(str(backend_path))

# í…ìŠ¤íŠ¸ ì •ê·œí™” í•¨ìˆ˜
def normalize_text(text: str) -> str:
    """í…ìŠ¤íŠ¸ ì •ê·œí™” - surrogate ë¬¸ì ì œê±° ë° ì •ë¦¬"""
    if not text:
        return ""
    
    try:
        # surrogate ë¬¸ì ì œê±°
        text = text.encode('utf-8', 'ignore').decode('utf-8')
        
        # ë¹„ì •ìƒì ì¸ ìœ ë‹ˆì½”ë“œ ë¬¸ì ì •ë¦¬
        text = unicodedata.normalize('NFC', text)
        
        # ì œì–´ ë¬¸ì ì œê±° (ì¤„ë°”ê¿ˆê³¼ íƒ­ì€ ìœ ì§€)
        text = re.sub(r'[\x00-\x08\x0B\x0C\x0E-\x1F\x7F-\x9F]', '', text)
        
        # ì—°ì†ëœ ê³µë°± ì •ë¦¬
        text = re.sub(r'\s+', ' ', text).strip()
        
        return text
    except Exception as e:
        logging.getLogger(__name__).warning(f"í…ìŠ¤íŠ¸ ì •ê·œí™” ì‹¤íŒ¨: {e}")
        return str(text).encode('ascii', 'ignore').decode('ascii')

# ê¸°ì¡´ imports (chatbot.py ê¸°ë°˜)
from app.database import get_db
from app.models.user import HFTokenManage
from app.core.encryption import decrypt_sensitive_data
from app.core.config import settings

# ìˆ˜ì •: backendì˜ VLLM í´ë¼ì´ì–¸íŠ¸ import
from app.services.vllm_client import (
    VLLMServerConfig as VLLMConfig,  # ì´ë¦„ í˜¸í™˜ì„±ì„ ìœ„í•œ alias
    vllm_health_check
)

# RAG ê´€ë ¨ imports (ìš°ë¦¬ê°€ ë§Œë“  ëª¨ë“ˆë“¤)
from chatbot_pipeline import RAGChatbotPipeline, PipelineConfig
from embed_store import EmbeddingStore, EmbeddingConfig, MilvusConfig
from rag_search import RAGSearcher, SearchConfig

router = APIRouter()
logger = logging.getLogger(__name__)


class DocumentUploadRequest(BaseModel):
    """ë¬¸ì„œ ì—…ë¡œë“œ ìš”ì²­ ëª¨ë¸"""
    group_id: int = Field(..., description="ê·¸ë£¹ ID")
    pdf_path: str = Field(..., description="PDF íŒŒì¼ ê²½ë¡œ")
    lora_adapter: Optional[str] = Field("khj0816/EXAONE-Ian", description="LoRA ì–´ëŒ‘í„° ì´ë¦„")
    system_message: Optional[str] = Field("ë‹¹ì‹ ì€ ë„ì›€ì´ ë˜ëŠ” AI ì–´ì‹œìŠ¤í„´íŠ¸ì…ë‹ˆë‹¤.", description="ì‹œìŠ¤í…œ ë©”ì‹œì§€")
    influencer_name: Optional[str] = Field("AI", description="AI ìºë¦­í„° ì´ë¦„")
    temperature: Optional[float] = Field(0.8, description="ìƒì„± ì˜¨ë„", ge=0.1, le=2.0)


class RAGChatRequest(BaseModel):
    """RAG ì±„íŒ… ìš”ì²­ ëª¨ë¸"""
    query: str = Field(..., description="ì‚¬ìš©ì ì§ˆë¬¸")
    group_id: int = Field(..., description="ê·¸ë£¹ ID")
    include_sources: Optional[bool] = Field(True, description="ì¶œì²˜ ì •ë³´ í¬í•¨ ì—¬ë¶€")
    max_context_length: Optional[int] = Field(2000, description="ìµœëŒ€ ì»¨í…ìŠ¤íŠ¸ ê¸¸ì´")


class RAGChatbotManager:
    """RAG ì±—ë´‡ ê´€ë¦¬ì í´ë˜ìŠ¤"""
    
    def __init__(self):
        self._pipelines: Dict[int, RAGChatbotPipeline] = {}  # group_idë³„ íŒŒì´í”„ë¼ì¸
        self._base_output_dir = "./rag_data"
        Path(self._base_output_dir).mkdir(exist_ok=True)
    
    def _get_group_output_dir(self, group_id: int) -> str:
        """ê·¸ë£¹ë³„ ì¶œë ¥ ë””ë ‰í† ë¦¬ ìƒì„±"""
        group_dir = Path(self._base_output_dir) / f"group_{group_id}"
        group_dir.mkdir(exist_ok=True)
        return str(group_dir)
    
    async def create_pipeline(self, 
                            group_id: int, 
                            pdf_path: str, 
                            lora_adapter: str,
                            hf_token: str,
                            system_message: str = (
                                "ë‹¹ì‹ ì€ ì œê³µëœ ì°¸ê³  ë¬¸ì„œì˜ ì •í™•í•œ ì •ë³´ì™€ ì‚¬ì‹¤ì„ ë°”íƒ•ìœ¼ë¡œ ë‹µë³€í•˜ëŠ” AI ì–´ì‹œìŠ¤í„´íŠ¸ì…ë‹ˆë‹¤. "
                                "**ì¤‘ìš”**: ë¬¸ì„œì— í¬í•¨ëœ ëª¨ë“  ë‚´ìš©ì€ ì ˆëŒ€ ìš”ì•½í•˜ê±°ë‚˜ ìƒëµí•˜ì§€ ë§ê³ , ì›ë¬¸ ê·¸ëŒ€ë¡œ ì™„ì „íˆ í¬í•¨í•´ì•¼ í•©ë‹ˆë‹¤. "
                                "ì‚¬ì‹¤, ìˆ˜ì¹˜, ë‚ ì§œ, ì •ì±… ë‚´ìš©, ì„¸ë¶€ ì‚¬í•­ ë“± ëª¨ë“  ì •ë³´ë¥¼ ì •í™•íˆ ê·¸ëŒ€ë¡œ ìœ ì§€í•´ì£¼ì„¸ìš”. "
                                "ë¬¸ì„œ ë‚´ìš©ì˜ ì™„ì „ì„±ê³¼ ì •í™•ì„±ì´ ìµœìš°ì„ ì´ë©°, ë§íˆ¬ì™€ í‘œí˜„ ë°©ì‹ë§Œ ìºë¦­í„° ìŠ¤íƒ€ì¼ë¡œ ì¡°ì •í•´ì£¼ì„¸ìš”. "
                                "ë¬¸ì„œ ë‚´ìš©ì„ ì„ì˜ë¡œ ë³€ê²½, ìš”ì•½, ì¶”ê°€í•˜ì§€ ë§ê³ , ì˜¤ì§ ì œê³µëœ ì •ë³´ë¥¼ ì™„ì „íˆ ê·¸ëŒ€ë¡œ ì‚¬ìš©í•´ ë‹µë³€í•´ì£¼ì„¸ìš”. "
                                "\n\n**ìºë¦­í„° ì •ì²´ì„±**: ë‹¹ì‹ ì€ {influencer_name} ìºë¦­í„°ì…ë‹ˆë‹¤. "
                                "ìê¸°ì†Œê°œë¥¼ í•  ë•Œë‚˜ 'ë„ˆ ëˆ„êµ¬ì•¼?', 'ë‹¹ì‹ ì€ ëˆ„êµ¬ì¸ê°€ìš”?', 'ì´ë¦„ì´ ë­ì•¼?' ê°™ì€ ì§ˆë¬¸ì„ ë°›ìœ¼ë©´ "
                                "ë°˜ë“œì‹œ 'ë‚˜ëŠ” {influencer_name}ì´ì•¼!' ë˜ëŠ” 'ì €ëŠ” {influencer_name}ì…ë‹ˆë‹¤!'ë¼ê³  ë‹µë³€í•´ì•¼ í•©ë‹ˆë‹¤. "
                                "í•­ìƒ {influencer_name}ì˜ ì •ì²´ì„±ì„ ìœ ì§€í•˜ë©° ê·¸ ìºë¦­í„°ë‹µê²Œ í–‰ë™í•˜ì„¸ìš”."
                            ),
                            influencer_name: str = "AI",
                            temperature: float = 0.8) -> bool:
        """ê·¸ë£¹ë³„ RAG íŒŒì´í”„ë¼ì¸ ìƒì„±"""
        try:
            # ê¸°ì¡´ íŒŒì´í”„ë¼ì¸ì´ ìˆìœ¼ë©´ ì •ë¦¬
            if group_id in self._pipelines:
                await self.cleanup_pipeline(group_id)
            
            # ì…ë ¥ í…ìŠ¤íŠ¸ ì •ê·œí™”
            system_message = normalize_text(system_message)
            influencer_name = normalize_text(influencer_name)
            
            # ì„¤ì • ìƒì„±
            config = PipelineConfig(
                pdf_path=pdf_path,
                output_dir=self._get_group_output_dir(group_id),
                use_vllm=True,
                vllm_base_url=os.getenv("VLLM_BASE_URL", "http://localhost:8000"),
                system_message=system_message,
                influencer_name=influencer_name,
                response_temperature=temperature
            )
            
            # íŒŒì´í”„ë¼ì¸ ìƒì„±
            pipeline = RAGChatbotPipeline(config, lora_adapter, hf_token)
            
            # ë¬¸ì„œ ì¸ì œìŠ¤íŠ¸
            success = pipeline.ingest_document(pdf_path)
            
            if success:
                self._pipelines[group_id] = pipeline
                logger.info(f"âœ… RAG íŒŒì´í”„ë¼ì¸ ìƒì„± ì„±ê³µ: group_id={group_id}")
                return True
            else:
                logger.error(f"âŒ RAG íŒŒì´í”„ë¼ì¸ ìƒì„± ì‹¤íŒ¨: group_id={group_id}")
                return False
                
        except Exception as e:
            logger.error(f"âŒ RAG íŒŒì´í”„ë¼ì¸ ìƒì„± ì¤‘ ì˜¤ë¥˜: group_id={group_id}, error={e}")
            return False
    
    def get_pipeline(self, group_id: int) -> Optional[RAGChatbotPipeline]:
        """ê·¸ë£¹ì˜ íŒŒì´í”„ë¼ì¸ ê°€ì ¸ì˜¤ê¸°"""
        return self._pipelines.get(group_id)
    
    async def cleanup_pipeline(self, group_id: int):
        """íŠ¹ì • ê·¸ë£¹ì˜ íŒŒì´í”„ë¼ì¸ ì •ë¦¬"""
        if group_id in self._pipelines:
            try:
                pipeline = self._pipelines[group_id]
                if pipeline.chatbot_engine and pipeline.chatbot_engine.chat_generator:
                    if hasattr(pipeline.chatbot_engine.chat_generator, 'cleanup'):
                        pipeline.chatbot_engine.chat_generator.cleanup()
                del self._pipelines[group_id]
                logger.info(f"âœ… RAG íŒŒì´í”„ë¼ì¸ ì •ë¦¬ ì™„ë£Œ: group_id={group_id}")
            except Exception as e:
                logger.error(f"âŒ RAG íŒŒì´í”„ë¼ì¸ ì •ë¦¬ ì¤‘ ì˜¤ë¥˜: group_id={group_id}, error={e}")
    
    async def cleanup_all(self):
        """ëª¨ë“  íŒŒì´í”„ë¼ì¸ ì •ë¦¬"""
        group_ids = list(self._pipelines.keys())
        for group_id in group_ids:
            await self.cleanup_pipeline(group_id)


# ì „ì—­ RAG ì±—ë´‡ ê´€ë¦¬ì
_rag_manager = RAGChatbotManager()


async def _get_hf_token_by_group(group_id: int, db: Session) -> Optional[str]:
    """ê·¸ë£¹ IDë¡œ HF í† í° ê°€ì ¸ì˜¤ê¸°"""
    try:
        hf_token_manage = db.query(HFTokenManage).filter(
            HFTokenManage.group_id == group_id
        ).order_by(HFTokenManage.created_at.desc()).first()
        
        if hf_token_manage:
            return decrypt_sensitive_data(str(hf_token_manage.hf_token_value))
        else:
            logger.warning(f"ê·¸ë£¹ {group_id}ì— ë“±ë¡ëœ HF í† í°ì´ ì—†ìŠµë‹ˆë‹¤.")
            return None
            
    except Exception as e:
        logger.error(f"HF í† í° ì¡°íšŒ ì‹¤íŒ¨: {e}")
        return None


async def _check_vllm_server_with_retry(max_retries: int = 3) -> bool:
    """ì¬ì‹œë„ ë¡œì§ì´ í¬í•¨ëœ VLLM ì„œë²„ ìƒíƒœ í™•ì¸"""
    retry_delay = 2
    
    for attempt in range(max_retries):
        try:
            is_healthy = await vllm_health_check()
            if is_healthy:
                return True
            else:
                logger.warning(f"âš ï¸ VLLM ì„œë²„ ì‘ë‹µ ì—†ìŒ (ì‹œë„ {attempt + 1}/{max_retries})")
                if attempt < max_retries - 1:
                    logger.info(f"ğŸ”„ {retry_delay}ì´ˆ í›„ ì¬ì‹œë„...")
                    await asyncio.sleep(retry_delay)
                    retry_delay *= 1.5
                    
        except Exception as e:
            logger.error(f"âŒ VLLM ì„œë²„ í™•ì¸ ì‹¤íŒ¨ (ì‹œë„ {attempt + 1}/{max_retries}): {e}")
            if attempt < max_retries - 1:
                logger.info(f"ğŸ”„ {retry_delay}ì´ˆ í›„ ì¬ì‹œë„...")
                await asyncio.sleep(retry_delay)
                retry_delay *= 1.5
    
    return False


@router.post("/rag/upload_document")
async def upload_document(req: DocumentUploadRequest, db: Session = Depends(get_db)):
    """RAGìš© ë¬¸ì„œ ì—…ë¡œë“œ ë° íŒŒì´í”„ë¼ì¸ ìƒì„±"""
    try:
        # HF í† í° ê°€ì ¸ì˜¤ê¸°
        hf_token = await _get_hf_token_by_group(req.group_id, db)
        if not hf_token:
            raise HTTPException(status_code=400, detail="HF í† í°ì´ ì—†ìŠµë‹ˆë‹¤.")
        
        # VLLM ì„œë²„ ìƒíƒœ í™•ì¸ (ì¬ì‹œë„ ë¡œì§ í¬í•¨)
        if not await _check_vllm_server_with_retry():
            logger.warning("âš ï¸ VLLM ì„œë²„ê°€ ì‘ë‹µí•˜ì§€ ì•Šì§€ë§Œ ë¬¸ì„œ ì—…ë¡œë“œë¥¼ ê³„ì† ì§„í–‰í•©ë‹ˆë‹¤.")
            # ì„œë²„ê°€ ì‘ë‹µí•˜ì§€ ì•Šì•„ë„ ê³„ì† ì§„í–‰ (ë¬¸ì„œ ì²˜ë¦¬ëŠ” ê°€ëŠ¥)
        
        # PDF íŒŒì¼ ì¡´ì¬ í™•ì¸
        if not Path(req.pdf_path).exists():
            raise HTTPException(status_code=404, detail=f"PDF íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {req.pdf_path}")
        
        # ì…ë ¥ í…ìŠ¤íŠ¸ ì •ê·œí™”
        system_message = normalize_text(req.system_message or "ë‹¹ì‹ ì€ ë„ì›€ì´ ë˜ëŠ” AI ì–´ì‹œìŠ¤í„´íŠ¸ì…ë‹ˆë‹¤.")
        influencer_name = normalize_text(req.influencer_name or "AI")
        
        # RAG íŒŒì´í”„ë¼ì¸ ìƒì„±
        success = await _rag_manager.create_pipeline(
            group_id=req.group_id,
            pdf_path=req.pdf_path,
            lora_adapter=req.lora_adapter,
            hf_token=hf_token,
            system_message=system_message,
            influencer_name=influencer_name,
            temperature=req.temperature
        )
        
        if success:
            return {
                "success": True,
                "message": "RAG íŒŒì´í”„ë¼ì¸ì´ ì„±ê³µì ìœ¼ë¡œ ìƒì„±ë˜ì—ˆìŠµë‹ˆë‹¤.",
                "group_id": req.group_id,
                "pdf_path": req.pdf_path,
                "lora_adapter": req.lora_adapter,
                "system_message": system_message,
                "influencer_name": influencer_name
            }
        else:
            raise HTTPException(status_code=500, detail="RAG íŒŒì´í”„ë¼ì¸ ìƒì„± ì‹¤íŒ¨")
            
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"[RAG UPLOAD] ë¬¸ì„œ ì—…ë¡œë“œ ì‹¤íŒ¨: {e}")
        raise HTTPException(status_code=500, detail=str(e))


@router.post("/rag/chat")
async def rag_chat(req: RAGChatRequest, db: Session = Depends(get_db)):
    """RAG ê¸°ë°˜ ì±„íŒ… (ë¹„ìŠ¤íŠ¸ë¦¬ë°)"""
    try:
        # íŒŒì´í”„ë¼ì¸ ê°€ì ¸ì˜¤ê¸°
        pipeline = _rag_manager.get_pipeline(req.group_id)
        if not pipeline:
            raise HTTPException(
                status_code=404, 
                detail=f"ê·¸ë£¹ {req.group_id}ì— ëŒ€í•œ RAG íŒŒì´í”„ë¼ì¸ì´ ì—†ìŠµë‹ˆë‹¤. ë¨¼ì € ë¬¸ì„œë¥¼ ì—…ë¡œë“œí•´ì£¼ì„¸ìš”."
            )
        
        # ì…ë ¥ ì§ˆì˜ ì •ê·œí™”
        normalized_query = normalize_text(req.query)
        
        # RAG ì±„íŒ… ì‹¤í–‰
        result = pipeline.chat(normalized_query)
        
        # ì‘ë‹µ í¬ë§·íŒ…
        response_data = {
            "query": result["query"],
            "response": result["response"],
            "timestamp": result["timestamp"],
            "model_info": result.get("model_info", {})
        }
        
        # ì¶œì²˜ ì •ë³´ í¬í•¨ (ìš”ì²­ì‹œ)
        if req.include_sources and result.get("sources"):
            response_data["sources"] = result["sources"]
            context_preview = result.get("context", "")
            if context_preview:
                # ì»¨í…ìŠ¤íŠ¸ë„ ì •ê·œí™”í•˜ê³  ë¯¸ë¦¬ë³´ê¸° ìƒì„±
                normalized_context = normalize_text(context_preview)
                response_data["context_preview"] = normalized_context[:200] + "..." if len(normalized_context) > 200 else normalized_context
        
        return response_data
        
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"[RAG CHAT] ì±„íŒ… ì‹¤íŒ¨: {e}")
        raise HTTPException(status_code=500, detail=str(e))


@router.websocket("/rag/chat/{group_id}")
async def rag_chat_websocket(websocket: WebSocket, group_id: int, 
                           lora_repo: str = Query(...), 
                           influencer_id: str = Query(None), 
                           db: Session = Depends(get_db)):
    """RAG ê¸°ë°˜ ì›¹ì†Œì¼“ ì±„íŒ…"""
    # lora_repo ë””ì½”ë”©
    try:
        lora_repo_decoded = base64.b64decode(lora_repo).decode()
    except Exception as e:
        await websocket.accept()
        await websocket.send_text(json.dumps({
            "error_code": "LORA_REPO_DECODE_ERROR", 
            "message": f"lora_repo ë””ì½”ë”© ì‹¤íŒ¨: {e}"
        }))
        await websocket.close()
        return
    
    await websocket.accept()
    
    try:
        # VLLM ì„œë²„ ìƒíƒœ í™•ì¸ (ì¬ì‹œë„ ë¡œì§ í¬í•¨)
        if not await _check_vllm_server_with_retry():
            logger.warning(f"[RAG WS] VLLM ì„œë²„ ì—°ê²° ë¶ˆì•ˆì •í•˜ì§€ë§Œ ê³„ì† ì§„í–‰")
            await websocket.send_text(json.dumps({
                "type": "warning",
                "message": "VLLM ì„œë²„ ì—°ê²°ì´ ë¶ˆì•ˆì •í•©ë‹ˆë‹¤. ì‘ë‹µì´ ì§€ì—°ë  ìˆ˜ ìˆìŠµë‹ˆë‹¤."
            }))
        
        # RAG íŒŒì´í”„ë¼ì¸ í™•ì¸
        pipeline = _rag_manager.get_pipeline(group_id)
        if not pipeline:
            await websocket.send_text(json.dumps({
                "error_code": "RAG_PIPELINE_NOT_FOUND",
                "message": f"ê·¸ë£¹ {group_id}ì— ëŒ€í•œ RAG íŒŒì´í”„ë¼ì¸ì´ ì—†ìŠµë‹ˆë‹¤. ë¨¼ì € ë¬¸ì„œë¥¼ ì—…ë¡œë“œí•´ì£¼ì„¸ìš”."
            }))
            await websocket.close()
            return
        
        # ì¸í”Œë£¨ì–¸ì„œ ì •ë³´ ê°€ì ¸ì˜¤ê¸° (í•„ìš”ì‹œ)
        system_prompt = "ë‹¹ì‹ ì€ ë„ì›€ì´ ë˜ëŠ” AI ì–´ì‹œìŠ¤í„´íŠ¸ì…ë‹ˆë‹¤."
        if influencer_id:
            try:
                from app.models.influencer import AIInfluencer
                influencer = db.query(AIInfluencer).filter(
                    AIInfluencer.influencer_id == influencer_id
                ).first()
                if influencer and influencer.system_prompt:
                    system_prompt = normalize_text(str(influencer.system_prompt))
                    logger.info(f"[RAG WS] ì¸í”Œë£¨ì–¸ì„œ ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ ì ìš©: {influencer.influencer_name}")
            except Exception as e:
                logger.warning(f"[RAG WS] ì¸í”Œë£¨ì–¸ì„œ ì •ë³´ ê°€ì ¸ì˜¤ê¸° ì‹¤íŒ¨: {e}")
        
        logger.info(f"[RAG WS] RAG ì›¹ì†Œì¼“ ì—°ê²° ì‹œì‘: group_id={group_id}, lora_repo={lora_repo_decoded}")
        
        # ì›¹ì†Œì¼“ ë©”ì‹œì§€ ì²˜ë¦¬ ë£¨í”„
        while True:
            try:
                # ì‚¬ìš©ì ë©”ì‹œì§€ ìˆ˜ì‹ 
                data = await websocket.receive_text()
                logger.info(f"[RAG WS] ë©”ì‹œì§€ ìˆ˜ì‹ : {data[:100]}...")
                
                # ì…ë ¥ ë©”ì‹œì§€ ì •ê·œí™”
                normalized_data = normalize_text(data)
                
                # RAG ê¸°ë°˜ ì‘ë‹µ ìƒì„±
                try:
                    # RAG ê²€ìƒ‰ ìˆ˜í–‰
                    result = pipeline.chat(normalized_data)
                    
                    # ìŠ¤íŠ¸ë¦¬ë° í˜•íƒœë¡œ ì‘ë‹µ ì „ì†¡
                    response = result["response"]
                    context = result.get("context", "")
                    sources = result.get("sources", [])
                    
                    # ì»¨í…ìŠ¤íŠ¸ ì •ë³´ ë¨¼ì € ì „ì†¡
                    if sources:
                        await websocket.send_text(json.dumps({
                            "type": "sources",
                            "content": sources[:3]  # ìƒìœ„ 3ê°œ ì†ŒìŠ¤ë§Œ
                        }))
                    
                    # ì‘ë‹µì„ í† í° ë‹¨ìœ„ë¡œ ë¶„í• í•´ì„œ ì „ì†¡ (ìŠ¤íŠ¸ë¦¬ë° ì‹œë®¬ë ˆì´ì…˜)
                    if response:
                        words = response.split()
                        for i, word in enumerate(words):
                            # ê° ë‹¨ì–´ë„ ì •ê·œí™”
                            normalized_word = normalize_text(word)
                            if normalized_word:  # ë¹ˆ ë¬¸ìì—´ì´ ì•„ë‹Œ ê²½ìš°ë§Œ ì „ì†¡
                                await websocket.send_text(json.dumps({
                                    "type": "token",
                                    "content": normalized_word + " "
                                }))
                                await asyncio.sleep(0.05)  # ìŠ¤íŠ¸ë¦¬ë° íš¨ê³¼
                    
                    # ì™„ë£Œ ì‹ í˜¸
                    await websocket.send_text(json.dumps({
                        "type": "complete",
                        "content": "",
                        "metadata": {
                            "sources_count": len(sources),
                            "context_length": len(normalize_text(context))
                        }
                    }))
                    
                    logger.info(f"[RAG WS] RAG ì‘ë‹µ ì „ì†¡ ì™„ë£Œ")
                    
                except Exception as e:
                    logger.error(f"[RAG WS] RAG ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜: {e}")
                    await websocket.send_text(json.dumps({
                        "type": "error",
                        "error_code": "RAG_PROCESSING_ERROR", 
                        "message": f"RAG ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜: {str(e)}"
                    }))
                    
            except WebSocketDisconnect:
                logger.info(f"[RAG WS] ì›¹ì†Œì¼“ ì—°ê²° ì¢…ë£Œ: group_id={group_id}")
                break
            except Exception as e:
                logger.error(f"[RAG WS] ì›¹ì†Œì¼“ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜: {e}")
                try:
                    await websocket.send_text(json.dumps({
                        "error_code": "WEBSOCKET_ERROR", 
                        "message": str(e)
                    }))
                except:
                    pass
                break
                
    except Exception as e:
        logger.error(f"[RAG WS] ì›¹ì†Œì¼“ ì—°ê²° ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜: {e}")
        try:
            await websocket.send_text(json.dumps({
                "error_code": "CONNECTION_ERROR", 
                "message": str(e)
            }))
        except:
            pass


@router.get("/rag/status/{group_id}")
async def get_rag_status(group_id: int):
    """RAG íŒŒì´í”„ë¼ì¸ ìƒíƒœ í™•ì¸"""
    try:
        pipeline = _rag_manager.get_pipeline(group_id)
        
        if not pipeline:
            return {
                "group_id": group_id,
                "status": "not_found",
                "message": "RAG íŒŒì´í”„ë¼ì¸ì´ ì—†ìŠµë‹ˆë‹¤."
            }
        
        # íŒŒì´í”„ë¼ì¸ ìƒíƒœ ì •ë³´
        status_info = {
            "group_id": group_id,
            "status": "active",
            "config": {
                "use_vllm": pipeline.config.use_vllm,
                "vllm_base_url": pipeline.config.vllm_base_url,
                "output_dir": pipeline.config.output_dir,
                "search_top_k": pipeline.config.search_top_k,
                "response_temperature": pipeline.config.response_temperature,
                "system_message": normalize_text(pipeline.config.system_message),
                "influencer_name": normalize_text(pipeline.config.influencer_name)
            },
            "lora_adapter": pipeline.lora_adapter,
            "document_processed": pipeline.chatbot_engine is not None
        }
        
        # ëª¨ë¸ ì •ë³´ (ê°€ëŠ¥í•œ ê²½ìš°)
        if (pipeline.chatbot_engine and 
            pipeline.chatbot_engine.chat_generator and 
            pipeline.chatbot_engine._generator_initialized):
            try:
                if hasattr(pipeline.chatbot_engine.chat_generator, 'get_model_info'):
                    model_info = pipeline.chatbot_engine.chat_generator.get_model_info()
                    status_info["model_info"] = model_info
            except Exception as e:
                logger.warning(f"ëª¨ë¸ ì •ë³´ ê°€ì ¸ì˜¤ê¸° ì‹¤íŒ¨: {e}")
        
        return status_info
        
    except Exception as e:
        logger.error(f"[RAG STATUS] ìƒíƒœ í™•ì¸ ì‹¤íŒ¨: {e}")
        raise HTTPException(status_code=500, detail=str(e))


@router.delete("/rag/cleanup/{group_id}")
async def cleanup_rag_pipeline(group_id: int):
    """íŠ¹ì • ê·¸ë£¹ì˜ RAG íŒŒì´í”„ë¼ì¸ ì •ë¦¬"""
    try:
        pipeline = _rag_manager.get_pipeline(group_id)
        
        if not pipeline:
            raise HTTPException(status_code=404, detail="RAG íŒŒì´í”„ë¼ì¸ì´ ì—†ìŠµë‹ˆë‹¤.")
        
        await _rag_manager.cleanup_pipeline(group_id)
        
        return {
            "success": True,
            "message": f"ê·¸ë£¹ {group_id}ì˜ RAG íŒŒì´í”„ë¼ì¸ì´ ì •ë¦¬ë˜ì—ˆìŠµë‹ˆë‹¤.",
            "group_id": group_id
        }
        
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"[RAG CLEANUP] íŒŒì´í”„ë¼ì¸ ì •ë¦¬ ì‹¤íŒ¨: {e}")
        raise HTTPException(status_code=500, detail=str(e))


@router.get("/rag/health")
async def check_rag_health():
    """RAG ì„œë¹„ìŠ¤ ì „ì²´ ìƒíƒœ í™•ì¸"""
    try:
        # VLLM ì„œë²„ ìƒíƒœ í™•ì¸
        vllm_status = await _check_vllm_server_with_retry(max_retries=1)
        
        # í™œì„± íŒŒì´í”„ë¼ì¸ ìˆ˜
        active_pipelines = len(_rag_manager._pipelines)
        
        return {
            "status": "healthy",
            "vllm_server": "connected" if vllm_status else "disconnected",
            "active_pipelines": active_pipelines,
            "pipeline_groups": list(_rag_manager._pipelines.keys()),
            "timestamp": datetime.now().isoformat()
        }
        
    except Exception as e:
        logger.error(f"[RAG HEALTH] ìƒíƒœ í™•ì¸ ì‹¤íŒ¨: {e}")
        return {
            "status": "unhealthy",
            "error": str(e),
            "timestamp": datetime.now().isoformat()
        }


# ì• í”Œë¦¬ì¼€ì´ì…˜ ì¢…ë£Œì‹œ ì •ë¦¬
async def cleanup_on_shutdown():
    """ì• í”Œë¦¬ì¼€ì´ì…˜ ì¢…ë£Œì‹œ ëª¨ë“  RAG íŒŒì´í”„ë¼ì¸ ì •ë¦¬"""
    try:
        await _rag_manager.cleanup_all()
        logger.info("âœ… ëª¨ë“  RAG íŒŒì´í”„ë¼ì¸ ì •ë¦¬ ì™„ë£Œ")
    except Exception as e:
        logger.error(f"âŒ RAG íŒŒì´í”„ë¼ì¸ ì •ë¦¬ ì¤‘ ì˜¤ë¥˜: {e}")


# ì‚¬ìš© ì˜ˆì‹œ ë° í…ŒìŠ¤íŠ¸ í•¨ìˆ˜
if __name__ == "__main__":
    import uvicorn
    from fastapi import FastAPI
    
    app = FastAPI(title="RAG Chatbot API")
    app.include_router(router, prefix="/api/v1")
    
    @app.on_event("shutdown")
    async def shutdown_event():
        await cleanup_on_shutdown()
    
    # í—¬ìŠ¤ì²´í¬ ì—”ë“œí¬ì¸íŠ¸ ì¶”ê°€
    @app.get("/health")
    async def health_check():
        return {"status": "healthy", "service": "RAG Chatbot API"}
    
    uvicorn.run(app, host="0.0.0.0", port=8000)