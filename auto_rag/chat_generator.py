import os
import asyncio
from typing import Optional, Dict, Any, List, Union
from dataclasses import dataclass
import logging
import unicodedata
import re
import time
import json
from abc import ABC, abstractmethod
from dotenv import load_dotenv
load_dotenv()

# ìˆ˜ì •: backendì˜ VLLM í´ë¼ì´ì–¸íŠ¸ ì„í¬íŠ¸
import sys
from pathlib import Path

# backend ê²½ë¡œ ì¶”ê°€
backend_path = Path(__file__).parent.parent
sys.path.append(str(backend_path))

from app.services.vllm_client import (
    VLLMClient, 
    VLLMServerConfig as VLLMConfig,
    get_vllm_client,
    vllm_health_check
)

# ë¡œê¹… ì„¤ì •
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)


# Single Responsibility Principle: í…ìŠ¤íŠ¸ ì •ê·œí™” ì „ìš© í´ë˜ìŠ¤
class TextNormalizer:
    """í…ìŠ¤íŠ¸ ì •ê·œí™” ë‹´ë‹¹ í´ë˜ìŠ¤"""
    
    @staticmethod
    def normalize(text: str) -> str:
        """í…ìŠ¤íŠ¸ ì •ê·œí™” - surrogate ë¬¸ì ì œê±° ë° ì •ë¦¬"""
        if not text:
            return ""
        
        try:
            # surrogate ë¬¸ì ì œê±°
            text = text.encode('utf-8', 'ignore').decode('utf-8')
            
            # ë¹„ì •ìƒì ì¸ ìœ ë‹ˆì½”ë“œ ë¬¸ì ì •ë¦¬
            text = unicodedata.normalize('NFC', text)
            
            # ì œì–´ ë¬¸ì ì œê±° (ì¤„ë°”ê¿ˆê³¼ íƒ­ì€ ìœ ì§€)
            text = re.sub(r'[\x00-\x08\x0B\x0C\x0E-\x1F\x7F-\x9F]', '', text)
            
            # ì—°ì†ëœ ê³µë°± ì •ë¦¬
            text = re.sub(r'\s+', ' ', text).strip()
            
            return text
        except Exception as e:
            logger.warning(f"í…ìŠ¤íŠ¸ ì •ê·œí™” ì‹¤íŒ¨: {e}")
            return str(text).encode('ascii', 'ignore').decode('ascii')


# Single Responsibility Principle: ëª¨ë¸ ID ê²€ì¦ ì „ìš© í´ë˜ìŠ¤
class ModelIdValidator:
    """ëª¨ë¸ ID ê²€ì¦ ë‹´ë‹¹ í´ë˜ìŠ¤"""
    
    @staticmethod
    def validate_model_id(model_id: Optional[str]) -> Optional[str]:
        """ëª¨ë¸ ID ê²€ì¦ ë° ì •ë¦¬"""
        if not model_id or not model_id.strip():
            return None
        
        normalized_id = TextNormalizer.normalize(model_id)
        
        # ìµœì†Œ ê¸¸ì´ ê²€ì¦
        if len(normalized_id) < 3:
            logger.warning(f"ëª¨ë¸ IDê°€ ë„ˆë¬´ ì§§ìŠµë‹ˆë‹¤: {normalized_id}")
            return None
        
        # í—ˆìš©ë˜ì§€ ì•ŠëŠ” ë¬¸ì ê²€ì¦
        if re.search(r'[<>"|?*]', normalized_id):
            logger.warning(f"ëª¨ë¸ IDì— í—ˆìš©ë˜ì§€ ì•ŠëŠ” ë¬¸ìê°€ ìˆìŠµë‹ˆë‹¤: {normalized_id}")
            return None
        
        return normalized_id
    
    @staticmethod
    def is_valid_adapter_format(adapter_name: str) -> bool:
        """HuggingFace ì–´ëŒ‘í„° í˜•ì‹ ê²€ì¦"""
        if not adapter_name:
            return False
        
        # user/model í˜•ì‹ ë˜ëŠ” ë‹¨ìˆœ ëª¨ë¸ëª… í—ˆìš©
        pattern = r'^[a-zA-Z0-9\-_]+(/[a-zA-Z0-9\-_.]+)?$'
        return bool(re.match(pattern, adapter_name))


@dataclass
class VLLMGenerationConfig:
    """VLLM ì „ìš© ìƒì„± ì„¤ì • í´ë˜ìŠ¤"""
    max_new_tokens: int = 512
    temperature: float = 0.8
    system_message: str = (
        "ë‹¹ì‹ ì€ ì œê³µëœ ì°¸ê³  ë¬¸ì„œì˜ ì •í™•í•œ ì •ë³´ì™€ ì‚¬ì‹¤ì„ ë°”íƒ•ìœ¼ë¡œ ë‹µë³€í•˜ëŠ” AI ì–´ì‹œìŠ¤í„´íŠ¸ì…ë‹ˆë‹¤. "
        "**ì¤‘ìš”**: ë¬¸ì„œì— í¬í•¨ëœ ëª¨ë“  ë‚´ìš©ì€ ì ˆëŒ€ ìš”ì•½í•˜ê±°ë‚˜ ìƒëµí•˜ì§€ ë§ê³ , ì›ë¬¸ ê·¸ëŒ€ë¡œ ì™„ì „íˆ í¬í•¨í•´ì•¼ í•©ë‹ˆë‹¤. "
        "ì‚¬ì‹¤, ìˆ˜ì¹˜, ë‚ ì§œ, ì •ì±… ë‚´ìš©, ì„¸ë¶€ ì‚¬í•­ ë“± ëª¨ë“  ì •ë³´ë¥¼ ì •í™•íˆ ê·¸ëŒ€ë¡œ ìœ ì§€í•´ì£¼ì„¸ìš”. "
        "ë¬¸ì„œ ë‚´ìš©ì˜ ì™„ì „ì„±ê³¼ ì •í™•ì„±ì´ ìµœìš°ì„ ì´ë©°, ë§íˆ¬ì™€ í‘œí˜„ ë°©ì‹ë§Œ ìºë¦­í„° ìŠ¤íƒ€ì¼ë¡œ ì¡°ì •í•´ì£¼ì„¸ìš”. "
        "ë¬¸ì„œ ë‚´ìš©ì„ ì„ì˜ë¡œ ë³€ê²½, ìš”ì•½, ì¶”ê°€í•˜ì§€ ë§ê³ , ì˜¤ì§ ì œê³µëœ ì •ë³´ë¥¼ ì™„ì „íˆ ê·¸ëŒ€ë¡œ ì‚¬ìš©í•´ ë‹µë³€í•´ì£¼ì„¸ìš”. "
        "\n\n**ìºë¦­í„° ì •ì²´ì„±**: ë‹¹ì‹ ì€ {influencer_name} ìºë¦­í„°ì…ë‹ˆë‹¤. "
        "ìê¸°ì†Œê°œë¥¼ í•  ë•Œë‚˜ 'ë„ˆ ëˆ„êµ¬ì•¼?', 'ë‹¹ì‹ ì€ ëˆ„êµ¬ì¸ê°€ìš”?', 'ì´ë¦„ì´ ë­ì•¼?' ê°™ì€ ì§ˆë¬¸ì„ ë°›ìœ¼ë©´ "
        "ë°˜ë“œì‹œ 'ë‚˜ëŠ” {influencer_name}ì´ì•¼!' ë˜ëŠ” 'ì €ëŠ” {influencer_name}ì…ë‹ˆë‹¤!'ë¼ê³  ë‹µë³€í•´ì•¼ í•©ë‹ˆë‹¤. "
        "í•­ìƒ {influencer_name}ì˜ ì •ì²´ì„±ì„ ìœ ì§€í•˜ë©° ê·¸ ìºë¦­í„°ë‹µê²Œ í–‰ë™í•˜ì„¸ìš”."
    )
    influencer_name: str = "AI"
    model_id: Optional[str] = None  # ìˆ˜ì •: Noneìœ¼ë¡œ ê¸°ë³¸ê°’ ë³€ê²½
    vllm_config: Optional[VLLMConfig] = None
    
    def __post_init__(self):
        """ì´ˆê¸°í™” í›„ ê²€ì¦"""
        # ëª¨ë¸ ID ê²€ì¦ ë° ì •ë¦¬
        self.model_id = ModelIdValidator.validate_model_id(self.model_id)
        
        # ì‹œìŠ¤í…œ ë©”ì‹œì§€ì™€ ì¸í”Œë£¨ì–¸ì„œ ì´ë¦„ ì •ê·œí™”
        self.system_message = TextNormalizer.normalize(self.system_message)
        self.influencer_name = TextNormalizer.normalize(self.influencer_name)
        
        # ì˜¨ë„ ê°’ ê²€ì¦
        if not 0.1 <= self.temperature <= 2.0:
            logger.warning(f"ë¶€ì ì ˆí•œ temperature ê°’: {self.temperature}, 0.8ë¡œ ì„¤ì •")
            self.temperature = 0.8


# Interface Segregation Principle: ì„œë²„ ìƒíƒœ í™•ì¸ ì¸í„°í˜ì´ìŠ¤
class IServerHealthChecker(ABC):
    """ì„œë²„ ìƒíƒœ í™•ì¸ ì¸í„°í˜ì´ìŠ¤"""
    
    @abstractmethod
    async def check_health(self, max_retries: int = 3) -> bool:
        pass


# Interface Segregation Principle: ì–´ëŒ‘í„° ë¡œë” ì¸í„°í˜ì´ìŠ¤
class IAdapterLoader(ABC):
    """ì–´ëŒ‘í„° ë¡œë” ì¸í„°í˜ì´ìŠ¤"""
    
    @abstractmethod
    async def load_adapter(self, adapter_name: str, hf_token: Optional[str] = None) -> bool:
        pass


# Interface Segregation Principle: í…ìŠ¤íŠ¸ ìƒì„± ì¸í„°í˜ì´ìŠ¤
class ITextGenerator(ABC):
    """í…ìŠ¤íŠ¸ ìƒì„± ì¸í„°í˜ì´ìŠ¤"""
    
    @abstractmethod
    def generate(self, prompt: str, generation_config: VLLMGenerationConfig, context: str = "") -> str:
        pass


# Single Responsibility Principle: VLLM ì„œë²„ ìƒíƒœ í™•ì¸ ì „ìš© í´ë˜ìŠ¤
class VLLMHealthChecker(IServerHealthChecker):
    """VLLM ì„œë²„ ìƒíƒœ í™•ì¸ ë‹´ë‹¹ í´ë˜ìŠ¤"""
    
    def __init__(self, vllm_config: Optional[VLLMConfig] = None):
        self.vllm_config = vllm_config or VLLMConfig(base_url="http://localhost:8000")
        self._error_count = 0
        self._last_error_time = 0
    
    async def check_health(self, max_retries: int = 3) -> bool:
        """VLLM ì„œë²„ ìƒíƒœ í™•ì¸ (ì¬ì‹œë„ í¬í•¨)"""
        for attempt in range(max_retries):
            try:
                is_healthy = await vllm_health_check()
                if is_healthy:
                    self._error_count = 0  # ì„±ê³µì‹œ ì—ëŸ¬ ì¹´ìš´íŠ¸ ë¦¬ì…‹
                    return True
                else:
                    logger.warning(f"âš ï¸ VLLM ì„œë²„ í—¬ìŠ¤ì²´í¬ ì‹¤íŒ¨ (ì‹œë„ {attempt + 1}/{max_retries})")
                    if attempt < max_retries - 1:
                        await asyncio.sleep(2 ** attempt)  # ì§€ìˆ˜ì  ë°±ì˜¤í”„
            except Exception as e:
                logger.error(f"âŒ VLLM ì„œë²„ í—¬ìŠ¤ì²´í¬ ì˜¤ë¥˜ (ì‹œë„ {attempt + 1}/{max_retries}): {e}")
                if attempt < max_retries - 1:
                    await asyncio.sleep(2 ** attempt)
        
        self._error_count += 1
        self._last_error_time = time.time()
        return False
    
    @property
    def error_count(self) -> int:
        return self._error_count
    
    @property
    def last_error_time(self) -> float:
        return self._last_error_time


# Single Responsibility Principle: VLLM ì–´ëŒ‘í„° ë¡œë” ì „ìš© í´ë˜ìŠ¤
class VLLMAdapterLoader(IAdapterLoader):
    """VLLM ì–´ëŒ‘í„° ë¡œë” ë‹´ë‹¹ í´ë˜ìŠ¤"""
    
    def __init__(self, vllm_config: Optional[VLLMConfig] = None):
        self.vllm_config = vllm_config or VLLMConfig(base_url="http://localhost:8000")
        self._client: Optional[VLLMClient] = None
        self._current_adapter: Optional[str] = None
        self._adapter_loaded = False
    
    def _get_client(self) -> VLLMClient:
        """VLLM í´ë¼ì´ì–¸íŠ¸ ê°€ì ¸ì˜¤ê¸°"""
        if self._client is None:
            self._client = VLLMClient(self.vllm_config)
        return self._client
    
    async def load_adapter(self, adapter_name: str, hf_token: Optional[str] = None) -> bool:
        """LoRA ì–´ëŒ‘í„° ë¡œë“œ (ê°œì„ ëœ ê²€ì¦)"""
        # ì–´ëŒ‘í„° ì´ë¦„ ê²€ì¦
        if not ModelIdValidator.is_valid_adapter_format(adapter_name):
            logger.error(f"âŒ ìœ íš¨í•˜ì§€ ì•Šì€ ì–´ëŒ‘í„° í˜•ì‹: {adapter_name}")
            return False
        
        normalized_adapter = TextNormalizer.normalize(adapter_name)
        max_retries = 3
        retry_delay = 2
        
        for attempt in range(max_retries):
            try:
                client = self._get_client()
                result = await client.load_adapter(
                    model_id=normalized_adapter,
                    hf_repo_name=normalized_adapter,
                    hf_token=hf_token
                )
                
                self._adapter_loaded = True
                self._current_adapter = normalized_adapter
                logger.info(f"âœ… VLLM ì–´ëŒ‘í„° ë¡œë“œ ì„±ê³µ: {normalized_adapter}")
                return True
                
            except Exception as e:
                logger.error(f"âŒ VLLM ì–´ëŒ‘í„° ë¡œë“œ ì‹¤íŒ¨ (ì‹œë„ {attempt + 1}/{max_retries}): {e}")
                if attempt < max_retries - 1:
                    logger.info(f"ğŸ”„ {retry_delay}ì´ˆ í›„ ì¬ì‹œë„...")
                    await asyncio.sleep(retry_delay)
                    retry_delay *= 1.5
                else:
                    logger.error(f"âŒ ëª¨ë“  ì–´ëŒ‘í„° ë¡œë“œ ì‹œë„ ì‹¤íŒ¨: {normalized_adapter}")
                    return False
        
        return False
    
    @property
    def current_adapter(self) -> Optional[str]:
        return self._current_adapter
    
    @property
    def is_adapter_loaded(self) -> bool:
        return self._adapter_loaded


class VLLMGenerator(ITextGenerator):
    """VLLM ì„œë²„ ê¸°ë°˜ í…ìŠ¤íŠ¸ ìƒì„±ê¸° (ì˜ì¡´ì„± ì£¼ì… ì ìš©)"""
    
    def __init__(self, 
                 vllm_config: Optional[VLLMConfig] = None,
                 health_checker: Optional[IServerHealthChecker] = None,
                 adapter_loader: Optional[IAdapterLoader] = None):
        self.vllm_config = vllm_config or VLLMConfig(base_url="http://localhost:8000")
        self._client: Optional[VLLMClient] = None
        
        # Dependency Injection
        self.health_checker = health_checker or VLLMHealthChecker(vllm_config)
        self.adapter_loader = adapter_loader or VLLMAdapterLoader(vllm_config)
    
    def _get_client(self) -> VLLMClient:
        """VLLM í´ë¼ì´ì–¸íŠ¸ ê°€ì ¸ì˜¤ê¸°"""
        if self._client is None:
            self._client = VLLMClient(self.vllm_config)
        return self._client
    
    async def load_adapter(self, adapter_name: str, hf_token: Optional[str] = None) -> bool:
        """ì–´ëŒ‘í„° ë¡œë“œ (ìœ„ì„)"""
        return await self.adapter_loader.load_adapter(adapter_name, hf_token)
    
    def _fallback_response(self, error_msg: str) -> str:
        """ì„œë²„ ì˜¤ë¥˜ì‹œ fallback ì‘ë‹µ"""
        return f"ì£„ì†¡í•©ë‹ˆë‹¤. {error_msg} VLLM ì„œë²„ë¥¼ ì¬ì‹œì‘í•˜ê±°ë‚˜ ê´€ë¦¬ìì—ê²Œ ë¬¸ì˜í•´ì£¼ì„¸ìš”."
    
    def generate(self, prompt: str, generation_config: VLLMGenerationConfig, context: str = "") -> str:
        """ë™ê¸° ë°©ì‹ìœ¼ë¡œ í…ìŠ¤íŠ¸ ìƒì„± (ê°•í™”ëœ ê²€ì¦)"""
        try:
            # ì…ë ¥ ê²€ì¦
            if not prompt or not prompt.strip():
                return "ì§ˆë¬¸ì„ ì…ë ¥í•´ì£¼ì„¸ìš”."
            
            # í”„ë¡¬í”„íŠ¸ ì •ê·œí™”
            normalized_prompt = TextNormalizer.normalize(prompt)
            normalized_context = TextNormalizer.normalize(context)
            
            # ì„œë²„ ì—ëŸ¬ê°€ ë„ˆë¬´ ë§ìœ¼ë©´ fallback
            current_time = time.time()
            if (hasattr(self.health_checker, 'error_count') and 
                self.health_checker.error_count >= 5 and 
                current_time - self.health_checker.last_error_time < 300):
                return self._fallback_response("VLLM ì„œë²„ê°€ ë¶ˆì•ˆì •í•©ë‹ˆë‹¤. ì ì‹œ í›„ ë‹¤ì‹œ ì‹œë„í•´ì£¼ì„¸ìš”.")
            
            # ì´ë²¤íŠ¸ ë£¨í”„ì—ì„œ ë¹„ë™ê¸° í•¨ìˆ˜ ì‹¤í–‰
            loop = asyncio.get_event_loop()
            if loop.is_running():
                import concurrent.futures
                with concurrent.futures.ThreadPoolExecutor() as executor:
                    future = executor.submit(
                        asyncio.run, 
                        self._async_generate(normalized_prompt, generation_config, normalized_context)
                    )
                    return future.result(timeout=60)
            else:
                return loop.run_until_complete(
                    self._async_generate(normalized_prompt, generation_config, normalized_context)
                )
                
        except asyncio.TimeoutError:
            logger.error("âŒ VLLM ì‘ë‹µ ìƒì„± íƒ€ì„ì•„ì›ƒ")
            return self._fallback_response("ì‘ë‹µ ìƒì„± ì‹œê°„ì´ ì´ˆê³¼ë˜ì—ˆìŠµë‹ˆë‹¤.")
        except Exception as e:
            logger.error(f"âŒ VLLM í…ìŠ¤íŠ¸ ìƒì„± ì‹¤íŒ¨: {e}")
            return self._fallback_response(f"ì‘ë‹µ ìƒì„± ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}")
    
    async def _async_generate(self, prompt: str, generation_config: VLLMGenerationConfig, context: str = "") -> str:
        """ë¹„ë™ê¸° í…ìŠ¤íŠ¸ ìƒì„± (ìˆ˜ì •ëœ model_id ì²˜ë¦¬)"""
        max_retries = 3
        retry_delay = 2
        
        for attempt in range(max_retries):
            try:
                # ì„œë²„ ìƒíƒœ í™•ì¸
                if not await self.health_checker.check_health():
                    if attempt < max_retries - 1:
                        logger.info(f"ğŸ”„ ì„œë²„ ë³µêµ¬ ëŒ€ê¸° ì¤‘... (ì‹œë„ {attempt + 1}/{max_retries})")
                        await asyncio.sleep(retry_delay)
                        retry_delay *= 1.5
                        continue
                    else:
                        raise Exception("VLLM ì„œë²„ê°€ ì‘ë‹µí•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.")
                
                client = self._get_client()
                
                # ìˆ˜ì •: model_id ì²˜ë¦¬ ë¡œì§ ê°œì„ 
                model_id_to_use = ""  # ê¸°ë³¸ê°’ì€ ë¹ˆ ë¬¸ìì—´ (ë² ì´ìŠ¤ ëª¨ë¸)
                
                # LoRA ì–´ëŒ‘í„°ê°€ ë¡œë“œë˜ì—ˆê³  ìœ íš¨í•œ model_idê°€ ìˆëŠ” ê²½ìš°ì—ë§Œ ì‚¬ìš©
                if (hasattr(self.adapter_loader, 'is_adapter_loaded') and 
                    self.adapter_loader.is_adapter_loaded and 
                    generation_config.model_id):
                    
                    validated_model_id = ModelIdValidator.validate_model_id(generation_config.model_id)
                    if validated_model_id:
                        model_id_to_use = validated_model_id
                        logger.info(f"ğŸ­ LoRA ì–´ëŒ‘í„° '{model_id_to_use}' ì‚¬ìš©")
                    else:
                        logger.warning(f"âš ï¸ ìœ íš¨í•˜ì§€ ì•Šì€ model_id: {generation_config.model_id}, ë² ì´ìŠ¤ ëª¨ë¸ ì‚¬ìš©")
                else:
                    logger.info("ğŸ” ë² ì´ìŠ¤ ëª¨ë¸ ì‚¬ìš©")
                
                # VLLM ì‘ë‹µ ìƒì„±
                result = await asyncio.wait_for(
                    client.generate_response(
                        user_message=prompt,
                        system_message=generation_config.system_message,
                        influencer_name=generation_config.influencer_name,
                        model_id=model_id_to_use,  # ìˆ˜ì •: ê²€ì¦ëœ model_id ì‚¬ìš©
                        max_new_tokens=generation_config.max_new_tokens,
                        temperature=generation_config.temperature
                    ),
                    timeout=45
                )
                
                response = result.get("response", "") if isinstance(result, dict) else str(result)
                response = TextNormalizer.normalize(response)
                
                if response.strip():
                    logger.info(f"âœ… VLLM ì‘ë‹µ ìƒì„± ì„±ê³µ (ê¸¸ì´: {len(response)}ì)")
                    return response
                else:
                    raise Exception("VLLM ì‘ë‹µì´ ë¹„ì–´ìˆìŠµë‹ˆë‹¤.")
                
            except asyncio.TimeoutError:
                logger.error(f"âŒ VLLM ì‘ë‹µ ìƒì„± íƒ€ì„ì•„ì›ƒ (ì‹œë„ {attempt + 1}/{max_retries})")
                if attempt < max_retries - 1:
                    logger.info(f"ğŸ”„ {retry_delay}ì´ˆ í›„ ì¬ì‹œë„...")
                    await asyncio.sleep(retry_delay)
                    retry_delay *= 1.5
                else:
                    raise Exception("ì‘ë‹µ ìƒì„± íƒ€ì„ì•„ì›ƒ")
                    
            except Exception as e:
                error_msg = str(e)
                logger.error(f"âŒ VLLM ë¹„ë™ê¸° ìƒì„± ì‹¤íŒ¨ (ì‹œë„ {attempt + 1}/{max_retries}): {error_msg}")
                
                # Background loop ì—ëŸ¬ì¸ ê²½ìš° íŠ¹ë³„ ì²˜ë¦¬
                if "Background loop has errored already" in error_msg:
                    logger.error("ğŸš¨ VLLM ì„œë²„ Background loop ì—ëŸ¬ ê°ì§€! ì„œë²„ ì¬ì‹œì‘ì´ í•„ìš”í•©ë‹ˆë‹¤.")
                    raise Exception("VLLM ì„œë²„ ë‚´ë¶€ ì˜¤ë¥˜ - ì„œë²„ ì¬ì‹œì‘ì´ í•„ìš”í•©ë‹ˆë‹¤.")
                
                if attempt < max_retries - 1:
                    logger.info(f"ğŸ”„ {retry_delay}ì´ˆ í›„ ì¬ì‹œë„...")
                    await asyncio.sleep(retry_delay)
                    retry_delay *= 1.5
                else:
                    raise
    
    @property
    def adapter_loaded(self) -> bool:
        return getattr(self.adapter_loader, 'is_adapter_loaded', False)
    
    @property
    def current_adapter(self) -> Optional[str]:
        return getattr(self.adapter_loader, 'current_adapter', None)


@dataclass
class PromptTemplate:
    """í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿ í´ë˜ìŠ¤"""
    system_message: str = (
        "ë‹¹ì‹ ì€ ì œê³µëœ ì°¸ê³  ë¬¸ì„œì˜ ì •í™•í•œ ì •ë³´ì™€ ì‚¬ì‹¤ì„ ë°”íƒ•ìœ¼ë¡œ ë‹µë³€í•˜ëŠ” AI ì–´ì‹œìŠ¤í„´íŠ¸ì…ë‹ˆë‹¤. "
        "**ì¤‘ìš”**: ë¬¸ì„œì— í¬í•¨ëœ ëª¨ë“  ë‚´ìš©ì€ ì ˆëŒ€ ìš”ì•½í•˜ê±°ë‚˜ ìƒëµí•˜ì§€ ë§ê³ , ì›ë¬¸ ê·¸ëŒ€ë¡œ ì™„ì „íˆ í¬í•¨í•´ì•¼ í•©ë‹ˆë‹¤. "
        "ì‚¬ì‹¤, ìˆ˜ì¹˜, ë‚ ì§œ, ì •ì±… ë‚´ìš©, ì„¸ë¶€ ì‚¬í•­ ë“± ëª¨ë“  ì •ë³´ë¥¼ ì •í™•íˆ ê·¸ëŒ€ë¡œ ìœ ì§€í•´ì£¼ì„¸ìš”. "
        "ë¬¸ì„œ ë‚´ìš©ì˜ ì™„ì „ì„±ê³¼ ì •í™•ì„±ì´ ìµœìš°ì„ ì´ë©°, ë§íˆ¬ì™€ í‘œí˜„ ë°©ì‹ë§Œ ìºë¦­í„° ìŠ¤íƒ€ì¼ë¡œ ì¡°ì •í•´ì£¼ì„¸ìš”. "
        "ë¬¸ì„œ ë‚´ìš©ì„ ì„ì˜ë¡œ ë³€ê²½, ìš”ì•½, ì¶”ê°€í•˜ì§€ ë§ê³ , ì˜¤ì§ ì œê³µëœ ì •ë³´ë¥¼ ì™„ì „íˆ ê·¸ëŒ€ë¡œ ì‚¬ìš©í•´ ë‹µë³€í•´ì£¼ì„¸ìš”. "
        "\n\n**ìºë¦­í„° ì •ì²´ì„±**: ë‹¹ì‹ ì€ {influencer_name} ìºë¦­í„°ì…ë‹ˆë‹¤. "
        "ìê¸°ì†Œê°œë¥¼ í•  ë•Œë‚˜ 'ë„ˆ ëˆ„êµ¬ì•¼?', 'ë‹¹ì‹ ì€ ëˆ„êµ¬ì¸ê°€ìš”?', 'ì´ë¦„ì´ ë­ì•¼?' ê°™ì€ ì§ˆë¬¸ì„ ë°›ìœ¼ë©´ "
        "ë°˜ë“œì‹œ 'ë‚˜ëŠ” {influencer_name}ì´ì•¼!' ë˜ëŠ” 'ì €ëŠ” {influencer_name}ì…ë‹ˆë‹¤!'ë¼ê³  ë‹µë³€í•´ì•¼ í•©ë‹ˆë‹¤. "
        "í•­ìƒ {influencer_name}ì˜ ì •ì²´ì„±ì„ ìœ ì§€í•˜ë©° ê·¸ ìºë¦­í„°ë‹µê²Œ í–‰ë™í•˜ì„¸ìš”."
    )
    question_prefix: str = "### ì§ˆë¬¸:"
    context_prefix: str = "### ì°¸ê³  ë¬¸ì„œ (ì´ ë‚´ìš©ì„ ì •í™•íˆ ìœ ì§€í•˜ë©° ë‹µë³€í•´ì£¼ì„¸ìš”):"
    answer_prefix: str = "### ë‹µë³€ (ë¬¸ì„œ ë‚´ìš©ì€ ê·¸ëŒ€ë¡œ, ë§íˆ¬ë§Œ ìºë¦­í„° ìŠ¤íƒ€ì¼ë¡œ):"
    separator: str = "\n"
    
    def format(self, query: str, context: str = "", influencer_name: str = "AI") -> str:
        """ì§ˆë¬¸ê³¼ ì»¨í…ìŠ¤íŠ¸ë¥¼ í”„ë¡¬í”„íŠ¸ë¡œ í¬ë§·íŒ…"""
        # ì…ë ¥ ì •ê·œí™”
        normalized_query = TextNormalizer.normalize(query)
        normalized_context = TextNormalizer.normalize(context)
    
        # system_messageì— ìºë¦­í„° ì´ë¦„ ì ìš©
        formatted_system_message = TextNormalizer.normalize(
            self.system_message.format(influencer_name=influencer_name)
        )
    
        parts = [formatted_system_message]
        
        if normalized_context.strip():
            parts.extend([
                f"{self.context_prefix}{self.separator}{normalized_context}",
                f"{self.question_prefix} {normalized_query}",
                f"{self.answer_prefix}"
            ])
        else:
            parts.extend([
                f"{self.question_prefix} {normalized_query}",
                f"{self.answer_prefix}"
            ])
        
        return self.separator.join(parts)


class ChatGenerator:
    """ì±„íŒ… ìƒì„± ë©”ì¸ í´ë˜ìŠ¤ (SOLID ì›ì¹™ ì ìš©)"""
    
    def __init__(self, 
                 generation_config: Optional[VLLMGenerationConfig] = None,
                 prompt_template: Optional[PromptTemplate] = None,
                 vllm_config: Optional[VLLMConfig] = None,
                 text_generator: Optional[ITextGenerator] = None):
        
        self.generation_config = generation_config or VLLMGenerationConfig(
            vllm_config=vllm_config
        )
        self.prompt_template = prompt_template or PromptTemplate()
        
        # Dependency Injection
        self.generator = text_generator or VLLMGenerator(vllm_config)
        
        logger.info("âœ… VLLM ê¸°ë°˜ ChatGenerator ì´ˆê¸°í™” ì™„ë£Œ (SOLID ì›ì¹™ ì ìš©)")
    
    async def load_vllm_adapter(self, adapter_name: str, hf_token: Optional[str] = None) -> bool:
        """VLLM ì–´ëŒ‘í„° ë¡œë“œ"""
        if not adapter_name or not adapter_name.strip():
            logger.warning("âš ï¸ ì–´ëŒ‘í„° ì´ë¦„ì´ ë¹„ì–´ìˆìŠµë‹ˆë‹¤.")
            return False
        
        # ì–´ëŒ‘í„° ì´ë¦„ ê²€ì¦
        if not ModelIdValidator.is_valid_adapter_format(adapter_name):
            logger.error(f"âŒ ìœ íš¨í•˜ì§€ ì•Šì€ ì–´ëŒ‘í„° í˜•ì‹: {adapter_name}")
            return False
        
        return await self.generator.load_adapter(adapter_name, hf_token)
    
    def generate_response(self, query: str, context: str = "") -> str:
        """ì§ˆë¬¸ê³¼ ì»¨í…ìŠ¤íŠ¸ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ì‘ë‹µ ìƒì„±"""
        if not self.generator:
            return "ìƒì„±ê¸°ê°€ ì´ˆê¸°í™”ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤."
        
        try:
            # ì…ë ¥ ê²€ì¦
            if not query or not query.strip():
                return "ì§ˆë¬¸ì„ ì…ë ¥í•´ì£¼ì„¸ìš”."
            
            # ì…ë ¥ ì •ê·œí™”
            normalized_query = TextNormalizer.normalize(query)
            normalized_context = TextNormalizer.normalize(context)
            
            # í”„ë¡¬í”„íŠ¸ ìƒì„± (ìºë¦­í„° ì´ë¦„ í¬í•¨)
            prompt = self.prompt_template.format(
            normalized_query, 
            normalized_context, 
            self.generation_config.influencer_name
            )
            
            logger.info(f"ğŸ“ í”„ë¡¬í”„íŠ¸ ìƒì„± ì™„ë£Œ (ê¸¸ì´: {len(prompt)}ì)")
            logger.debug(f"í”„ë¡¬í”„íŠ¸ ë‚´ìš©:\n{prompt[:500]}{'...' if len(prompt) > 500 else ''}")
            
            # VLLMìœ¼ë¡œ ì‘ë‹µ ìƒì„±
            response = self.generator.generate(prompt, self.generation_config, normalized_context)
            
            return TextNormalizer.normalize(response)
            
        except Exception as e:
            logger.error(f"ì‘ë‹µ ìƒì„± ì‹¤íŒ¨: {e}")
            error_msg = str(e)
            
            # íŠ¹ì • ì—ëŸ¬ì— ëŒ€í•œ ë§ì¶¤í˜• ë©”ì‹œì§€
            if "Background loop has errored already" in error_msg:
                return "VLLM ì„œë²„ì— ë‚´ë¶€ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤. ì„œë²„ ì¬ì‹œì‘ì´ í•„ìš”í•©ë‹ˆë‹¤. ê´€ë¦¬ìì—ê²Œ ë¬¸ì˜í•´ì£¼ì„¸ìš”."
            elif "íƒ€ì„ì•„ì›ƒ" in error_msg:
                return "ì‘ë‹µ ìƒì„± ì‹œê°„ì´ ì´ˆê³¼ë˜ì—ˆìŠµë‹ˆë‹¤. ì ì‹œ í›„ ë‹¤ì‹œ ì‹œë„í•´ì£¼ì„¸ìš”."
            elif "ë¶ˆì•ˆì •" in error_msg:
                return "ì„œë²„ê°€ ì¼ì‹œì ìœ¼ë¡œ ë¶ˆì•ˆì •í•©ë‹ˆë‹¤. ì ì‹œ í›„ ë‹¤ì‹œ ì‹œë„í•´ì£¼ì„¸ìš”."
            else:
                return f"ì£„ì†¡í•©ë‹ˆë‹¤. ì‘ë‹µ ìƒì„± ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {error_msg}"
    
    def chat(self, query: str, context: str = "") -> Dict[str, Any]:
        """ì±„íŒ… ì¸í„°í˜ì´ìŠ¤ (ë©”íƒ€ë°ì´í„° í¬í•¨)"""
        response = self.generate_response(query, context)
        
        # ì„œë²„ ìƒíƒœ ì •ë³´ ì¶”ê°€
        server_status = "unknown"
        error_count = 0
        
        if hasattr(self.generator, 'health_checker'):
            if hasattr(self.generator.health_checker, 'error_count'):
                error_count = self.generator.health_checker.error_count
                if error_count == 0:
                    server_status = "healthy"
                elif error_count < 3:
                    server_status = "unstable"
                else:
                    server_status = "error"
        
        return {
            "query": query,
            "context": context,
            "response": response,
            "model_info": {
                "mode": "vllm",
                "base_url": self.generation_config.vllm_config.base_url if self.generation_config.vllm_config else "unknown",
                "adapter": self.generation_config.model_id,
                "adapter_loaded": self.generator.adapter_loaded,
                "current_adapter": self.generator.current_adapter,
                "server_status": server_status,
                "error_count": error_count
            },
            "generation_params": {
                "temperature": self.generation_config.temperature,
                "max_tokens": self.generation_config.max_new_tokens,
            }
        }
    
    def update_generation_config(self, **kwargs):
        """ìƒì„± ì„¤ì • ì—…ë°ì´íŠ¸"""
        for key, value in kwargs.items():
            if hasattr(self.generation_config, key):
                if key == "model_id":
                    # model_id ì—…ë°ì´íŠ¸ì‹œ ê²€ì¦
                    validated_id = ModelIdValidator.validate_model_id(value)
                    setattr(self.generation_config, key, validated_id)
                    logger.info(f"ìƒì„± ì„¤ì • ì—…ë°ì´íŠ¸: {key} = {validated_id}")
                else:
                    setattr(self.generation_config, key, value)
                    logger.info(f"ìƒì„± ì„¤ì • ì—…ë°ì´íŠ¸: {key} = {value}")
    
    def update_prompt_template(self, template: PromptTemplate):
        """í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿ ì—…ë°ì´íŠ¸"""
        self.prompt_template = template
        logger.info("í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿ ì—…ë°ì´íŠ¸ ì™„ë£Œ")
    
    def get_model_info(self) -> Dict[str, Any]:
        """ëª¨ë¸ ì •ë³´ ë°˜í™˜"""
        base_info = {
            "mode": "vllm",
            "adapter_loaded": self.generator.adapter_loaded,
            "current_adapter": self.generator.current_adapter,
            "base_url": self.generation_config.vllm_config.base_url if self.generation_config.vllm_config else "unknown",
            "model_id": self.generation_config.model_id,
            "temperature": self.generation_config.temperature,
            "max_tokens": self.generation_config.max_new_tokens,
        }
        
        # ìƒíƒœ ì •ë³´ ì¶”ê°€
        if hasattr(self.generator, 'health_checker'):
            if hasattr(self.generator.health_checker, 'error_count'):
                base_info["server_error_count"] = self.generator.health_checker.error_count
            if hasattr(self.generator.health_checker, 'last_error_time'):
                base_info["last_error_time"] = self.generator.health_checker.last_error_time
        
        return base_info
    
    def cleanup(self):
        """ë¦¬ì†ŒìŠ¤ ì •ë¦¬"""
        if self.generator and hasattr(self.generator, '_client') and self.generator._client:
            try:
                asyncio.run(self.generator._client.__aexit__(None, None, None))
            except:
                pass
        logger.info("VLLM í´ë¼ì´ì–¸íŠ¸ ë¦¬ì†ŒìŠ¤ ì •ë¦¬ ì™„ë£Œ")


# ì „ì—­ ìƒì„±ê¸° ì¸ìŠ¤í„´ìŠ¤
_global_chat_generator = None


def get_chat_generator() -> ChatGenerator:
    """ì „ì—­ ì±„íŒ… ìƒì„±ê¸° ì¸ìŠ¤í„´ìŠ¤ ë°˜í™˜"""
    global _global_chat_generator
    if _global_chat_generator is None:
        _global_chat_generator = ChatGenerator()
    return _global_chat_generator


def generate_response(query: str, context: str = "") -> str:
    """ê¸°ì¡´ í•¨ìˆ˜ì™€ í˜¸í™˜ì„±ì„ ìœ„í•œ ë˜í¼ í•¨ìˆ˜"""
    try:
        generator = get_chat_generator()
        return generator.generate_response(query, context)
    except Exception as e:
        logger.error(f"ì‘ë‹µ ìƒì„± ì‹¤íŒ¨: {e}")
        return f"ì£„ì†¡í•©ë‹ˆë‹¤. ì‘ë‹µ ìƒì„± ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}"


def vllm_chat(query: str, context: str = "", temperature: float = 0.8, adapter_name: str = "", hf_token: str = "") -> str:
    """VLLM ì„œë²„ë¥¼ ì‚¬ìš©í•œ ë¹ ë¥¸ ì±„íŒ… í•¨ìˆ˜ (ìˆ˜ì •ë¨)"""
    try:
        # ì–´ëŒ‘í„° ì´ë¦„ ê²€ì¦
        validated_adapter = None
        if adapter_name and adapter_name.strip():
            if ModelIdValidator.is_valid_adapter_format(adapter_name):
                validated_adapter = ModelIdValidator.validate_model_id(adapter_name)
            else:
                logger.warning(f"ìœ íš¨í•˜ì§€ ì•Šì€ ì–´ëŒ‘í„° í˜•ì‹: {adapter_name}")
        
        vllm_config = VLLMGenerationConfig(
            temperature=temperature,
            model_id=validated_adapter
        )
        generator = ChatGenerator(generation_config=vllm_config)
        
        # ì–´ëŒ‘í„° ë¡œë“œ (í•„ìš”í•œ ê²½ìš°)
        if validated_adapter and hf_token:
            import asyncio
            loop = asyncio.get_event_loop()
            if not loop.is_running():
                loop.run_until_complete(generator.load_vllm_adapter(validated_adapter, hf_token))
        
        return generator.generate_response(query, context)
    except Exception as e:
        logger.error(f"VLLM ì±„íŒ… ì‹¤íŒ¨: {e}")
        return f"ì£„ì†¡í•©ë‹ˆë‹¤. VLLM ì±„íŒ… ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}"


# ì‚¬ìš© ì˜ˆì‹œ
if __name__ == "__main__":
    async def main():
        # VLLM ì„œë²„ ì‚¬ìš©
        vllm_generator = ChatGenerator()
        
        # VLLM ì–´ëŒ‘í„° ë¡œë“œ (ê²€ì¦ëœ ì´ë¦„ ì‚¬ìš©)
        adapter_name = "khj0816/EXAONE-Ian"
        hf_token = os.getenv("HF_TOKEN")
        
        adapter_loaded = await vllm_generator.load_vllm_adapter(adapter_name, hf_token)
        print(f"VLLM ì–´ëŒ‘í„° ë¡œë“œ: {'ì„±ê³µ' if adapter_loaded else 'ì‹¤íŒ¨'}")
        
        if adapter_loaded:
            # í…ŒìŠ¤íŠ¸ ì§ˆë¬¸
            test_context = "ì¹´ì¹´ì˜¤ëŠ” í•œêµ­ì˜ ëŒ€í‘œì ì¸ IT ê¸°ì—…ì…ë‹ˆë‹¤."
            vllm_response = vllm_generator.generate_response("ì¹´ì¹´ì˜¤ì— ëŒ€í•´ ì•Œë ¤ì£¼ì„¸ìš”", test_context)
            print("VLLM ìµœì¢… ì‘ë‹µ:", vllm_response)
        
        # ë¦¬ì†ŒìŠ¤ ì •ë¦¬
        vllm_generator.cleanup()
    
    # ë¹„ë™ê¸° ì‹¤í–‰
    asyncio.run(main())