import sys
import os
from dotenv import load_dotenv
load_dotenv()
import argparse
import json
import asyncio
from pathlib import Path
from typing import Optional, List, Dict, Union
from dataclasses import dataclass
from datetime import datetime
import logging
import unicodedata
import re
from abc import ABC, abstractmethod

# backend ê²½ë¡œ ì¶”ê°€
backend_path = Path(__file__).parent.parent
sys.path.append(str(backend_path))

try:
    from app.core.config import settings
    DYNAMIC_VLLM_URL = settings.VLLM_SERVER_URL  # .envì˜ VLLM_SERVER_URL ì‚¬ìš©
    print(f"âœ… Backend ì„¤ì •ì—ì„œ VLLM URL ë¡œë“œ: {DYNAMIC_VLLM_URL}")
except ImportError as e:
    print(f"âŒ Backend ì„¤ì • import ì‹¤íŒ¨: {e}")
    print("ğŸ’¡ backend/.env íŒŒì¼ì— VLLM_SERVER_URLì„ ì„¤ì •í•´ì£¼ì„¸ìš”")
    DYNAMIC_VLLM_URL = "http://localhost:8000"  # ê¸°ë³¸ê°’
except AttributeError as e:
    print(f"âš ï¸ VLLM_SERVER_URLì´ ì„¤ì •ë˜ì§€ ì•ŠìŒ: {e}")
    print("ğŸ’¡ backend/.env íŒŒì¼ì— VLLM_SERVER_URL=your_urlì„ ì¶”ê°€í•´ì£¼ì„¸ìš”")
    DYNAMIC_VLLM_URL = "http://localhost:8000"  # ê¸°ë³¸ê°’

# í™˜ê²½ë³€ìˆ˜ì—ë„ ì„¤ì • (ì „ì²´ ì‹œìŠ¤í…œ ë™ê¸°í™”)
os.environ["VLLM_BASE_URL"] = DYNAMIC_VLLM_URL

# ìˆ˜ì •: backendì˜ VLLM í´ë¼ì´ì–¸íŠ¸ import
from app.services.vllm_client import (
    VLLMServerConfig as VLLMConfig,
    vllm_health_check
)

# ìš°ë¦¬ê°€ ë§Œë“  ëª¨ë“ˆë“¤ ì„í¬íŠ¸
from document_loader import load_pdf_and_generate_qa, PDFToQAProcessor
from embed_store import EmbeddingStore, EmbeddingConfig, MilvusConfig
from rag_search import RAGSearcher, SearchConfig, retrieve_relevant_chunks
from chat_generator import (
    ChatGenerator, VLLMGenerationConfig, PromptTemplate,
    TextNormalizer, ModelIdValidator, 
    generate_response, vllm_chat
)

# ë¡œê¹… ì„¤ì •
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)


# Single Responsibility Principle: ì„¤ì • ê²€ì¦ ì „ìš© í´ë˜ìŠ¤
class ConfigValidator:
    """íŒŒì´í”„ë¼ì¸ ì„¤ì • ê²€ì¦ ë‹´ë‹¹ í´ë˜ìŠ¤"""
    
    @staticmethod
    def validate_pipeline_config(config: 'PipelineConfig') -> bool:
        """íŒŒì´í”„ë¼ì¸ ì„¤ì • ê²€ì¦"""
        try:
            # PDF ê²½ë¡œ ê²€ì¦
            if config.pdf_path and not Path(config.pdf_path).exists():
                logger.error(f"PDF íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {config.pdf_path}")
                return False
            
            # ì¶œë ¥ ë””ë ‰í† ë¦¬ ìƒì„± ê°€ëŠ¥ì„± ê²€ì¦
            try:
                Path(config.output_dir).mkdir(parents=True, exist_ok=True)
            except Exception as e:
                logger.error(f"ì¶œë ¥ ë””ë ‰í† ë¦¬ ìƒì„± ì‹¤íŒ¨: {config.output_dir}, {e}")
                return False
            
            # ìˆ˜ì¹˜ ë²”ìœ„ ê²€ì¦
            if not 10 <= config.min_paragraph_length <= 1000:
                logger.warning(f"min_paragraph_length ê°’ì´ ë¹„ì •ìƒì ì…ë‹ˆë‹¤: {config.min_paragraph_length}")
                config.min_paragraph_length = 30
            
            if not 1 <= config.max_qa_pairs <= 10000:
                logger.warning(f"max_qa_pairs ê°’ì´ ë¹„ì •ìƒì ì…ë‹ˆë‹¤: {config.max_qa_pairs}")
                config.max_qa_pairs = 100
            
            if not 1 <= config.search_top_k <= 20:
                logger.warning(f"search_top_k ê°’ì´ ë¹„ì •ìƒì ì…ë‹ˆë‹¤: {config.search_top_k}")
                config.search_top_k = 3
            
            if not 0.0 <= config.search_threshold <= 1.0:
                logger.warning(f"search_threshold ê°’ì´ ë¹„ì •ìƒì ì…ë‹ˆë‹¤: {config.search_threshold}")
                config.search_threshold = 0.7
            
            if not 0.1 <= config.response_temperature <= 2.0:
                logger.warning(f"response_temperature ê°’ì´ ë¹„ì •ìƒì ì…ë‹ˆë‹¤: {config.response_temperature}")
                config.response_temperature = 0.8
            
            if not 50 <= config.response_max_tokens <= 4096:
                logger.warning(f"response_max_tokens ê°’ì´ ë¹„ì •ìƒì ì…ë‹ˆë‹¤: {config.response_max_tokens}")
                config.response_max_tokens = 512
            
            # VLLM URL ê²€ì¦
            if config.use_vllm and not config.vllm_base_url.startswith(('http://', 'https://')):
                logger.error(f"ìœ íš¨í•˜ì§€ ì•Šì€ VLLM URL: {config.vllm_base_url}")
                return False
            
            # ì‹œìŠ¤í…œ ë©”ì‹œì§€ì™€ ì¸í”Œë£¨ì–¸ì„œ ì´ë¦„ ì •ê·œí™”
            config.system_message = TextNormalizer.normalize(config.system_message)
            config.influencer_name = TextNormalizer.normalize(config.influencer_name)
            
            return True
            
        except Exception as e:
            logger.error(f"ì„¤ì • ê²€ì¦ ì¤‘ ì˜¤ë¥˜: {e}")
            return False


@dataclass
class PipelineConfig:
    """íŒŒì´í”„ë¼ì¸ ì „ì²´ ì„¤ì • (backend .env ì‚¬ìš©)"""
    # íŒŒì¼ ê²½ë¡œ
    pdf_path: str = ""
    output_dir: str = "./rag_output"
    
    # ì²˜ë¦¬ ì„¤ì •
    min_paragraph_length: int = 30
    max_qa_pairs: int = 100
    
    # ê²€ìƒ‰ ì„¤ì •
    search_top_k: int = 3
    search_threshold: float = 0.7
    
    # ìƒì„± ì„¤ì •
    response_max_tokens: int = 512
    response_temperature: float = 0.8
    
    # VLLM ì„¤ì • (backend .envì—ì„œ ë¡œë“œ)
    use_vllm: bool = True
    vllm_base_url: str = DYNAMIC_VLLM_URL  # backend .envì˜ VLLM_SERVER_URL ì‚¬ìš©
    system_message: str = (
        "ë‹¹ì‹ ì€ ì œê³µëœ ì°¸ê³  ë¬¸ì„œì˜ ì •í™•í•œ ì •ë³´ì™€ ì‚¬ì‹¤ì„ ë°”íƒ•ìœ¼ë¡œ ë‹µë³€í•˜ëŠ” AI ì–´ì‹œìŠ¤í„´íŠ¸ì…ë‹ˆë‹¤. "
        "**ì¤‘ìš”**: ë¬¸ì„œì— í¬í•¨ëœ ëª¨ë“  ë‚´ìš©ì€ ì ˆëŒ€ ìš”ì•½í•˜ê±°ë‚˜ ìƒëµí•˜ì§€ ë§ê³ , ì›ë¬¸ ê·¸ëŒ€ë¡œ ì™„ì „íˆ í¬í•¨í•´ì•¼ í•©ë‹ˆë‹¤. "
        "ì‚¬ì‹¤, ìˆ˜ì¹˜, ë‚ ì§œ, ì •ì±… ë‚´ìš©, ì„¸ë¶€ ì‚¬í•­ ë“± ëª¨ë“  ì •ë³´ë¥¼ ì •í™•íˆ ê·¸ëŒ€ë¡œ ìœ ì§€í•´ì£¼ì„¸ìš”. "
        "ë¬¸ì„œ ë‚´ìš©ì˜ ì™„ì „ì„±ê³¼ ì •í™•ì„±ì´ ìµœìš°ì„ ì´ë©°, ë§íˆ¬ì™€ í‘œí˜„ ë°©ì‹ë§Œ ìºë¦­í„° ìŠ¤íƒ€ì¼ë¡œ ì¡°ì •í•´ì£¼ì„¸ìš”. "
        "ë¬¸ì„œ ë‚´ìš©ì„ ì„ì˜ë¡œ ë³€ê²½, ìš”ì•½, ì¶”ê°€í•˜ì§€ ë§ê³ , ì˜¤ì§ ì œê³µëœ ì •ë³´ë¥¼ ì™„ì „íˆ ê·¸ëŒ€ë¡œ ì‚¬ìš©í•´ ë‹µë³€í•´ì£¼ì„¸ìš”. "
        "\n\n**ìºë¦­í„° ì •ì²´ì„±**: ë‹¹ì‹ ì€ {influencer_name} ìºë¦­í„°ì…ë‹ˆë‹¤. "
        "ìê¸°ì†Œê°œë¥¼ í•  ë•Œë‚˜ 'ë„ˆ ëˆ„êµ¬ì•¼?', 'ë‹¹ì‹ ì€ ëˆ„êµ¬ì¸ê°€ìš”?', 'ì´ë¦„ì´ ë­ì•¼?' ê°™ì€ ì§ˆë¬¸ì„ ë°›ìœ¼ë©´ "
        "ë°˜ë“œì‹œ 'ë‚˜ëŠ” {influencer_name}ì´ì•¼!' ë˜ëŠ” 'ì €ëŠ” {influencer_name}ì…ë‹ˆë‹¤!'ë¼ê³  ë‹µë³€í•´ì•¼ í•©ë‹ˆë‹¤. "
        "í•­ìƒ {influencer_name}ì˜ ì •ì²´ì„±ì„ ìœ ì§€í•˜ë©° ê·¸ ìºë¦­í„°ë‹µê²Œ í–‰ë™í•˜ì„¸ìš”."
    )
    influencer_name: str = "AI"
    
    # ë””ë²„ê·¸ ì„¤ì •
    save_intermediate: bool = True
    verbose: bool = True
    
    def __post_init__(self):
        """ì´ˆê¸°í™” í›„ ê²€ì¦"""
        if not ConfigValidator.validate_pipeline_config(self):
            logger.warning("âš ï¸ ì„¤ì • ê²€ì¦ì—ì„œ ì¼ë¶€ ë¬¸ì œê°€ ë°œê²¬ë˜ì—ˆì§€ë§Œ ê³„ì† ì§„í–‰í•©ë‹ˆë‹¤.")


# Interface Segregation Principle: ë¬¸ì„œ ì²˜ë¦¬ ì¸í„°í˜ì´ìŠ¤
class IDocumentProcessor(ABC):
    """ë¬¸ì„œ ì²˜ë¦¬ ì¸í„°í˜ì´ìŠ¤"""
    
    @abstractmethod
    def process_document(self, pdf_path: str) -> List[Dict]:
        pass


# Interface Segregation Principle: ë²¡í„° ìŠ¤í† ì–´ ê´€ë¦¬ ì¸í„°í˜ì´ìŠ¤
class IVectorStoreManager(ABC):
    """ë²¡í„° ìŠ¤í† ì–´ ê´€ë¦¬ ì¸í„°í˜ì´ìŠ¤"""
    
    @abstractmethod
    def store_qa_data(self, qa_data: List[Dict], source_file: str) -> bool:
        pass
    
    @abstractmethod
    def get_embedding_store(self) -> EmbeddingStore:
        pass


# Interface Segregation Principle: ì±—ë´‡ ì—”ì§„ ì¸í„°í˜ì´ìŠ¤
class IChatbotEngine(ABC):
    """ì±—ë´‡ ì—”ì§„ ì¸í„°í˜ì´ìŠ¤"""
    
    @abstractmethod
    def answer_question(self, query: str) -> Dict:
        pass


class DocumentProcessor(IDocumentProcessor):
    """ë¬¸ì„œ ì²˜ë¦¬ í´ë˜ìŠ¤ (ìˆ˜ì •ë¨)"""
    
    def __init__(self, config: PipelineConfig):
        self.config = config
        self.pdf_processor = PDFToQAProcessor()
    
    def process_document(self, pdf_path: str) -> List[Dict]:
        """PDF ë¬¸ì„œë¥¼ QA ìŒìœ¼ë¡œ ë³€í™˜ (ê°œì„ ëœ ê²€ì¦)"""
        # ì…ë ¥ ê²€ì¦
        if not pdf_path or not pdf_path.strip():
            raise ValueError("PDF ê²½ë¡œê°€ ë¹„ì–´ìˆìŠµë‹ˆë‹¤.")
        
        pdf_path_obj = Path(pdf_path)
        if not pdf_path_obj.exists():
            raise FileNotFoundError(f"PDF íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {pdf_path}")
        
        if not pdf_path_obj.suffix.lower() == '.pdf':
            raise ValueError(f"PDF íŒŒì¼ì´ ì•„ë‹™ë‹ˆë‹¤: {pdf_path}")
        
        logger.info(f"ğŸ“„ PDF ë¬¸ì„œ ì²˜ë¦¬ ì‹œì‘: {pdf_path}")
        
        try:
            # PDF â†’ QA ìƒì„±
            qa_pairs = self.pdf_processor.process(pdf_path)
            
            if not qa_pairs:
                logger.warning("âš ï¸ PDFì—ì„œ QA ìŒì„ ìƒì„±í•˜ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.")
                return []
            
            # ê°œìˆ˜ ì œí•œ ì ìš©
            if len(qa_pairs) > self.config.max_qa_pairs:
                qa_pairs = qa_pairs[:self.config.max_qa_pairs]
                logger.info(f"QA ìŒì„ {self.config.max_qa_pairs}ê°œë¡œ ì œí•œ")
            
            # í…ìŠ¤íŠ¸ ì •ê·œí™” ë° ê²€ì¦ ì ìš©
            normalized_qa_pairs = []
            for qa in qa_pairs:
                try:
                    normalized_qa = {
                        "question": TextNormalizer.normalize(qa.question),
                        "answer": TextNormalizer.normalize(qa.answer),
                        "context": TextNormalizer.normalize(qa.context),
                        "source_page": qa.source_page
                    }
                    
                    # ë¹„ì–´ìˆëŠ” ì§ˆë¬¸ì´ë‚˜ ë‹µë³€ í•„í„°ë§
                    if (normalized_qa["question"].strip() and 
                        normalized_qa["answer"].strip() and
                        len(normalized_qa["question"]) >= 5 and
                        len(normalized_qa["answer"]) >= 10):
                        normalized_qa_pairs.append(normalized_qa)
                    else:
                        logger.debug(f"ë¹ˆ QA ìŒ í•„í„°ë§: Q={qa.question[:50]}")
                        
                except Exception as e:
                    logger.warning(f"QA ìŒ ì •ê·œí™” ì‹¤íŒ¨: {e}")
                    continue
            
            if not normalized_qa_pairs:
                raise ValueError("ìœ íš¨í•œ QA ìŒì„ ìƒì„±í•˜ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.")
            
            # ì¤‘ê°„ ê²°ê³¼ ì €ì¥
            if self.config.save_intermediate:
                self._save_qa_pairs(normalized_qa_pairs, pdf_path)
            
            logger.info(f"âœ… ë¬¸ì„œ ì²˜ë¦¬ ì™„ë£Œ: {len(normalized_qa_pairs)}ê°œ QA ìŒ ìƒì„±")
            return normalized_qa_pairs
            
        except Exception as e:
            logger.error(f"âŒ ë¬¸ì„œ ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
            raise
    
    def _save_qa_pairs(self, qa_pairs, pdf_path: str):
        """QA ìŒì„ JSON íŒŒì¼ë¡œ ì €ì¥"""
        try:
            output_dir = Path(self.config.output_dir)
            output_dir.mkdir(exist_ok=True)
            
            filename = Path(pdf_path).stem
            output_path = output_dir / f"{filename}_qa_pairs.json"
            
            with open(output_path, 'w', encoding='utf-8') as f:
                json.dump(qa_pairs, f, ensure_ascii=False, indent=2)
            
            logger.info(f"ğŸ’¾ QA ìŒ ì €ì¥: {output_path}")
            
        except Exception as e:
            logger.error(f"QA ìŒ ì €ì¥ ì‹¤íŒ¨: {e}")


class VectorStoreManager(IVectorStoreManager):
    """ë²¡í„° ìŠ¤í† ì–´ ê´€ë¦¬ í´ë˜ìŠ¤ (ìˆ˜ì •ë¨)"""
    
    def __init__(self, config: PipelineConfig):
        self.config = config
        
        # ì¶œë ¥ ë””ë ‰í† ë¦¬ ë¨¼ì € ìƒì„±
        Path(config.output_dir).mkdir(parents=True, exist_ok=True)
        
        # ì„ë² ë”© ë° ë²¡í„° ìŠ¤í† ì–´ ì„¤ì •
        embedding_config = EmbeddingConfig(
            device="cuda" if os.getenv("CUDA_AVAILABLE") else "cpu"
        )
        
        milvus_config = MilvusConfig(
            uri=f"{config.output_dir}/rag_vectorstore.db",
            collection_name="document_qa_collection"
        )
        
        try:
            self.embedding_store = EmbeddingStore(embedding_config, milvus_config)
        except Exception as e:
            logger.error(f"ì„ë² ë”© ìŠ¤í† ì–´ ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
            raise
    
    def store_qa_data(self, qa_data: List[Dict], source_file: str) -> bool:
        """QA ë°ì´í„°ë¥¼ ë²¡í„° ìŠ¤í† ì–´ì— ì €ì¥ (ê°œì„ ëœ ê²€ì¦)"""
        if not qa_data:
            logger.error("ì €ì¥í•  QA ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
            return False
        
        if not source_file or not source_file.strip():
            logger.error("ì†ŒìŠ¤ íŒŒì¼ ê²½ë¡œê°€ ë¹„ì–´ìˆìŠµë‹ˆë‹¤.")
            return False
        
        logger.info(f"ğŸ”„ ë²¡í„° ìŠ¤í† ì–´ì— ë°ì´í„° ì €ì¥ ì¤‘... ({len(qa_data)}ê°œ í•­ëª©)")
        
        try:
            success = self.embedding_store.store_qa_chunks(qa_data, source_file)
            
            if success:
                logger.info(f"âœ… ë²¡í„° ìŠ¤í† ì–´ ì €ì¥ ì™„ë£Œ: {len(qa_data)}ê°œ í•­ëª©")
            else:
                logger.error("âŒ ë²¡í„° ìŠ¤í† ì–´ ì €ì¥ ì‹¤íŒ¨")
            
            return success
            
        except Exception as e:
            logger.error(f"âŒ ë²¡í„° ìŠ¤í† ì–´ ì €ì¥ ì¤‘ ì˜¤ë¥˜: {e}")
            return False
    
    def get_embedding_store(self) -> EmbeddingStore:
        """ì„ë² ë”© ìŠ¤í† ì–´ ë°˜í™˜"""
        return self.embedding_store


class ChatbotEngine(IChatbotEngine):
    """ì±—ë´‡ ì—”ì§„ í´ë˜ìŠ¤ (ìˆ˜ì •ë¨)"""
    
    def __init__(self, 
                 config: PipelineConfig, 
                 embedding_store: EmbeddingStore, 
                 lora_adapter: Optional[str] = None,
                 hf_token: Optional[str] = None):
        self.config = config
        self.lora_adapter = lora_adapter
        self.hf_token = hf_token
        
        # LoRA ì–´ëŒ‘í„° ê²€ì¦
        if self.lora_adapter:
            if not ModelIdValidator.is_valid_adapter_format(self.lora_adapter):
                logger.error(f"âŒ ìœ íš¨í•˜ì§€ ì•Šì€ LoRA ì–´ëŒ‘í„° í˜•ì‹: {self.lora_adapter}")
                self.lora_adapter = None
            else:
                self.lora_adapter = ModelIdValidator.validate_model_id(self.lora_adapter)
        
        # ê²€ìƒ‰ ì„¤ì •
        search_config = SearchConfig(
            top_k=config.search_top_k,
            score_threshold=config.search_threshold
        )
        self.searcher = RAGSearcher(embedding_store, search_config)
        
        # ìƒì„± ì„¤ì • (lazy loading)
        self.chat_generator = None
        self._generator_initialized = False
    
    def _initialize_generator(self):
        """ì±„íŒ… ìƒì„±ê¸° ì§€ì—° ì´ˆê¸°í™” (ìˆ˜ì •ë¨)"""
        if not self._generator_initialized:
            logger.info("ğŸ¤– AI ëª¨ë¸ ë¡œë”© ì¤‘... (ì‹œê°„ì´ ê±¸ë¦´ ìˆ˜ ìˆìŠµë‹ˆë‹¤)")
            
            try:
                # VLLM ëª¨ë“œ ì„¤ì •
                vllm_config = VLLMConfig(base_url=self.config.vllm_base_url)
                generation_config = VLLMGenerationConfig(
                    max_new_tokens=self.config.response_max_tokens,
                    temperature=self.config.response_temperature,
                    system_message=self.config.system_message,
                    influencer_name=self.config.influencer_name,
                    model_id=self.lora_adapter,  # ê²€ì¦ëœ ì–´ëŒ‘í„° ì‚¬ìš©
                    vllm_config=vllm_config
                )
                
                prompt_template = PromptTemplate()
                
                # ChatGenerator ì´ˆê¸°í™”
                self.chat_generator = ChatGenerator(
                    generation_config=generation_config,
                    prompt_template=prompt_template,
                    vllm_config=vllm_config
                )
                
                # VLLM ì–´ëŒ‘í„° ë¡œë“œ (ì¬ì‹œë„ ë¡œì§ ì¶”ê°€)
                if self.lora_adapter and self.hf_token:
                    self._load_adapter_with_retry()
                
                self._generator_initialized = True
                logger.info("âœ… AI ëª¨ë¸ ë¡œë”© ì™„ë£Œ")
                
            except Exception as e:
                logger.error(f"âŒ AI ëª¨ë¸ ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
                raise
    
    def _load_adapter_with_retry(self):
        """ì–´ëŒ‘í„° ë¡œë“œ (ì¬ì‹œë„ ë¡œì§)"""
        max_retries = 3
        retry_delay = 2
        
        for attempt in range(max_retries):
            try:
                logger.info(f"ğŸ”„ LoRA ì–´ëŒ‘í„° ë¡œë“œ ì‹œë„ {attempt + 1}/{max_retries}: {self.lora_adapter}")
                
                loop = asyncio.get_event_loop()
                if loop.is_running():
                    # ìƒˆ ì´ë²¤íŠ¸ ë£¨í”„ì—ì„œ ì‹¤í–‰
                    import concurrent.futures
                    with concurrent.futures.ThreadPoolExecutor() as executor:
                        future = executor.submit(
                            asyncio.run, 
                            self.chat_generator.load_vllm_adapter(self.lora_adapter, self.hf_token)
                        )
                        adapter_loaded = future.result(timeout=60)
                else:
                    adapter_loaded = loop.run_until_complete(
                        self.chat_generator.load_vllm_adapter(self.lora_adapter, self.hf_token)
                    )
                
                if adapter_loaded:
                    logger.info(f"âœ… VLLM LoRA ì–´ëŒ‘í„° ë¡œë“œ ì„±ê³µ: {self.lora_adapter}")
                    break
                else:
                    logger.warning(f"âš ï¸ VLLM LoRA ì–´ëŒ‘í„° ë¡œë“œ ì‹¤íŒ¨ (ì‹œë„ {attempt + 1}/{max_retries}): {self.lora_adapter}")
                    if attempt < max_retries - 1:
                        logger.info(f"ğŸ”„ {retry_delay}ì´ˆ í›„ ì¬ì‹œë„...")
                        import time
                        time.sleep(retry_delay)
                        retry_delay *= 1.5
                        
            except Exception as e:
                logger.error(f"âŒ VLLM ì–´ëŒ‘í„° ë¡œë“œ ì¤‘ ì˜¤ë¥˜ (ì‹œë„ {attempt + 1}/{max_retries}): {e}")
                if attempt < max_retries - 1:
                    logger.info(f"ğŸ”„ {retry_delay}ì´ˆ í›„ ì¬ì‹œë„...")
                    import time
                    time.sleep(retry_delay)
                    retry_delay *= 1.5
                else:
                    logger.warning("âš ï¸ ëª¨ë“  ì–´ëŒ‘í„° ë¡œë“œ ì‹œë„ ì‹¤íŒ¨, ê¸°ë³¸ ëª¨ë¸ë¡œ ê³„ì† ì§„í–‰")
    
    def answer_question(self, query: str) -> Dict:
        """ì‚¬ìš©ì ì§ˆë¬¸ì— ëŒ€í•œ ë‹µë³€ ìƒì„± (ê°œì„ ëœ ê²€ì¦)"""
        try:
            # ì…ë ¥ ê²€ì¦
            if not query or not query.strip():
                return self._create_error_response("ì§ˆë¬¸ì„ ì…ë ¥í•´ì£¼ì„¸ìš”.", query)
            
            # ì…ë ¥ ì§ˆì˜ ì •ê·œí™”
            normalized_query = TextNormalizer.normalize(query)
            
            if len(normalized_query) < 2:
                return self._create_error_response("ì§ˆë¬¸ì´ ë„ˆë¬´ ì§§ìŠµë‹ˆë‹¤.", query)
            
            # 1. ê´€ë ¨ ë¬¸ë§¥ ê²€ìƒ‰
            if self.config.verbose:
                logger.info(f"ğŸ” ê´€ë ¨ ë¬¸ë§¥ ê²€ìƒ‰ ì¤‘...")
            
            search_results = self.searcher.search(normalized_query, top_k=self.config.search_top_k)
            context = self.searcher.get_context(search_results)
            
            # ì»¨í…ìŠ¤íŠ¸ë„ ì •ê·œí™”
            normalized_context = TextNormalizer.normalize(context)
            
            if self.config.verbose:
                logger.info(f"ğŸ“š {len(search_results)}ê°œ ê´€ë ¨ ë¬¸ì„œ ë°œê²¬")
            
            # 2. AI ì‘ë‹µ ìƒì„±
            if not self._generator_initialized:
                self._initialize_generator()
            
            if self.config.verbose:
                logger.info(f"ğŸ¤– AI ì‘ë‹µ ìƒì„± ì¤‘...")
            
            response = self.chat_generator.generate_response(normalized_query, normalized_context)
            
            # ì‘ë‹µë„ ì •ê·œí™”
            normalized_response = TextNormalizer.normalize(response)
            
            return {
                "query": normalized_query,
                "response": normalized_response,
                "context": normalized_context,
                "sources": [
                    {
                        "source": r.source,
                        "page": r.page,
                        "score": round(r.score, 3)
                    }
                    for r in search_results
                ],
                "model_info": {
                    "mode": "vllm",
                    "adapter": self.lora_adapter,
                    "temperature": self.config.response_temperature
                },
                "timestamp": datetime.now().isoformat()
            }
            
        except Exception as e:
            logger.error(f"âŒ ì§ˆë¬¸ ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
            return self._create_error_response(f"ì§ˆë¬¸ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}", query)
    
    def _create_error_response(self, error_message: str, original_query: str) -> Dict:
        """ì—ëŸ¬ ì‘ë‹µ ìƒì„±"""
        return {
            "query": TextNormalizer.normalize(original_query),
            "response": error_message,
            "context": "",
            "sources": [],
            "model_info": {
                "mode": "error",
                "error": error_message
            },
            "timestamp": datetime.now().isoformat()
        }


# Interface Segregation Principle: ì„œë²„ ìƒíƒœ í™•ì¸ ì¸í„°í˜ì´ìŠ¤
class IServerStatusChecker(ABC):
    """ì„œë²„ ìƒíƒœ í™•ì¸ ì¸í„°í˜ì´ìŠ¤"""
    
    @abstractmethod
    async def check_vllm_server(self) -> bool:
        pass


class ServerStatusChecker(IServerStatusChecker):
    """ì„œë²„ ìƒíƒœ í™•ì¸ í´ë˜ìŠ¤"""
    
    def __init__(self, base_url: str):
        self.base_url = base_url
    
    async def check_vllm_server(self) -> bool:
        """VLLM ì„œë²„ ìƒíƒœ í™•ì¸"""
        max_retries = 3
        retry_delay = 2
        
        for attempt in range(max_retries):
            try:
                is_healthy = await vllm_health_check()
                
                if is_healthy:
                    logger.info(f"âœ… VLLM ì„œë²„ ì—°ê²° í™•ì¸: {self.base_url}")
                    return True
                else:
                    logger.warning(f"âš ï¸ VLLM ì„œë²„ ì‘ë‹µ ì—†ìŒ (ì‹œë„ {attempt + 1}/{max_retries})")
                    if attempt < max_retries - 1:
                        logger.info(f"ğŸ”„ {retry_delay}ì´ˆ í›„ ì¬ì‹œë„...")
                        await asyncio.sleep(retry_delay)
                        retry_delay *= 1.5
                        
            except Exception as e:
                logger.error(f"âŒ VLLM ì„œë²„ í™•ì¸ ì‹¤íŒ¨ (ì‹œë„ {attempt + 1}/{max_retries}): {e}")
                if attempt < max_retries - 1:
                    logger.info(f"ğŸ”„ {retry_delay}ì´ˆ í›„ ì¬ì‹œë„...")
                    await asyncio.sleep(retry_delay)
                    retry_delay *= 1.5
        
        logger.error(f"âŒ VLLM ì„œë²„ ì—°ê²° ì‹¤íŒ¨: {self.base_url}")
        return False


class RAGChatbotPipeline:
    """RAG ì±—ë´‡ íŒŒì´í”„ë¼ì¸ ë©”ì¸ í´ë˜ìŠ¤ (SOLID ì›ì¹™ ì ìš©)"""
    
    def __init__(self, 
                 config: PipelineConfig, 
                 lora_adapter: Optional[str] = None,
                 hf_token: Optional[str] = None,
                 document_processor: Optional[IDocumentProcessor] = None,
                 vector_store_manager: Optional[IVectorStoreManager] = None,
                 server_status_checker: Optional[IServerStatusChecker] = None):
        
        # ì„¤ì • ê²€ì¦
        if not ConfigValidator.validate_pipeline_config(config):
            raise ValueError("ìœ íš¨í•˜ì§€ ì•Šì€ íŒŒì´í”„ë¼ì¸ ì„¤ì •ì…ë‹ˆë‹¤.")
        
        self.config = config
        self.lora_adapter = lora_adapter
        self.hf_token = hf_token
        
        # LoRA ì–´ëŒ‘í„° ê²€ì¦
        if self.lora_adapter:
            if not ModelIdValidator.is_valid_adapter_format(self.lora_adapter):
                logger.error(f"âŒ ìœ íš¨í•˜ì§€ ì•Šì€ LoRA ì–´ëŒ‘í„° í˜•ì‹: {self.lora_adapter}")
                self.lora_adapter = None
            else:
                self.lora_adapter = ModelIdValidator.validate_model_id(self.lora_adapter)
        
        # ì¶œë ¥ ë””ë ‰í† ë¦¬ ìƒì„±
        Path(config.output_dir).mkdir(parents=True, exist_ok=True)
        logger.info(f"ğŸ“ ì¶œë ¥ ë””ë ‰í† ë¦¬ ì¤€ë¹„: {config.output_dir}")
        
        # Dependency Injection
        self.document_processor = document_processor or DocumentProcessor(config)
        self.vector_store_manager = vector_store_manager or VectorStoreManager(config)
        self.server_status_checker = server_status_checker or ServerStatusChecker(config.vllm_base_url)
        
        self.chatbot_engine = None
    
    async def check_vllm_server(self) -> bool:
        """VLLM ì„œë²„ ìƒíƒœ í™•ì¸ (ìœ„ì„)"""
        return await self.server_status_checker.check_vllm_server()
    
    def ingest_document(self, pdf_path: str) -> bool:
        """ë¬¸ì„œ ì¸ì œìŠ¤íŠ¸ (PDF â†’ QA â†’ ë²¡í„° ì €ì¥)"""
        try:
            logger.info(f"ğŸ“‹ ë¬¸ì„œ ì¸ì œìŠ¤íŠ¸ ì‹œì‘: {pdf_path}")
            
            # ì…ë ¥ ê²€ì¦
            if not pdf_path or not pdf_path.strip():
                logger.error("âŒ PDF ê²½ë¡œê°€ ë¹„ì–´ìˆìŠµë‹ˆë‹¤.")
                return False
            
            # VLLM ì„œë²„ ìƒíƒœ í™•ì¸
            try:
                loop = asyncio.get_event_loop()
                if loop.is_running():
                    import concurrent.futures
                    with concurrent.futures.ThreadPoolExecutor() as executor:
                        future = executor.submit(asyncio.run, self.check_vllm_server())
                        server_healthy = future.result(timeout=30)
                else:
                    server_healthy = loop.run_until_complete(self.check_vllm_server())
                
                if not server_healthy:
                    logger.warning("âš ï¸ VLLM ì„œë²„ê°€ ì‘ë‹µí•˜ì§€ ì•Šì§€ë§Œ ë¬¸ì„œ ì¸ì œìŠ¤íŠ¸ëŠ” ê³„ì† ì§„í–‰í•©ë‹ˆë‹¤.")
                    
            except Exception as e:
                logger.warning(f"âš ï¸ VLLM ì„œë²„ í™•ì¸ ì¤‘ ì˜¤ë¥˜: {e}, ê³„ì† ì§„í–‰í•©ë‹ˆë‹¤.")
            
            # 1. PDF â†’ QA ìƒì„±
            qa_data = self.document_processor.process_document(pdf_path)
            
            if not qa_data:
                logger.error("âŒ QA ë°ì´í„° ìƒì„± ì‹¤íŒ¨")
                return False
            
            # 2. ë²¡í„° ìŠ¤í† ì–´ì— ì €ì¥
            success = self.vector_store_manager.store_qa_data(qa_data, pdf_path)
            
            if success:
                # 3. ì±—ë´‡ ì—”ì§„ ì´ˆê¸°í™” (LoRA ì–´ëŒ‘í„° í¬í•¨)
                embedding_store = self.vector_store_manager.get_embedding_store()
                self.chatbot_engine = ChatbotEngine(
                    self.config, 
                    embedding_store, 
                    self.lora_adapter,
                    self.hf_token
                )
                
                logger.info(f"ğŸ‰ ë¬¸ì„œ ì¸ì œìŠ¤íŠ¸ ì™„ë£Œ!")
                logger.info(f"ğŸ”§ ëª¨ë¸ ëª¨ë“œ: VLLM ì„œë²„")
                if self.lora_adapter:
                    logger.info(f"ğŸ¯ LoRA ì–´ëŒ‘í„°: {self.lora_adapter}")
                
                return True
            else:
                logger.error("âŒ ë¬¸ì„œ ì¸ì œìŠ¤íŠ¸ ì‹¤íŒ¨")
                return False
                
        except Exception as e:
            logger.error(f"âŒ ë¬¸ì„œ ì¸ì œìŠ¤íŠ¸ ì¤‘ ì˜¤ë¥˜: {e}")
            return False
    
    def chat(self, query: str) -> Dict:
        """ì±„íŒ… ì¸í„°í˜ì´ìŠ¤"""
        if not self.chatbot_engine:
            return {
                "query": TextNormalizer.normalize(query),
                "response": "ë¬¸ì„œë¥¼ ë¨¼ì € ì¸ì œìŠ¤íŠ¸í•´ì£¼ì„¸ìš”.",
                "context": "",
                "sources": [],
                "timestamp": datetime.now().isoformat()
            }
        
        return self.chatbot_engine.answer_question(query)
    
    def interactive_chat(self):
        """ëŒ€í™”í˜• ì±„íŒ… ëª¨ë“œ"""
        print("\nğŸ¤– RAG ì±—ë´‡ì´ ì¤€ë¹„ë˜ì—ˆìŠµë‹ˆë‹¤!")
        print(f"ğŸ”§ ëª¨ë¸ ëª¨ë“œ: VLLM ì„œë²„")
        if self.lora_adapter:
            print(f"ğŸ¯ LoRA ì–´ëŒ‘í„°: {self.lora_adapter}")
        print("ğŸ’¡ ë„ì›€ë§: 'help', ì¢…ë£Œ: 'exit'/'quit'/'ì¢…ë£Œ'")
        print("-" * 50)
        
        chat_history = []
        
        while True:
            try:
                user_input = input("\nğŸ’¬ ì§ˆë¬¸ì„ ì…ë ¥í•˜ì„¸ìš”: ").strip()
                
                if not user_input:
                    continue
                
                if user_input.lower() in ["exit", "quit", "ì¢…ë£Œ"]:
                    print("ğŸ‘‹ ì±„íŒ…ì„ ì¢…ë£Œí•©ë‹ˆë‹¤. ì•ˆë…•íˆ ê°€ì„¸ìš”!")
                    break
                
                if user_input.lower() == "help":
                    self._show_help()
                    continue
                
                if user_input.lower() == "history":
                    self._show_history(chat_history)
                    continue
                
                if user_input.lower() == "info":
                    self._show_model_info()
                    continue
                
                # ì§ˆë¬¸ ì²˜ë¦¬
                result = self.chat(user_input)
                
                # ì‘ë‹µ ì¶œë ¥
                print(f"\nğŸ¤– ë‹µë³€: {result['response']}")
                
                if self.config.verbose and result['sources']:
                    print(f"\nğŸ“š ì°¸ê³  ë¬¸ì„œ:")
                    for i, source in enumerate(result['sources'][:3]):
                        print(f"  {i+1}. {source['source']} (í˜ì´ì§€ {source['page']}, ì‹ ë¢°ë„: {source['score']})")
                
                # íˆìŠ¤í† ë¦¬ ì €ì¥
                chat_history.append(result)
                
            except KeyboardInterrupt:
                print("\n\nğŸ‘‹ ì±„íŒ…ì„ ì¢…ë£Œí•©ë‹ˆë‹¤.")
                break
            except Exception as e:
                print(f"\nâŒ ì˜¤ë¥˜ ë°œìƒ: {e}")
    
    def _show_help(self):
        """ë„ì›€ë§ í‘œì‹œ"""
        print("\nğŸ“– ì‚¬ìš© ê°€ëŠ¥í•œ ëª…ë ¹ì–´:")
        print("  help     - ì´ ë„ì›€ë§ í‘œì‹œ")
        print("  history  - ì±„íŒ… ê¸°ë¡ í‘œì‹œ")
        print("  info     - ëª¨ë¸ ì •ë³´ í‘œì‹œ")
        print("  exit     - ì±„íŒ… ì¢…ë£Œ")
        print("  ê·¸ ì™¸    - ì¼ë°˜ ì§ˆë¬¸")
    
    def _show_history(self, history: List[Dict]):
        """ì±„íŒ… ê¸°ë¡ í‘œì‹œ"""
        if not history:
            print("\nğŸ“ ì•„ì§ ì±„íŒ… ê¸°ë¡ì´ ì—†ìŠµë‹ˆë‹¤.")
            return
        
        print(f"\nğŸ“ ìµœê·¼ ì±„íŒ… ê¸°ë¡ ({len(history)}ê°œ):")
        for i, chat in enumerate(history[-5:], 1):  # ìµœê·¼ 5ê°œë§Œ
            print(f"  {i}. Q: {chat['query'][:50]}...")
            print(f"     A: {chat['response'][:50]}...")
    
    def _show_model_info(self):
        """ëª¨ë¸ ì •ë³´ í‘œì‹œ"""
        if self.chatbot_engine and self.chatbot_engine.chat_generator:
            try:
                info = self.chatbot_engine.chat_generator.get_model_info()
                print(f"\nğŸ”§ ëª¨ë¸ ì •ë³´:")
                for key, value in info.items():
                    print(f"  {key}: {value}")
            except Exception as e:
                print(f"\nâš ï¸ ëª¨ë¸ ì •ë³´ ê°€ì ¸ì˜¤ê¸° ì‹¤íŒ¨: {e}")
        else:
            print("\nâš ï¸ ëª¨ë¸ì´ ì•„ì§ ì´ˆê¸°í™”ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")


# í¸ì˜ í•¨ìˆ˜ë“¤ (ê¸°ì¡´ ì½”ë“œì™€ í˜¸í™˜ì„±)
def ingest_document(pdf_path: str, 
                   config: Optional[PipelineConfig] = None, 
                   lora_adapter: Optional[str] = None,
                   hf_token: Optional[str] = None,
                   use_vllm: bool = True) -> bool:
    """ë¬¸ì„œ ì¸ì œìŠ¤íŠ¸ í¸ì˜ í•¨ìˆ˜"""
    pipeline_config = config or PipelineConfig(pdf_path=pdf_path, use_vllm=use_vllm)
    pipeline = RAGChatbotPipeline(pipeline_config, lora_adapter, hf_token)
    return pipeline.ingest_document(pdf_path)


def answer_question(query: str, config: Optional[PipelineConfig] = None) -> str:
    """ì§ˆë¬¸ ë‹µë³€ í¸ì˜ í•¨ìˆ˜ (ê¸°ì¡´ í˜¸í™˜ì„±)"""
    try:
        return retrieve_relevant_chunks(query)
    except Exception as e:
        logger.error(f"ë‹µë³€ ìƒì„± ì‹¤íŒ¨: {e}")
        return "ì£„ì†¡í•©ë‹ˆë‹¤. ë‹µë³€ ìƒì„± ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤."


def create_vllm_pipeline(pdf_path: str, 
                        lora_adapter: str, 
                        hf_token: str,
                        vllm_base_url: str = DYNAMIC_VLLM_URL,
                        system_message: str = (
                            "ë‹¹ì‹ ì€ ì œê³µëœ ì°¸ê³  ë¬¸ì„œì˜ ì •í™•í•œ ì •ë³´ì™€ ì‚¬ì‹¤ì„ ë°”íƒ•ìœ¼ë¡œ ë‹µë³€í•˜ëŠ” AI ì–´ì‹œìŠ¤í„´íŠ¸ì…ë‹ˆë‹¤. "
                            "**ì¤‘ìš”**: ë¬¸ì„œì— í¬í•¨ëœ ëª¨ë“  ë‚´ìš©ì€ ì ˆëŒ€ ìš”ì•½í•˜ê±°ë‚˜ ìƒëµí•˜ì§€ ë§ê³ , ì›ë¬¸ ê·¸ëŒ€ë¡œ ì™„ì „íˆ í¬í•¨í•´ì•¼ í•©ë‹ˆë‹¤. "
                            "ì‚¬ì‹¤, ìˆ˜ì¹˜, ë‚ ì§œ, ì •ì±… ë‚´ìš©, ì„¸ë¶€ ì‚¬í•­ ë“± ëª¨ë“  ì •ë³´ë¥¼ ì •í™•íˆ ê·¸ëŒ€ë¡œ ìœ ì§€í•´ì£¼ì„¸ìš”. "
                            "ë¬¸ì„œ ë‚´ìš©ì˜ ì™„ì „ì„±ê³¼ ì •í™•ì„±ì´ ìµœìš°ì„ ì´ë©°, ë§íˆ¬ì™€ í‘œí˜„ ë°©ì‹ë§Œ ìºë¦­í„° ìŠ¤íƒ€ì¼ë¡œ ì¡°ì •í•´ì£¼ì„¸ìš”. "
                            "ë¬¸ì„œ ë‚´ìš©ì„ ì„ì˜ë¡œ ë³€ê²½, ìš”ì•½, ì¶”ê°€í•˜ì§€ ë§ê³ , ì˜¤ì§ ì œê³µëœ ì •ë³´ë¥¼ ì™„ì „íˆ ê·¸ëŒ€ë¡œ ì‚¬ìš©í•´ ë‹µë³€í•´ì£¼ì„¸ìš”. "
                            "\n\n**ìºë¦­í„° ì •ì²´ì„±**: ë‹¹ì‹ ì€ {influencer_name} ìºë¦­í„°ì…ë‹ˆë‹¤. "
                            "ìê¸°ì†Œê°œë¥¼ í•  ë•Œë‚˜ 'ë„ˆ ëˆ„êµ¬ì•¼?', 'ë‹¹ì‹ ì€ ëˆ„êµ¬ì¸ê°€ìš”?', 'ì´ë¦„ì´ ë­ì•¼?' ê°™ì€ ì§ˆë¬¸ì„ ë°›ìœ¼ë©´ "
                            "ë°˜ë“œì‹œ 'ë‚˜ëŠ” {influencer_name}ì´ì•¼!' ë˜ëŠ” 'ì €ëŠ” {influencer_name}ì…ë‹ˆë‹¤!'ë¼ê³  ë‹µë³€í•´ì•¼ í•©ë‹ˆë‹¤. "
                            "í•­ìƒ {influencer_name}ì˜ ì •ì²´ì„±ì„ ìœ ì§€í•˜ë©° ê·¸ ìºë¦­í„°ë‹µê²Œ í–‰ë™í•˜ì„¸ìš”."
                        ),
                        influencer_name: str = "AI",
                        temperature: float = 0.8) -> RAGChatbotPipeline:
    """VLLM ê¸°ë°˜ RAG íŒŒì´í”„ë¼ì¸ ìƒì„± í¸ì˜ í•¨ìˆ˜"""
    config = PipelineConfig(
        pdf_path=pdf_path,
        use_vllm=True,
        vllm_base_url=vllm_base_url,
        system_message=system_message,
        influencer_name=influencer_name,
        response_temperature=temperature
    )
    
    return RAGChatbotPipeline(config, lora_adapter, hf_token)


def main():
    """ë©”ì¸ í•¨ìˆ˜"""
    parser = argparse.ArgumentParser(description="RAG ì±—ë´‡ íŒŒì´í”„ë¼ì¸ (VLLM ì§€ì›, SOLID ì›ì¹™ ì ìš©)")
    parser.add_argument("pdf_path", help="PDF íŒŒì¼ ê²½ë¡œ")
    parser.add_argument("--output-dir", default="./rag_output", help="ì¶œë ¥ ë””ë ‰í† ë¦¬")
    parser.add_argument("--verbose", action="store_true", help="ìƒì„¸ ì¶œë ¥")
    parser.add_argument("--max-tokens", type=int, default=512, help="ìµœëŒ€ ìƒì„± í† í° ìˆ˜")
    parser.add_argument("--temperature", type=float, default=0.8, help="ìƒì„± ì˜¨ë„ (ë†’ì„ìˆ˜ë¡ ì°½ì˜ì )")
    parser.add_argument("--search-k", type=int, default=3, help="ê²€ìƒ‰í•  ë¬¸ì„œ ìˆ˜")
    parser.add_argument("--lora-adapter", type=str, default="khj0816/EXAONE-Ian", help="LoRA ì–´ëŒ‘í„° ì´ë¦„")
    parser.add_argument("--hf-token", type=str, help="HuggingFace í† í°")
    parser.add_argument("--system-message", default="ë‹¹ì‹ ì€ ë„ì›€ì´ ë˜ëŠ” AI ì–´ì‹œìŠ¤í„´íŠ¸ì…ë‹ˆë‹¤.", help="ì‹œìŠ¤í…œ ë©”ì‹œì§€")
    parser.add_argument("--influencer-name", default="AI", help="AI ìºë¦­í„° ì´ë¦„")
    
    args = parser.parse_args()
    
    # LoRA ì–´ëŒ‘í„° ì„¤ì • ë° ê²€ì¦
    lora_adapter = None
    if args.lora_adapter and args.lora_adapter.lower() != "none":
        if ModelIdValidator.is_valid_adapter_format(args.lora_adapter):
            lora_adapter = ModelIdValidator.validate_model_id(args.lora_adapter)
            if not lora_adapter:
                logger.error(f"âŒ ìœ íš¨í•˜ì§€ ì•Šì€ LoRA ì–´ëŒ‘í„°: {args.lora_adapter}")
                sys.exit(1)
        else:
            logger.error(f"âŒ ì˜ëª»ëœ LoRA ì–´ëŒ‘í„° í˜•ì‹: {args.lora_adapter}")
            sys.exit(1)
    
    hf_token = os.getenv("HF_TOKEN")
    
    if not hf_token and lora_adapter:
        logger.warning("âš ï¸ HuggingFace í† í°ì´ ì—†ìŠµë‹ˆë‹¤. LoRA ì–´ëŒ‘í„° ë¡œë“œì— ì‹¤íŒ¨í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.")
    
    # ì„¤ì • ìƒì„± ë° ê²€ì¦
    try:
        config = PipelineConfig(
            pdf_path=args.pdf_path,
            output_dir=args.output_dir,
            verbose=args.verbose,
            response_max_tokens=args.max_tokens,
            response_temperature=args.temperature,
            search_top_k=args.search_k,
            use_vllm=True,
            vllm_base_url=DYNAMIC_VLLM_URL,  # backend .envì—ì„œ ë¡œë“œëœ URL ì‚¬ìš©
            system_message=TextNormalizer.normalize(args.system_message),
            influencer_name=TextNormalizer.normalize(args.influencer_name)
        )
    except Exception as e:
        logger.error(f"âŒ ì„¤ì • ìƒì„± ì‹¤íŒ¨: {e}")
        sys.exit(1)
    
    # íŒŒì´í”„ë¼ì¸ ìƒì„± ë° ì‹¤í–‰
    try:
        pipeline = RAGChatbotPipeline(config, lora_adapter, hf_token)
        
        # ë¬¸ì„œ ì¸ì œìŠ¤íŠ¸
        if not pipeline.ingest_document(args.pdf_path):
            print("âŒ ë¬¸ì„œ ì¸ì œìŠ¤íŠ¸ ì‹¤íŒ¨")
            sys.exit(1)
        
        # ëŒ€í™”í˜• ì±„íŒ… ì‹œì‘
        pipeline.interactive_chat()
        
    except Exception as e:
        logger.error(f"âŒ íŒŒì´í”„ë¼ì¸ ì‹¤í–‰ ì‹¤íŒ¨: {e}")
        sys.exit(1)


if __name__ == "__main__":
    if len(sys.argv) < 2:
        print("ğŸš€ RAG ì±—ë´‡ íŒŒì´í”„ë¼ì¸ (VLLM ì§€ì›, SOLID ì›ì¹™ ì ìš©)")
        print("\nì‚¬ìš©ë²•:")
        print("  python chatbot_pipeline.py <PDFíŒŒì¼ê²½ë¡œ> [ì˜µì…˜]")
        print("\nì£¼ìš” ì˜µì…˜:")
        print("  --lora-adapter STR    LoRA ì–´ëŒ‘í„° ì´ë¦„ (ê¸°ë³¸: khj0816/EXAONE-Ian)")
        print("  --hf-token STR        HuggingFace í† í°")
        print("  --system-message STR  ì‹œìŠ¤í…œ ë©”ì‹œì§€")
        print("  --influencer-name STR AI ìºë¦­í„° ì´ë¦„ (ê¸°ë³¸: AI)")
        print("  --temperature FLOAT   ìƒì„± ì˜¨ë„ (ê¸°ë³¸: 0.8)")
        print("  --verbose             ìƒì„¸ ì¶œë ¥")
        print("\nì˜ˆì‹œ:")
        print("  # VLLM ì„œë²„ ì‚¬ìš©")
        print("  python chatbot_pipeline.py document.pdf --hf-token your_token")
        print("  python chatbot_pipeline.py document.pdf --lora-adapter user/model --hf-token your_token")
        print("  ")
        print("  # ìºë¦­í„° ì„¤ì •")
        print("  python chatbot_pipeline.py document.pdf --influencer-name 'í•œì„¸ë‚˜' --system-message 'ë‹¹ì‹ ì€ ì¹œê·¼í•œ AI ì–´ì‹œìŠ¤í„´íŠ¸ í•œì„¸ë‚˜ì…ë‹ˆë‹¤.'")
        print("\nğŸ’¡ VLLM ì„œë²„ë¥¼ ë¨¼ì € ì‹œì‘í•´ì•¼ í•©ë‹ˆë‹¤:")
        print("  python -m vllm.entrypoints.openai.api_server --model LGAI-EXAONE/EXAONE-3.5-2.4B-Instruct --enable-lora")
        print("\nğŸ“‹ Backend .env íŒŒì¼ì— VLLM_SERVER_URLì„ ì„¤ì •í•˜ì„¸ìš”:")
        print("  echo 'VLLM_SERVER_URL=https://your-vllm-server-url' >> backend/.env")
        sys.exit(1)
    
    main()